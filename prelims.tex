%!TEX root = main.tex
% Preliminary Definitions and Results for negFE

\section{Preliminaries}
\label{sec:prelim}
For distributions $X, Y$ over the same discrete domain,
\[
\Delta(X, Y)\overset{def}= \frac{1}{2}\sum_{x \in X} \left| \Pr[X=x] - \Pr[Y=y]\right|.
\]
For a metric space $(\mathcal{M}, \dis)$ let $B_t(x) = \{y | \dis(x, y)\le t\}$. If the size of $B_t(x)$ is the same for all points $x$ we use $|B_t|$ to denote this quantity and say that the \emph{size of balls is center independent}.  All logarithms are base $2$.

This work considers the possibility of constructing fuzzy extractors from a finite family of distributions that we will call $\mathcal{W}$. 
Throughout, we will need the ability to describe a particular value in this family.  
We let $Z$ be an index for the distributions in the family and we denote the $Z$th distribution $W_Z$. 
The distribution $W_Z$ can then be sampled, and we denote a sample of $W_Z$ as $w \in \zeroone{n}$. 

\subsection{Notions of Entroy}

\begin{definition}[Entropy]
    \emph{Shannon Entropy} or simply Entropy, denoted $\ent{X}$, for some discrete random variable $X$ is a measure of how stable the outcomes of the random variable are. It is calculated as \[\ent{X} \defined \sum\limits_{i=1}^n \p{x_i}\log{\p{x_i}}\] where there are $n$ values that $X$ takes and we denote them as $x_i$. 
    
    If the two possible outcomes of $x$ are $0$ and $1$ with $\Pr[X=1] = p$ the binary entropy of $X$ is  \[h_2(X) :=-p\cdot\log{p} - (1-p)\cdot\log{1-p}.\] 
\end{definition}

\begin{definition}[Min Entropy]
For a discrete random variable $X$, 
    \emph{min Entropy}, denoted $\minent{X}$, is negative log of the probability of the most likely event of $X$: \[\minent{X} \defined -\log{\max\limits_{x_i} p(x_i)}.\]  
\end{definition}

\begin{definition}[Average Min Entropy]
    \emph{Average Min Entropy}, denoted $\acminent{X}{Y}$ for two random variables $X$ and $Y$ is \[ \acminent{X}{Y} \defined -\log{\Exlim{y \leftarrow Y}{\max\limits_{x} \Prob{X = x\ |\ Y = y}}}.\] 
\end{definition}

\begin{definition}[Smooth Conditional Min Entropy~\cite{renner2005simple}]
    \emph{Smooth Conditional Min Entropy}, denoted $\acminent{X}{Y}$ for two random variables $X$ and $Y$ is \[\max_{(X',Y') | \Delta((X', Y'), (X, Y))\le \epsilon} \left( \acminent{X'}{Y'}\right).
    \] 
\end{definition}


\begin{definition}[Hartley Entropy]
For a discrete random variable $X$ \emph{Hartley entropy}, denoted $\hart{X}$ is the logarithm of the support size of $X$: 
\[    \hart{X} = | \sbr{x \in X\,|\, \Prob{X = x} > 0}|.
    \]
%    For a pair of discrete random variables $X, Y$ conditional Hartley entropy is 
%    \[
%    \hart{X|Y} = -\log{\expe_{y\leftarrow Y} 2^{-\hart{X|Y=y}}}.
%    \]
\end{definition}

\subsection{Basic Entropy Results}
\begin{lemma}
Let $(X, Y)$ be a pair of random variables and let $S(X, Y)$ be a set that is the output of a randomized function on input $(X, Y)$ then 
\[
\Haveps(X|Y, \eta)\ge \Haveps(X|Y) + \log{ 1/\Pr[Y\in S(X, Y)]}.
\]
\label{lem:smooth ent cond}
\end{lemma}
\begin{proof}
Let $X', Y'$ be a distribution such that $\Delta((X, Y), (X', Y')) \le \epsilon$.  By \cite[Lemma 7.8]{fuller2020fuzzy} for any event $\eta$
\begin{align*}
\Hav(X' | Y') =\log{\frac{1}{\Pr[\eta]}}+\Hav(X'| \eta, Y').
\end{align*}
Let $f$ denote the randomized function such that $\eta = f(X, Y)$.    
The proof completes by noting that $\Delta((X, Y), (X', Y'))\le \epsilon$ implies that 
\[
\Delta((X, Y, Y\in S(X, Y)), (X', Y,' Y'\in S(X', Y')))\le \epsilon.\]  which implies that 
\[
\Haveps(X| Y) \le \Haveps(X|Y, \eta) +\log{\frac{1}{\Pr[Y\in S(X,Y)]}}.
\]
This completes the proof of Lemma~\ref{lem:smooth ent cond}.
\end{proof}


\begin{lemma}{\cite[Lemma 2.2a]{dodis2008fuzzy}}
    \label{lem:markovpred}
    Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be random variables. Let $Y$ be a random variable arbitrarility correlated with $\vec{X}$. 
    Let $\alpha > 0$, then for all but a $(1-1/\alpha)$ fraction of the $X_i$ the entropy loss is less than $\log{\alpha}/k$. 
\end{lemma}


\begin{lemma}
    \label{lem:conditionalminentloss}
    Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be independent random variables. 
    Let $Y$ be a random variable arbitrarily correlated with $\vec{X}$. 
    Then 
    \[
        \acminent{\vec{X}}{Y} \geq \minent{\vec{X}} - \hart{Y} = \sum \minent{X_i} - \hart{Y}
    \]
\end{lemma} 
\noindent
Lemma~\ref{lem:conditionalminentloss} follows by \cite[Lemma 2.2b]{dodis2008fuzzy} and independence of the $X_i$.



\subsection{Fuzzy Min-Entropy and Hamming Balls}
\begin{definition}[Fuzzy min-entropy~\cite{fuller2020fuzzy}]

For a distribution $W$ and a distance parameter $t$, the fuzzy min-entropy of $W$, denoted $\Hfuzz(W)$ is 
\[
\Hfuzz(W) \overset{def}= -\log{ \max_{w^*} \left(\sum_{w, \dis(w, w^*)\le t} \Pr[W=w] \right)}.
\]
\end{definition}

\begin{proposition} \label{lem:max fuzz ent}
For all distributions $W$ over a metric space $(\mathcal{M}, \dis)$, 
\[\Hfuzz(W) \le \log{|\mathcal{M}|} - \log{|B_t|}.
\]
\end{proposition}
\noindent
For $\mathcal{M} = \zo^n$ and the binary Hamming metric,
Using Ash~\cite[Lemma 4.7.2, Equation 4.7.5, p. 115]{ash2012information} one has
\begin{align} nh_2(t/n)  -1/2\log{n} - 1/2 \le \log{|B_t|} \le  nh_2(t/n)\label{eq:size of balls}.\end{align}
and thus, 
\[
\Hfuzz(W) \le n(1-h_2(t/n)).
\]

\noindent
We now introduce the notion of $\beta$-density which measures the size of a Hamming ball in comparison to the whole metric space.  $\beta$-density is crucial for all of our proofs. 
\begin{definition}
Let $(\mathcal{M}, \dis)$ be a metric space where the size of balls is center independent.  The $\beta$ density is
\[
\beta := \log{\frac{|\mathcal{M}|- |B_t|}{|B_t|}} 
\]
\end{definition}
\begin{claim} 
For the binary Hamming metric over $\zo^n$ for $t<n/2$
\[
n(1-h_2(t/n))-\frac{\log{n}}{2}-1 \le \beta \le n(1-h_2(t/n)).
\]

\end{claim}

\begin{proof}
By Equation~\ref{eq:size of balls} one has: 
\begin{align*}
\beta &= \log{\frac{2^n - |B_t|}{|B_t|}} \\&= \log{\frac{2^n}{|B_t|} -1} \\&\ge \log{2^{n(1-h_2(t/n)) - 1/2 \log{n}-1/2} -1} \\&\ge n(1-h_2(t/n)) - 1/2\log{n}-1\\
\beta&\le \log{2^{n(1-h_2(t/n))} -1} \le n(1-h_2(t/n)).
\end{align*}
\end{proof}


    \subsection{Fuzzy Extractors and Secure Sketches}
\begin{definition}[Secure Sketch~\cite{dodis2008fuzzy}]
For metric space $(\mathcal{M}, \dis)$ and distribution $W$ with probability mass function $z$, a $(\mathcal{M}, \tilde{m}, t, \epsilon, \delta, \ell)$-\emph{secure sketch} is a pair of algorithms $(\sketch_z, \rec_z)$ with the following properties 
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, then \[\Pr_{ss \leftarrow \sketch(w)}[\rec_z(w', ss) = w]\ge 1-\delta.\]
\item \textbf{Security}  $\Haveps(W | \sketch_z(W)) \ge \tilde{m}$.
\item \textbf{Space Bounded} The circuits $\sketch_z$ and $\rec_z$ require at most $\ell$ bits to describe.  That is, $|\sketch_z| +|\rec_z|\le \ell$.
\end{enumerate}
\end{definition}

\paragraph{The use of smooth min-entropy} In the above, the secure sketch is required to retain smooth conditional min-entropy of $W$ conditioned on the sketch.  Many definitions consider $\epsilon=0$ or average min-entropy.  However, smooth min-entropy is the necessary condition for privacy amplification through the use of an average-case randomness extractor~\cite{renner2005simple} so it captures whether the non-interactive information-reconciliation task is possible. 

\begin{definition}[Fuzzy Extractor~\cite{dodis2008fuzzy}]
For metric space $(\mathcal{M}, \dis)$ and probability distribution $W$ with probability mass function $z$, a $(\mathcal{M}, \kappa, t, \epsilon)$-\emph{fuzzy extractor} is a pair of algorithms $(\gen_z, \rep_z)$ with the following properties 
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, then 
\[\Pr_{r, p \leftarrow \gen(w)}[\rep(w', p) = r]=1.\] 
\item \textbf{Security} Let $R, P \leftarrow \gen_z(W)$ and $U_\kappa$ be a uniformly distributed random variable over $\zo^\kappa$ it holds that \[\Delta((R, P), (U_\kappa, P))\le \epsilon.\]
\item  \textbf{Space Bounded} The circuits $\gen_z$ and $\rep_z$ require $\ell$ bits to describe.  That is, $|\gen| +|\rec|\le \ell$.
\end{enumerate}
\label{def:fe}
\end{definition}

\noindent
We now define fuzzy extractors and secure sketches with advice.  This is an intermediate definition that will be used in proofs throughout.  As we show below, the impossibility of building a fuzzy extractor with advice for a family $\mathcal{W}$ implies the impossibility of building a fuzzy extractor for some $W\in\mathcal{W}$.

\begin{definition}[Secure Sketch with distributional advice]
\label{def:ss distributional}
Let $\mathcal{W}$ be a family of distributions indexed by $z$.  That is, denote each distribution in $\mathcal{W}$ as $W_Z$ with $Z$ describing the probability mass function of $W$.  
For metric space $(\{0,1\}^n, \dis)$, a $(\{0,1\}^n, \mathcal{W}, \tilde{m}, t, \epsilon, \delta, \ell)$-\emph{secure sketch with distributional advice} is a triple of algorithms $(\gen, \rep, \aux)$ with the following properties:
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, let \[\Pr_{(r, p) \leftarrow \sketch(w)}[\rec(w', p) = w]\ge 1-\delta.\]
\item \textbf{Security} Let $\aux$ be a deterministic function with output in $\zo^\ell$.  For all distributions $W_Z \in \mathcal{W}$, define $\advise_Z:= \aux(Z)$ and let $ss \leftarrow \sketch(W_Z, \advise_Z)$. Then,
\[
\expe_{Z} [\Haveps(W_Z| ss)] \ge \tilde{m}.
\]
\end{enumerate}
\end{definition}


\begin{definition}[Fuzzy Extractor with distributional advice]
\label{def:fe distributional}
Let $\mathcal{W}$ be a family of distributions indexed by $z$.  That is, denote each distribution in $\mathcal{W}$ as $W_Z$ with $Z$ describing the probability mass function of $W$.  
For metric space $(\{0,1\}^n, \dis)$, a $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-\emph{fuzzy extractor with distributional advice} is a triple of algorithms $(\gen, \rep, \aux)$ with the following properties:
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, let \[\Pr_{(r, p) \leftarrow \gen(w)}[\rep(w', p) = r]=1.\]
\item \textbf{Security} Let $\aux$ be a deterministic function with output in $\zo^\ell$.  For a distribution $W_Z \in \mathcal{W}$, define $\advise_Z:= \aux(Z)$, let $(R, P) \leftarrow \gen(W, \advise_Z)$ and $U_\kappa$ be a uniformly distributed random variable over $\zo^\kappa$ it holds that \[\Delta((R, P, Z), (U_\kappa, P, Z))\le \epsilon.\]
\end{enumerate}
\end{definition}


\subsection{Upper bound for size of Fuzzy Extractors and Secure Sketches}
Fuller, Reyzin, and Smith~\cite{fuller2016fuzzy,fuller2020fuzzy} show that the size of a fuzzy extractor can be upper bound in a general case.  We present a stronger version of their lemma that is contained in their proof.  The difference is we bound the union of viable points across difference values of $\mathsf{key}$ while they only bound the size of a single $\mathsf{key}$ corresponding to the true point.  Their argument is purely geometric, so it also applies to fuzzy extractors with distributional advice. 
We restate their lemma here for completeness. 

\begin{lemma}[Lemma 5.2 in \cite{fuller2020fuzzy}]
    \label{lem:smallgeneralviable}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, and $\epsilon > 0$. 
    Suppose $(\mathsf{Gen, Rep})$ is a $(\mathcal{M,W},\kappa, t, \ell, \epsilon)$-fuzzy extractor with distributional advice for some distribution family $\mathcal{W}$ over $\mathcal{M}$. 
    For any fixed $p$, for any value $\advise \in \zo^\ell$, there is a set $\mathsf{GoodKey}_p \subseteq \zeroone{\kappa}$ of size at least $2^{\kappa - 1}$ such that for every $\mathsf{key} \in \goodkey_p$,
    \[
       \sum_{\mathsf{key} \in \goodkey_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v, \advise)}}|}\right) \leq n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}.
    \]   
\end{lemma}

Fuller, Reyzin, and Smith~\cite[Lemma 7.3]{fuller2020fuzzy} showed that good secure sketches are bounded in size as they imply good Shannon error correcting codes.  This result holds true if one considers a secure sketch that retains smooth min-entropy with no loss in parameters because it only relies on the correctness of the secure sketch (not the security property).  

\begin{lemma}
\label{lem:smallgeneralviable ss}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric. Suppose $(\sketch, \rec)$ is a $(\zo^n, \mathcal{W}, \tilde{m}, t, \epsilon_\sketch, \delta)$-secure sketch, for some family $\mathcal{W}$. 
    
%    Let $W\in \mathcal{W}$, and let $W', SS'$ be a distribution such that $\Delta((W, \sketch(W)), (W', SS'))\le \epsilon$ and $\Hav(W' |SS')\ge \tilde{m}$.
   For every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
    \log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
\end{lemma}


\subsection{Converting between advice and the distribution sensitive setting}
\begin{lemma}

Let $\mathcal{W}$ be a distribution family indexed by the random family $Z$ and suppose that no $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon, 2\ell)$-fuzzy extractor with distributional advice exists.  For all families $\mathcal{W}'\subseteq \mathcal{W}$ with probability mass functions $Z'\subseteq Z$ where $\Pr[Z\in Z']\ge \zeta$ there is some $Z'$ there is no  $(\{0,1\}^n,\kappa, t, (\epsilon-(1-\zeta))/\zeta, \ell)$ fuzzy extractor $(\gen_Z, \rep_Z)$.
\label{lem:distributional advise suffices}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:distributional advise suffices}]
We proceed by contrapositive.  Let $\mathcal{W}'$ be some subset of $\mathcal{W}$ with relative size at least $\zeta$ where  for every $W_Z\in\mathcal{W}'$ there exists an $(\{0,1\}^n,\kappa, t, (\epsilon-(1-\zeta))/\zeta, \ell)$-fuzzy extractor.  We denote these algorithms by $(\gen_Z, \rep_Z)$ respectively.  We now describe how to build the fuzzy extractor $(\gen, \rep, \advise)$ with distributional advice.  Let 
\[
\advise(Z) = \begin{cases} (\gen_Z, \rep_Z)& Z\in Z'\\\perp&\text{otherwise.}\end{cases}
\]
In both cases, $\advise(Z)$ has length at most $\ell$. Then define $\gen(x, C)$ as follows:  if $C = \perp$ sample a random key $r$ output $(r, r)$, otherwise interpret $C$ as two circuits $\gen', \rep'$ and output $\gen'(x)$.  Define $\rep(x, p, C)$ interpret $C$ if $C = \perp$ output $p$, otherwise parse $C$ as two circuits $\gen', \rep'$ and output $\rep'(x', p)$.  
Then 
\begin{align*}
\Delta((R, P, Z), (U_\kappa, P, Z)) &= \Delta((R, P, Z), (U_\kappa, P, Z) | Z\in Z')+\Delta((R, P, Z), (U_\kappa, P, Z) | Z\not\in Z')\\
&\le \frac{\epsilon-(1-\zeta)}{\zeta} * \zeta + 1* (1-\zeta) = \epsilon.
\end{align*}
It is clear that $(\gen, \rep, \advise)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon, 2\ell)$ fuzzy extractor with distributional advise.
\end{proof}

\paragraph{Interpretation} 
In particular if one sets $\zeta = 1-\sqrt{\epsilon}$ this implies that for all subsets $\mathcal{W}' \subseteq \mathcal{W}$ where $\Pr[Z\in Z']\ge 1-\sqrt{\epsilon}.$ there is no $(\{0,1\}^n,\kappa, t, \sqrt{\epsilon}, \ell)$-fuzzy extractor for some element of $\mathcal{W}'$.  This shows that at least $1-\sqrt{\epsilon}$ fraction of elements in $\mathcal{W}$ do not have $(\{0,1\}^n,\kappa, t, \sqrt{\epsilon}, \ell)$-fuzzy extractors.

\begin{lemma}
Let $\mathcal{W}$ be a distribution family indexed by the random family $Z$ and suppose that no $(\zo^n, \mathcal{W}, \tilde{m}, t, \epsilon, \delta, 2\ell)$-secure sketch with distributional advice exists.  For all families $\mathcal{W}'\subseteq \mathcal{W}$ with probability mass functions $Z'\subseteq Z$ where $\Pr[Z\in Z']\ge 1/2$ where 
there is some $Z'$ for which no  $(\zo^n, \tilde{m}-1, t, \epsilon, \delta, \ell)$ fuzzy extractor $(\sketch_{Z'}, \rec_{Z'})$  exists.
\label{lem:distributional advise suffices ss}
\end{lemma}

\begin{proof}
The proof of Lemma~\ref{lem:distributional advise suffices ss} follows the structure of the proof of Lemma~\ref{lem:distributional advise suffices} with the following observation for smooth conditional min-entropy.  Let $1_{Z'}$ be a indicator random variables with $\Pr[1_{Z'} = 1] =1/2$, then 
\begin{align*}
\expe_{Z} [\Haveps(W_Z| ss)] &\le \min\left\{\expe_{Z} [\Haveps(W_Z| ss, 1_{Z'} = 1)],\expe_{Z} [\Haveps(W_Z| ss, 1_{Z'} = 0)] \right\}+1\\
&\le \expe_{Z} [\Haveps(W_Z| ss, 1_{Z'} = 1)]+1\\
&\le \tilde{m}-1+1 = \tilde{m}.
\end{align*}
\end{proof}

The natural interpretation of the above is that half of the distributions of $Z$ have no efficient secure sketch retaining $\tilde{m}-1$ bits of entropy.

