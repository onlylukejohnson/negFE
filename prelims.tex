%!TEX root = main.tex
% Preliminary Definitions and Results for negFE

\section{Preliminaries}
In this section we will introduce existing results to help clarify our place in the literature, provide necessary existing definitions to show what we are borrowing and what we build on, and we will provide new definitions in support of our novel results.  For distributions $X, Y$, $\Delta(X, Y)$ represents the statistical distance between the two distributions.  That is, 
\[
\Delta(X, Y)\overset{def}= \frac{1}{2}\sum_{x \in X} \left| \Pr[X=x] - \Pr[Y=y]\right|.
\]

For a metric space $(\mathcal{M}, \dis)$ let $B_t(x) = \{y | \dis(x, y)\le t\}$. For a metric space if the size of $B_t(x)$ is the same for all points $x$ we use $|B_t|$ to denote this quantity and say that the \emph{size of balls is center independent}.

This work considers the possibility of constructing fuzzy extractors from a finite family of distributions that we will call $\mathcal{W}$. 
Throughout, we will need the ability to describe a particular value in this family.  
We let $Z$ be an index for the distributions in the family and we denote the $Z$th distribution $W_Z$. 
The distribution $W_Z$ can then be sampled, and we denote a sample of $W_Z$ as $w_{Z} \in \zeroone{n}$. When $Z$ is clear, we will omit the subscript.

\subsection{Notions of Entroy}

\begin{definition}[Entropy]
    \emph{Shannon Entropy} or simply Entropy, denoted $\ent{X}$, for some discrete random variable $X$ is a measure of how stable the outcomes of the random variable are. It is calculated as \[\ent{X} \defined \sum\limits_{i=1}^n \p{x_i}\log{\p{x_i}}\] where there are $n$ values that $X$ takes and we denote them as $x_i$. 
\end{definition}

\begin{definition}[Min Entropy]
    \emph{Min Entropy}, denoted $\minent{X}$, is a best case measure of the stability of the random variable $X$. It is calculated as \[\minent{X} \defined -\log{\max\limits_{x_i} p(x_i)}.\]  
\end{definition}

\begin{definition}[Average Min Entropy]
    \emph{Average Min Entropy}, denoted $\acminent{X}{Y}$ for two random variables $X$ and $Y$ is \[ \acminent{X}{Y} \defined -\log{\Exlim{y \leftarrow Y}{\max\limits_{x} \Prob{X = x\ |\ Y = y}}}.\] 
\end{definition}

\begin{definition}[Smooth Conditional Min Entropy~\cite{renner2005simple}]
    \emph{Smooth Conditional Min Entropy}, denoted $\acminent{X}{Y}$ for two random variables $X$ and $Y$ is \[\max_{Z | \Delta((Z, Y), (X, Y))\le \epsilon} \left( \acminent{Z}{Y}\right).
    \] 
\end{definition}


\begin{definition}[Hartley Entropy]
    \emph{Hartley Entropy} also called \emph{Hartley's Function} measures the uncertainty of a random variable in a basic way, measuring the number of outcomes the random variable has with non-zero probability. Formally, $
    \hart{X} = | \sbr{x \in X\,|\, \Prob{X = x} > 0}|.
    $
\end{definition}

\begin{lemma}{\cite[Lemma 2.2a]{dodis2008fuzzy}}
    \label{lem:markovpred}
    Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be random variables. Let $Y$ be a random variable arbitrarility correlated with $\vec{X}$. 
    Let $\alpha > 0$, then for all but a $(1-1/\alpha)$ fraction of the $X_i$ the entropy loss is less than $\log{\alpha}/k$. 
\end{lemma}

\begin{lemma}
    \label{lem:conditionalminentloss}
    Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be independent random variables. 
    Let $Y$ be a random variable arbitrarily correlated with $\vec{X}$. 
    Then 
    \[
        \acminent{\vec{X}}{Y} \geq \sum \minent{X_i} - \hart{Y}
    \]
\end{lemma} 



\subsection{Fuzzy Min-Entropy and Hamming Balls}
\begin{definition}[Fuzzy min-entropy~\cite{fuller2020fuzzy}]

For a distribution $W$ and a distance parameter $t$, the fuzzy min-entropy of $W$, denoted $\Hfuzz(W)$ is 
\[
\Hfuzz(W) \overset{def}= -\log{ \max_{w^*} \left(\sum_{w, \dis(w, w^*)\le t} \Pr[W=w] \right)}.
\]
\end{definition}

\begin{lemma} \label{lem:max fuzz ent}
For all distributions $W$ over a metric space $(\mathcal{M}, \dis)$, 
\[\Hfuzz(W) \le \log{|\mathcal{M}|} - \log{|B_t|}.
\]
\end{lemma}
\noindent
For $\mathcal{M} = \zo^n$ and the binary Hamming metric,
Using Ash~\cite[Lemma 4.7.2, Equation 4.7.5, p. 115]{ash2012information} one has
\begin{align} nh_2(t/n)  -1/2\log{n} - 1/2 \le \log{|B_t|} \le  nh_2(t/n)\label{eq:size of balls}.\end{align}
and thus, 
\[
\Hfuzz(W) \le n(1-h_2(t/n)).
\]

\begin{definition}
Let $(\mathcal{M}, \dis)$ be a metric space where the size of balls is center independent.  The $\beta$ density is
\[
\beta := \log{\frac{|\mathcal{M}|- |B_t|}{|B_t|}} 
\]
\end{definition}
\begin{claim} 
For the binary Hamming metric 
\[
n(1-h_2(t/n))-\frac{\log{n}}{2}-1 \le \beta \le n(1-h_2(t/n)).
\]

\end{claim}

\begin{proof}
By Equation~\ref{eq:size of balls} one has: 
\begin{align*}
\beta &= \log{\frac{2^n - |B_t|}{|B_t|}} \\&= \log{\frac{2^n}{|B_t|} -1} \\&\ge \log{2^{n(1-h_2(t/n)) - 1/2 \log{n}-1/2} -1} \\&\ge n(1-h_2(t/n)) - 1/2\log{n}-1\\
\beta&\le \log{2^{n(1-h_2(t/n))} -1} \le n(1-h_2(t/n)).
\end{align*}
\end{proof}


Our main results show the impossibility of constructing fuzzy extractors for the family of distributions $\mathcal{W}$ of $2^k$ random points sampled from $\zo^n$ (without replacement).  Unfortunately, there are distributions $W_Z$ for some values $Z$ that do not have fuzzy min-entropy so there is no reason to expect a fuzzy extractor to be able to secure these distributions.  However, below we show a closely related family of distributions where all $W_Z$ have fuzzy min-entropy that is statistically close to $k$ random points.

\input{sampling}

%\begin{definition}[Markov's Inequality]
%    Markov's inequality is a tail bound for random variables that gives an upper bound on the probability of a random variable deviating from its mean. Let $\Pr[X>0] = 1$. Then the following inequality holds for any $\alpha > 0$: 
%    \[ 
%      \Prob{X \geq \alpha \cdot \Ex{X}} \leq 1/\alpha .
%    \]
%\end{definition}

\subsection{Fuzzy Extractors and Secure Sketches}
\begin{definition}[Secure Sketch~\cite{dodis2008fuzzy}]
For metric space $(\mathcal{M}, \dis)$, a $(\mathcal{M}, \mathcal{W}, \tilde{m}, t, \epsilon, \delta, \ell)$-\emph{secure sketch} is a pair of algorithms $(\sketch, \rec)$ with the following properties 
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, then \[\Pr_{ss \leftarrow \sketch(w)}[\rec(w', ss) = w]\ge 1-\delta.\]
\item \textbf{Security} For all distributions $W \in \mathcal{W}$ it holds that $\Haveps(W | \sketch(W)) \ge \tilde{m}$.
\item \textbf{Space Bounded} The circuits $\sketch$ and $\rec$ require at most $\ell$ bits to describe.  That is, $|\sketch| +|\rec|\le \ell$.
\end{enumerate}
\end{definition}

\paragraph{The use of smooth min-entropy} In the above, the secure sketch is required to retain smooth conditional min-entropy of $W$ conditioned on the sketch.  Many definitions consider $\epsilon=0$ or average min-entropy.  However, smooth min-entropy is the necessary condition for privacy amplification through the use of an average-case randomness extractor~\cite{renner2005simple} so it captures whether the non-interactive information-reconciliation task is possible. 

\begin{definition}[Fuzzy Extractor~\cite{dodis2008fuzzy}]
For metric space $(\mathcal{M}, \dis)$, a $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon)$-\emph{fuzzy extractor} is a pair of algorithms $(\gen, \rep)$ with the following properties 
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, then 
\[\Pr_{r, p \leftarrow \gen(w)}[\rep(w', p) = r]=1.\] 
\item \textbf{Security} For all distributions $W \in \mathcal{W}$, let $R, P \leftarrow \gen(W)$ and $U_\kappa$ be a uniformly distributed random variable over $\zo^\kappa$ it holds that \[\Delta((R, P), (U_\kappa, P))\le \epsilon.\]
\item  \textbf{Space Bounded} The circuits $\gen$ and $\rep$ require at most $\ell$ bits to describe.  That is, $|\gen| +|\rec|\le \ell$.
\end{enumerate}
\end{definition}

%\subsection{Conditional Min-Entropy Technical Results}
%It has been shown that universal fuzzy extractors are impossible in the information theoretic setting. 
%
%\subsection{Average Conditional Min-Entropy Loss}
%
%
%\begin{proof}
%    Since each $X_i$ is independent then $\minent{\vec{X}} = \sum \minent{X_i}$.
%    Now, by definition,
%    \begin{align*}
%        \acminent{\vec{X}}{Y} &= -\log{\Exlim{y \leftarrow Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x}\ |\ Y = y}}} \\
%        &= -\log{\sum\limits_{y} \max\limits_{\vec{x}} \Prob{\vec{X} = \vec{x} \ |\ Y = y} \cdot \Prob{Y = y}}\\
%        &= -\log{\sum\limits_{y} \max\limits_{\vec{x}} \Prob{\vec{X} = \vec{x} \vee Y = y}}\\
%        &\geq -\log{\sum\limits_{y} \max\limits_{\vec{x}, y'} \Prob{\vec{X} = \vec{x}  \wedge Y = y'}}\\
%        &= -\log{2^{\hart{Y}} \cdot 2^{\minent{\vec{X},Y}}}\\
%        &= \minent{\vec{X},Y} - \hart{Y}\\
%        &\geq \minent{\vec{X}} - \hart{Y} \\   
%        &= \sum \minent{X_i} - \hart{Y}
%    \end{align*}
%\end{proof}
%
%\subsubsection{Markov Bound for Predictability}
%Markov bounds are tail bounds that use Markov's Inequality to bound the probability that a random variable deviates significantly from its expected value. In Markov's inequality, we necessarily lose a multiplicative factor (here called $alpha$) in order to control the probability of the event occuring. When discussing entropy, we are dealing with a log scaled value which makes losing multiplicative factors costly. Instead, we can perform a Markov bound on the predictability scale. In this case, rather than lose a multiplicative factor in entropy, we lose a multiplicative factor in predictability which translates to a small number of bits of entropy lost for the controlled outcomes.
%
%

%\begin{align*}
%    \acminent{\vec{X}}{Y} &= \Delta\\
%    -\log{\Exlim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}}} &= \Delta\\
%    \Exlim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} &= 2^{-\Delta}\\
%    \Problim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y} \geq \alpha \cdot 2^{-\Delta}} &\leq \frac{1}{\alpha} \\
%    \Problim{Y}{\log{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} \geq \log{\alpha} -\Delta} &\leq \frac{1}{\alpha} \\
%    \Problim{Y}{-\log{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} < \Delta -\log{\alpha}} &\leq \frac{1}{\alpha} \\
%    \Problim{y\leftarrow Y}{\minent{\vec{X}\, |\, Y=y} < \Delta -\log{\alpha}} &\leq \frac{1}{\alpha}
%\end{align*}
    

\subsubsection{New Fuzzy Extractor Definitions}
Fuzzy extractors and secure sketches with advice. 

\begin{definition}[Secure Sketch with distributional advice]
\label{def:ss distributional}
Let $\mathcal{W}$ be a family of distributions indexed by $z$.  That is, denote each distribution in $\mathcal{W}$ as $W_Z$ with $Z$ describing the probability mass function of $W$.  
For metric space $(\{0,1\}^n, \dis)$, a $(\{0,1\}^n, \mathcal{W}, \tilde{m}, t, \epsilon, \delta, \ell)$-\emph{secure sketch with distributional advice} is a triple of algorithms $(\gen, \rep, \aux)$ with the following properties:
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, let \[\Pr_{(r, p) \leftarrow \sketch(w)}[\rec(w', p) = w]\ge 1-\delta.\]
\item \textbf{Security} Let $\aux$ be a deterministic function with output in $\zo^\ell$.  For all distributions $W_Z \in \mathcal{W}$, define $\advise:= \aux(Z)$, let $ss \leftarrow \sketch(W, \advise)$ then $\Haveps(W| ss) \ge \tilde{m}$.
\end{enumerate}
\end{definition}


\begin{definition}[Fuzzy Extractor with distributional advice]
\label{def:fe distributional}
Let $\mathcal{W}$ be a family of distributions indexed by $z$.  That is, denote each distribution in $\mathcal{W}$ as $W_Z$ with $Z$ describing the probability mass function of $W$.  
For metric space $(\{0,1\}^n, \dis)$, a $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-\emph{fuzzy extractor with distributional advice} is a triple of algorithms $(\gen, \rep, \aux)$ with the following properties:
\begin{enumerate} 
\itemsep0em
\item \textbf{Correctness} For all $w, w'$ such that $\dis(w, w')$, let \[\Pr_{(r, p) \leftarrow \gen(w)}[\rep(w', p) = r]=1.\]
\item \textbf{Security} Let $\aux$ be a deterministic function with output in $\zo^\ell$.  be For all distributions $W_Z \in \mathcal{W}$, define $\advise:= \aux(Z)$, let $(R, P) \leftarrow \gen(W, \advise)$ and $U_\kappa$ be a uniformly distributed random variable over $\zo^\kappa$ it holds that \[\Delta((R, P, Z), (U_\kappa, P, Z))\le \epsilon.\]
\end{enumerate}
\end{definition}


\begin{lemma}

Let $\mathcal{W}$ be a distribution family indexed by the random family $Z$ and suppose that no $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon, \ell)$-fuzzy extractor with distributional advice exists.  There is some distribution $W_Z \in \mathcal{W}$ for which no  $(\{0,1\}^n, \{W_Z\}, \kappa, t, \epsilon, \ell)$ efficient fuzzy extractor exists.
\label{lem:distributional advise suffices}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:distributional advise suffices}]
We proceed by contrapositive.  Suppose that for every $W_Z\in\mathcal{W}$ there exists an efficient fuzzy extractor.  We denote these algorithms by $(\gen_Z, \rep_Z)$ respectively.  Across $Z$ let $\ell$ represent the maximum space needed to implement some algorithm $\gen_Z$ or $\rep_Z$. We now describe how to build the fuzzy extractor $(\gen, \rep, \advise)$ with distributional advice.  Let $(\gen_Z, \rep_Z) := \advise(Z)$ which has length at most $\ell$.

Then define $\gen(x, C)$ as follows: 1) interpret $C$ as two circuits $\gen', \rep'$ and output $\gen'(x)$.  Define $\rep(x, p, C)$ interpret $C$ as two circuits $\gen', \rep'$ and output $\rep'(x', p)$.  It is clear that $(\gen, \rep, \advise)$ is a $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon, 2\ell)$ fuzzy extractor with distributional advise.
\end{proof}

\begin{lemma}
Let $\mathcal{W}$ be a distribution family indexed by the random family $Z$ and suppose that no $(\mathcal{M}, \mathcal{W}, \kappa, t, \epsilon, \delta, \ell)$-secure sketch with distributional advice exists.  Then there must be some distribution $W_Z \in \mathcal{W}$ for which no  $(\{0,1\}^n, \{W_Z\}, \kappa, t, \epsilon, \delta, \ell)$ efficient fuzzy extractor exists.
\label{lem:distributional advise suffices ss}
\end{lemma}

\noindent
The proof of Lemma~\ref{lem:distributional advise suffices ss} is identical to the proof of Lemma~\ref{lem:distributional advise suffices} and is omitted.

\subsection{Upper bound for size of Fuzzy Extractors and Secure Sketches}
Fuller, Reyzin, and Smith~\cite{fuller2016fuzzy,fuller2020fuzzy} show that the size of a fuzzy extractor can be upper bound in a general case.  We present a stronger version of their lemma that is contained in their proof.  The difference is we bound the union of viable points across difference values of $\mathsf{key}$ while they only bound the size of a single $\mathsf{key}$ corresponding to the true point.  Their argument is purely geometric, so it also applies to fuzzy extractors with distributional advice. 
We restate their lemma here for the completeness. 

\begin{lemma}[Lemma 5.2 in \cite{fuller2020fuzzy}]
    \label{lem:smallgeneralviable}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, and $\epsilon > 0$. 
    Suppose $(\mathsf{Gen, Rep})$ is a $(\mathcal{M,W},\kappa, t, \ell, \epsilon)$-fuzzy extractor with distributional advice for some distribution family $\mathcal{W}$ over $\mathcal{M}$. 
    For any fixed $p$, for any value $\advise \in \zo^\ell$, there is a set $\mathsf{GoodKey}_p \subseteq \zeroone{\kappa}$ of size at least $2^{\kappa - 1}$ such that for every $\mathsf{key} \in \goodkey_p$,
    \[
       \sum_{\mathsf{key} \in \goodkey_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v, \advise)}}|}\right) \leq n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}.
    \]   
\end{lemma}

\begin{lemma}[Lemma 7.3 in \cite{fuller2020fuzzy}]
\label{lem:smallgeneralviable ss}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric. Suppose $(\sketch, \rec)$ is a $(\zo^n, \mathcal{W}, \tilde{m}, t)$ secure sketch with error $\delta$, for some family $\mathcal{W}$. Then for every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
    \log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{2\delta}.
    \]
\end{lemma}



