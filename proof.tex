%!TEX root = main.tex

% Proof for NegFE

\section{Efficient Information-Theoretic Fuzzy Extractors do not exist for most distributions with fuzzy min-entropy}
\label{sec:fe}
\input{family}
We also summarize notation in Table~\ref{tab:notation}.

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters. 
\begin{enumerate}
\itemsep0em
\item Let $\alpha = \log{{2^n\choose 2^k}}$,
\item That $t<n/2$,
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =  n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}$.
\end{enumerate}
For a $1/4$ of the values $z\in\pcodeset$ there is no $(\{0,1\}^n, W_z, \kappa, t, \epsilon, \ell)$-fuzzy extractor for 
$\epsilon<1/3 - (\epsilon_1+\epsilon_2+\epsilon_3)/3.$ 
For 
\begin{align*}\log{\epsilon_1}&:= -\left(\kappa+\frac{\alpha -\ell}{2^k} - \mu -2k+\log{\nu}\right),\\
\epsilon_2&:=\frac{\nu+1}{2^{\kappa-1}}\\
\epsilon_3&:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^{n-1}.
\end{align*}
\label{thm:main theorem}
\end{theorem}

\begin{table}
\centering
\begin{tabular}{r | l}
Notation & Meaning\\\hline
$k$ & log size support of input distribution\\
$\ell$ & Length of $\advice$ and circuit size of fuzzy extractor\\
$\tilde{m}$ & Residual min-entropy of fuzzy extractor conditioned on helper\\
$n$ & Dimension of input points\\
$P/p$ & Public Helper value of fuzzy extractor\\
$R/r$ & Key of fuzzy extractor\\
$\sketch/ss$ & Public Helper value of secure sketch\\
$t$ & Distance for correction\\
$B_t$ & Hamming Ball of radius $t$\\
$U$ & Uniform distribution\\
\multirow{2}{*}{$\epsilon$} & Statistical Distance Parameter for fuzzy extractor,\\
& Smoothness parameter for min-entropy of secure sketch\\
$\alpha = \log{{2^n\choose 2^k}}$ & Log size of number of uniform distributions with $2^k$ points.  \\
$\beta$ & Ratio of Metric Space Size to Size of Hamming Ball (Def~\ref{def:b density})\\
$\gamma$ & lower bound on fuzzy min-entropy of distributions \\
$\kappa$ & key length of fuzzy extractor\\
$\nu$ & Number of points that adversary fully describes in advice\\
$\mu =  n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}$ & Log of maximum number of viable points (Lem~\ref{lem:smallgeneralviable})\\
\end{tabular}
\caption{Summary of notation}
\label{tab:notation}
\end{table}
Lemma~\ref{lem:distributional advice suffices} states that to show the hardness of building an efficient fuzzy extractor (Definition~\ref{def:fe}) for a constant fraction of $W_z\in \pcodeu$ it suffices to show the hardness of building a fuzzy extractor with distributional advice (Definition~\ref{def:fe distributional}) for the family $\pcodeu$. Thus, the proof of Theorem~\ref{thm:main theorem} focuses on the distributional advice setting.
Our proof uses the following structure:
\begin{enumerate}

\item $\pcodeu$ is complicated to work with, as a first step, we show that few elements in $\znkset$ that are not in $\pcodeset$. Lemma~\ref{lem:close family} shows that the statistical distance between $\wnkz$ and $\pcodeuz$ is small.  This shows that the hardness of building a fuzzy extractor with distributional advice for the family $\wnk$ implies hardness for $\pcodeu$. For the remainder of the proof, we consider $\wnk$.
\item Lemma~\ref{lem:convert distinguisher} shows one cannot build a fuzzy extractor with distribution advice for $\wnk$. Proving this requires several steps
\begin{enumerate}
\item Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string. We switch to considering a fixed value of $\advice$.  At the end of the proof we average across the distribution of $\advice$.

\item Claim~\ref{clm:entr of members} the majority of the support of $\znk | \aux = \advice$ is difficult to predict and thus unlikely to be included in the set of viable points by a small amount. We call such points \textbf{Hard Points}.  There are some points that the adversary has a large amount of information on that we call \textbf{Free Points}.

\item Corollary~\ref{corollary:info loss} puts together the above two steps to show that the adversary includes a small number of points from the particular distribution $W_z$ in the viable set. 

\end{enumerate}

Since the construction cannot align the viable points with the distribution there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

\noindent
The rest of this section is organized as follows:
\begin{description}
\item[Section~\ref{ssec:proof overview}] We present and prove Lemmas~\ref{lem:close family} and \ref{lem:convert distinguisher},
\item[Section~\ref{ssec:proof main theorem}] We present the proof of Theorem~\ref{thm:main theorem} combining Lemmas~\ref{lem:close family} and \ref{lem:convert distinguisher}, and 
\item[Section~\ref{ssec:analysis params}] We present our preferred setting of parameters.
\end{description}

\subsection{Main Technical Lemmas and Proofs}
\label{ssec:proof overview}



\input{sampling}

Fuller, Reyzin, and Smith~\cite{fuller2016fuzzy,fuller2020fuzzy} bound the number of points that could have produced the output of a fuzzy extractor.  Such points are called viable.  We present a stronger version of their lemma that is contained in their proof.  The difference is we bound the union of viable points across different values of $\mathsf{key}$ while they only bound the size of a single $\mathsf{key}$ corresponding to the true point.  Their argument is purely geometric, so it also applies to fuzzy extractors with distributional advice. 
We state the stronger version of \cite[Lemma 5.2]{fuller2020fuzzy}.

\begin{lemma}
    \label{lem:smallgeneralviable}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, $\epsilon > 0, \ell\in\mathbb{Z}^+$. 
    Suppose $(\mathsf{Gen, Rep})$ is a $(\mathcal{M,W},\kappa, t, \ell, \epsilon)$-fuzzy extractor with distributional advice for some distribution family $\mathcal{W}$ over $\mathcal{M}$. 
    For any fixed $p$, for any value $\advice \in \zo^\ell$, there is a set $\mathsf{GoodKey}_p \subseteq \zeroone{\kappa}$ of size at least $2^{\kappa - 1}$ such that,
    \[
      \mu:= \sum_{\mathsf{key} \in \goodkey_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v, \advice)}}|}\right) \leq n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}.
    \]   
\end{lemma}

\noindent
We now present our main technical lemma and its proof. 
\begin{lemma}
\label{lem:convert distinguisher}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, $\epsilon > 0, \ell\in\mathbb{Z}^+$. For the family $\wnk$.
    There is no $(\mathsf{Gen, Rep})$ that is a $(\mathcal{M},\wnk,\kappa, t, \ell, \epsilon)$-fuzzy extractor with distributional advice for
\[
\epsilon<1/2-\left(\epsilon_1+\epsilon_2\right)\\
\]
where 
\begin{align*}
\epsilon_1&= 2^{-\kappa-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2&=\frac{\nu+1}{2^{\kappa}},\\
  \mu &\leq n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}},\\
\alpha &= \log{{2^n\choose 2^k}}.
\end{align*}
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $r, p, z$ that are correctly generated by the fuzzy extractor.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher}]
Before proceeding we introduce some additional notation.
 Let $(\gen, \rep, \aux)$ be a fuzzy extractor with distributional advice. Let $a$ be some string.  For a tuple $(v, p, r, a)$ define 
\[
\viable(v, p, r,a) = \begin{cases} 1& \Pr[\gen(v, a) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
 Recalling that $|\znkset| = 2^\alpha = {2^n\choose 2^k}$.  Let $A$ be a random variable denoting $\aux(\znk)$.  Thus,  
\begin{align*}
\Hoo(\znk)&=\alpha,\\
\Hav(\znk|A)&\ge \alpha - \ell.
\end{align*}
Define 
\begin{align*}
2^{-\alpha_{a,w}}:&=\Pr_{z\leftarrow \znk | A=a}[w\in \supp(W_z)],\\
\alpha_a:&=\Hav(\znk|A=a)=\max_{W\subseteq \zo^n | |W|=2^k}\left(\prod_{w\in W} 2^{-\alpha_{a,w}} \right).
\end{align*}  
 Note that \[\expe_{a\leftarrow A} \left(2^{-\alpha_a} \right) =2^{-\Hav(\znk|A)}\le 2^{-(\alpha-\ell)}.\] 

Claim~\ref{clm:entr of members} bounds how much information $a$ contains about the points in the distribution.  Note that we consider a fixed $a$, considering the expectation across $a\in A$ after Corollary~\ref{corollary:info loss}.   Define the notation
$\hviable(w, p, \key, a, \easy)$
\begin{enumerate}
\item 
$0$ if $ w\not\in \easy$ or $\forall z\in\supp(\znk|A=a), w\not\in\supp(W_z)$,
\item $
\Pr_{z\leftarrow \znk |A =a}\left[\viable(w, p, \mathsf{key}, a) = 1 \wedge w\in \supp(W_z)\right]$ otherwise.
\end{enumerate}

\begin{claim}
Let $\mu$ and $\goodkey$ be defined as in Lemma~\ref{lem:smallgeneralviable}.
Let $A$ be a distribution and let $a$ be a fixed value such that $\Hoo(\wnk|A=a) =\alpha_a$.  Fix some value $w$ and some value $p$. Define $\goodkey_p$ as in Lemma~\ref{lem:smallgeneralviable}.  Each value $w^*\in\zo^n$ defines a set $\easy_{a, w^*}$ where $|\easy_{a,w^*}| \le \nu+1$ such that
\begin{align*}
\log{\sum_{\key\in\goodkey_p}\hviable(w, p, \key,a, \easy_{a,w^*})}\le -\frac{\alpha_a}{2^k}+1+\mu+k-\log{\nu}.
\end{align*}

\label{clm:entr of members}
\end{claim}

\begin{proof}[Proof of Claim~\ref{clm:entr of members}]
Let $A$ be an random variable over $\zo^\ell$.
Fix some value $a$.  Let $w_{1, a},...,w_{2^k, a}$ denote an arbitrary subset of $\zo^n$ of  size $2^k$. Then 
\[
\sum_{i=1}^{2^k} \alpha_{a, w_{i,a}}\le \alpha_a.
\]
Then one has that 
\[\expe_{i\leftarrow U_{\zo^k}} \left(\alpha_{a, w_{i,a}} \right)\le \frac{\alpha_a}{2^k}.\]

\label{clm:entropy distributes}
%\end{claim}

%\begin{proof}
%We note that the value of $\znk$ is fully specified by its $2^k$ values it in its support.  Thus, 
%\[
%\expe_{a\leftarrow A} \max_{w_1,..., w_{2^k}}\Pr_{z\leftarrow \znk}[w_1,...., w_{2^k} \in \supp(W_z) ]\le 2^{-\Hav(\znk|A)} \le 2^{-(\alpha-\ell)}.
%\]
%Let $i\le 2^k$, for any sequence of distinct  $w_1,..., w_i$ and let $A$ be an arbitrary random variable. It holds that 
%\[
%\expe_{a\leftarrow A} \max_{w_i^*}\Pr_{z\leftarrow \znk}[w_i^*\in \supp(W_z) | a \wedge w_1,..., w_{i_1} \in \supp(W_z)]\ge \expe_{a\leftarrow A}\max_{w_i^*}\Pr_{z\leftarrow \znk}[w_i^*\in \supp(W_z) | a].
%\]
%%It is true that for all distinct $w_{z,1},..., w_{z,i}$ that 
%%\[
%%\Hav(W_{z,i} | \advice, W_{z, 1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}) \le \Hav(W_{z,i} | \advice).
%%\]
%%We note that for all values $x, y\in \zo^n$, 
%%\[
%%\Pr_{z\leftarrow \znk}[x\in \supp(W_z) | y\in \supp(W_z)] = \begin{cases} \frac{2^k}{2^n-1} &x\neq y\\1&x=y.\end{cases}
%%\]
% This is because conditioned on $w_i \in \supp(W_{z})$ restricts the other points to be sampling from $\zo^n\setminus w_i$ values increasing the probability of all other outcomes proportionally.  Note this is true for an arbitrary conditioning on $A$.  
%We now proceed to the proof of the claim. 
%Suppose Claim~\ref{clm:entropy distributes} is false. Then there exists the following predictor $\mathcal{P}$ for whether $w_1,..., w_k$ are in the support of $W_z$ conditioned on $a\in\zo^\ell$:
%\begin{enumerate}
%\item Input $a$.
%\item For $i=1$ to $k$: choose $w_{i}$ that is most likely to be in $\supp(W_z) | a \wedge w_1,..., w_{i-1} \in \supp(W_z)$.
%\item Output the joint prediction $w_{1},..., w_{k}$.  
%\end{enumerate}
%
%We seek to show that if $\sum_{i=1}^{2^k} <\alpha - \ell$ one has a contradiction. 
%The probability that $\mathcal{P}$ issues a correct prediction is
%\begin{align*}
%&\expe_{a\leftarrow A}\left(\sum_{i=1}^{2^k}\left( \max_{w_i}\Pr_{z\leftarrow \znk}[w_i \in \supp(W_z)| | a, w_1,..., w_{i-1}\in \supp(W_z)]\right) \right)\ge\\
%&\expe_{a\leftarrow A}\left(\sum_{i=1}^{2^k}\left( \max_{w_i}\Pr_{z\leftarrow \znk}[w_i \in \supp(W_z)| | a]\right) \right) = 2^{-\alpha_i}
% \prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advice)} = \prod_{i=1}^{2^k} 2^{-\alpha_i}  = 2^{-\sum_{i=1}^{2^k} \alpha_i}.
%\end{align*}
%
%\noindent
%Suppose that $\sum_{i=1}^{2^k} \alpha_i < \alpha-\advice$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advice)}$, a contradiction.  Thus, \[\sum_{1\le i\le k} \frac{\alpha_i}{2^k} \ge (\alpha-\advice)/2^k.\] 
%This completes the proof of Claim~\ref{clm:entropy distributes}.
%\end{proof}

\noindent
We need an elementary lemma which states that not too many of $\alpha_{a, w_i}$ are much larger than $\alpha_a/2^k$:
\begin{claim}
    \label{lem:markovpred} %Fix some value $a$.
    Define $\easy_a \subset \{w_{1,a},..., w_{2^k, a}\}$ as \[\easy_a =\left\{w \middle| \alpha_{a,w} \ge \frac{\alpha_{a}}{2^k} -(k-\log{\nu})\right\}.\] %as the set of all 
    %Let $\vec{X} = (X_1, X_2, \ldots, X_{2^k})$ be numbers. Suppose that \[\expe_i[\Pr(X_i | Y=y)] \ge \Delta.\] 
    Then 
    \[
    \left|\easy_a\right|\le \nu.
    \]
\end{claim}
\begin{proof}[Proof of Claim~\ref{lem:markovpred}]
By Markov's inequality
\begin{align*}
    \Problim{i\leftarrow U_{\zo^k}}{\Pr_{z\leftarrow \znk | A=a}[w_{i,a}\in \supp(W_z)] \ge \beta \expe_i  \Pr_{z\leftarrow \znk | A=a}[w_{i,a}\in \supp(W_z)]}&=\\
        \Problim{i\leftarrow U_{\zo^k}}{\Pr_{z\leftarrow \znk | A=a}[w_{i,a}\in \supp(W_z)] \ge \beta 2^{-\alpha_{a,w}}}
    &\le 1/\beta.
    \end{align*}
    Setting $\beta = 2^k/\nu$ implies the statement of the Claim.
%%    {\expe_{y}\left( \max\limits_{x} \Prob{X_i = x \,|\, Y = y}\right) \geq \alpha \cdot 2^{-\Delta}} \leq \frac{1}{\alpha} 
%%  \]
%  Which implies that 
%      \[  
%    \Problim{i}{-\log{\expe_y\left(\max\limits_{x} \Prob{X_i = x \,|\, Y = y}\right)} \leq \Delta -\log{\alpha}} \leq \frac{1}{\alpha} 
%    \]
%      and finally
%        \[
%      \Problim{i}{\Hav(X_i\, |\, Y)< \Delta -\log{\alpha}} \leq \frac{1}{\alpha}
%      \]
%
%The statement of the lemma follows by setting $\alpha = 2^k/\nu$.
\end{proof}
\noindent
We know continue with the proof of Claim~\ref{clm:entropy distributes}. By Claim~\ref{lem:markovpred} it is true that there exists a set $\easy_a\subseteq \zo^n$ of size at most  $\nu$ where  such that for all $w\not \in \easy_a$ is true that $\alpha_{a, w}< \alpha_a/2^k+(k-\log{\nu}).$
Let $w^*$ denote the point that will be given to $\gen$ that is $(\mathsf{key}, p)\leftarrow \gen(w^*, a)$.  We define the set $\easy_{a,w^*} = \easy_a \cup \{w^*\}$.
Then,
\begin{align*}
&\Pr_{z\leftarrow \znk| A=a \wedge \mathsf{key}\in \goodkey, p}[w\in \supp(W_z) | w\not \in \easy_{a,w^*}] \ge\\
&\Pr_{z\leftarrow \znk| A=a \wedge w^*\in \supp(W_z)}[w\in \supp(W_z) | w\not \in \easy_{a,w^*}]\ge 2^{-\left(\frac{\alpha_a}{2^k}-k+\log{\nu}-1\right)}.
\end{align*}
This is because conditioning on a single bit that $w^*\in \supp(W_z)$ increases the predictability of an random variable by at most a factor of $2$. 
We now proceed to bounding $\hviable(w, p, \key, a, \easy_{a,w^*})$.  By assumption, By Lemma~\ref{lem:smallgeneralviable} we know that there are at most $2^\mu$ points in $\viable(w, p, \key, a)$.  Thus, by union bound over the set of viable points,
\[
\hviable(w, p, \key, a, \easy_{a,w^*}) \le 2^{-\frac{\alpha_a}{2^k}+1+\mu+k-\log{\nu}}.
\]
\noindent
This completes the proof of Claim~\ref{clm:entr of members}.
\end{proof}


\begin{corollary}
\label{corollary:info loss}
Let $\mu$ and $\goodkey$ be defined as in Lemma~\ref{lem:smallgeneralviable}.  Fix an arbitrary point $w^*\in \zo^n$ and some $a$ and define $\easy_{a,w^*}$ as in Claim~\ref{clm:entr of members}.  By Claim~\ref{clm:entr of members} on average across $z\leftarrow \znk| A=a$ (by union bound across the points in $W_z$) one has that:
\begin{align*}
\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle| \begin{aligned}w\in \supp(W_z)\\w\not\in \easy_{a,w^*} \\\exists \key\in\goodkey_p, \viable(w,p,\key,a)\end{aligned} \right\} \right| \le 2^{-\frac{\alpha_a}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
Then on average across $a\in A$ (using the fact that $\expe_{a\leftarrow A} \left(2^{-\alpha_a} \right) \le 2^{-(\alpha-\ell)}.$) one has 
\begin{align*}
\expe_{a\leftarrow A}\left(\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle| \begin{aligned}w\in \supp(W_z)\\w\not\in \easy_{a,w^*} \\\exists \key\in\goodkey_p, \viable(w,p,\key,a)\end{aligned} \right\} \right| \right)\le 2^{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, 
\begin{align*}
\expe_{a\leftarrow A}\left(\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle| \begin{aligned}w\in \supp(W_z)\\\exists \key\in\goodkey_p, \viable(w,p,\key,a)\end{aligned} \right\} \right| \right)\le 2^{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
\end{corollary}

\noindent
With Corollary~\ref{corollary:info loss} in hand we are ready to prove Lemma~\ref{lem:convert distinguisher}.
We prove the result for an average member of $Z$.
Consider the following distinguisher $\mathcal{D}$ for triples of the form $r, p, z$:
\begin{enumerate}
\item If $r \not\in \mathsf{GoodKey}_p$ output $1$,
\item If $\sum_{w\in\supp(W_z)} \viable(w, r, p, \advice(z)) =0 $ output $0$,
\item Else output $1$.
\end{enumerate}
First note that by perfect correctness it is always the case that when given $\mathsf{key}, p$ that $\mathcal{D}$ outputs $1$.  We proceed to bound the probability that $\mathcal{D}$ outputs $1$ when given $U_\kappa, p$.  Note that the probability that $\Pr[U_\kappa \in \mathtt{GoodKey}_p] \ge 1/2$ by the definition of $\mathtt{GoodKey}_p$. 

We bound the number of parts with at least one point in viable.  We begin by assuming that all points in viable are in different  values $r$ so the bound on the size of 
\[
\left\{ 
w\middle| \begin{aligned}w\in \supp(W_z)\\\exists \key\in\goodkey_p, \viable(w,p,\key,a)\end{aligned} \right\}\]
gives a bound on the number of keys for which $\mathcal{D}$ could output $1$. By Corollary~\ref{corollary:info loss} 
\[
\expe_{a\leftarrow A}\left(\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle| \begin{aligned}w\in \supp(W_z)\\\exists \key\in\goodkey_p, \viable(w,p,\key,a)\end{aligned} \right\} \right| \right)\le 2^{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\]
Thus the fraction of non-empty parts in $\mathsf{Goodkey}_p$ on average is at most 
\[
2^{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\]
Thus, the probability that $\mathcal{D}$ outputs $0$ when given $U_\kappa, p, z$ is at least 
$1/2-(\epsilon_1+\epsilon_2)$
where 
\begin{align*}
\epsilon_1:&=2^{-\kappa-\frac{\alpha-\ell}{2^k}+2+\mu+2k-\log{\nu}},\\
\epsilon_2:&=\frac{\nu+1}{2^{\kappa-1}}.
\end{align*}
\noindent
This completes the Proof of Lemma~\ref{lem:convert distinguisher}.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:main theorem}}
\label{ssec:proof main theorem}
\begin{proof}[Proof of Theorem~\ref{thm:main theorem}]
Define the pair of random variables $(R_{\znk},P_{\znk}) \leftarrow \gen(\wnkz, \advice(\znk))$.
Restating Lemma~\ref{lem:convert distinguisher} one has that 
\[
\Delta((R_{\znk}, P_{\znk}, \znk, (U_\kappa, P_{\znk}, \znk))\ge 1/2-(\epsilon_1+\epsilon_2).
\]
Let $\mathcal{D}$ be one distinguisher that always outputs $1$ on any value $\mathsf{key}, p, z$ for any distribution $W_z$, then
\begin{align*}
\Pr[\mathcal{D}((R_{\znk}, P_{\znk}, \znk))=1] &=1\\
\Pr[\mathcal{D}(U_\kappa, P_{\znk}, \znk)=1]&\le 1/2+(\epsilon_1+\epsilon_2).
\end{align*}
Recall that $\Delta(\wnkz, \pcodeuz) \le \epsilon_3$ by Lemma~\ref{lem:close family} .  Define the pair of random variables \[(R_{\pcode}, P_{\pcode})\leftarrow \gen(\pcodeuz, \advice(\pcode)).\]
by the information processing lemma it is thus true that 
\begin{align*}
\Pr[\mathcal{D}(R_{\pcode}, P_{\pcode}, \pcode)=1]&=1,\\
\Pr[\mathcal{D}(U_n, P_{\pcode}, \pcode)=1]&\le 1/2+(\epsilon_1+\epsilon_2+\epsilon_3).
\end{align*}
Finally, the theorem follows by application of Lemma~\ref{lem:distributional advice suffices} with the setting of $\zeta = 1/4$.
\end{proof} 



\subsection{Analysis of parameters}
\label{ssec:analysis params}
We separately consider $\epsilon_1, \epsilon_2$ and $\epsilon_3$. We refer to these three terms as hard points, free points, and distributional closeness respectively. This is because $\epsilon_1$ describes how much information the $\advice$ has about hard points in $W_Z$, $\epsilon_2$ considers a small number of points that are more thoroughly described by $\advice$, and $\epsilon_3$ controls the statistical distance between $U_{n, k}$ and $\mathtt{PCode}_{n, k, t, \alpha}$.  We consider parameters in order of simplicity (rather than index ordering).



\subsubsection{Free Points - $\epsilon_2$}
For $\epsilon_2$ to be negligible it suffices that $\nu/2^\kappa$ is negligible.  Note that for the fuzzy extractor to have meaningful security requires $\kappa = \omega(\log{n})$.  We set
\begin{description}
\item[Condition 1] $\nu = 2^{c_{\kappa}\kappa}$ for some constant $0 < c_{\kappa}<1$.
\end{description} 
This yields 
\[
\epsilon_2= \frac{\nu+1}{2^{\kappa-1}} = 2^{(c_{\kappa}-1)\kappa-\kappa+1} = \ngl(n).
\]
\subsubsection{Distributional Closeness - $\epsilon_3$}
Recall that $\epsilon_3:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^{n+1}$.  Consider the following settings:
\begin{description}
\item[Condition 2] That $\gamma\le \beta-\log{2e}$, which implies $2^{\gamma - \beta} \le \frac{1}{2e}$, and 
\item[Condition 3] For constant $0<c_{|k|} < 1$, we set $k = \gamma + c_{|k|}n$ which implies $2^{k-\gamma}\ge n+1+\omega(\log{n})$. 
\end{description}
Together, these settings imply that 
\begin{align*}
\epsilon_3&=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^{n+1}\\
&\le\left(\frac{1}{2}\right)^{n+\omega(\log{n})+1}2^{n+1}= 2^{-\omega(\log{n})} = \ngl(n).
\end{align*}

\paragraph{Discussion}
By Proposition~\ref{prop:max fuzz ent} for any $W$ in $\zo^n$ it is true that \[
\Hfuzz(W)\le n\left(1-h_2\left(\frac{t}{n}\right)\right)+ \frac{\log{n}}{2} +1/2.\]  Thus, the additional constraint that 
\[
\Hfuzz(W):=\gamma \le \beta - \log{2e}
\le n\left(1-h_2\left(\frac{t}{n}\right)\right)+\frac{\log{n}}{2}+\frac{1}{2}-\log{2e}.\] imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 


\subsubsection{Hard Points - $\epsilon_1$} 
We now turn to our analysis of $\epsilon_1$.  Recall that $\log{\epsilon_1}:= -\left(\kappa+\frac{\alpha -\ell}{2^k} - \mu -2k+\log{\nu}\right).$
Recall that 
\begin{align*}
(n/k)^k &\le {n\choose k} < ((ne)/k)^k\\
\mu&\le nh_2(1/2-t/n),\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
Recall that $\nu = 2^{c_\kappa \kappa}$.
This implies that 
\begin{align*}
-\log{\epsilon_1}&= \kappa+\frac{\alpha-\ell}{2^k} - \mu -2k+\log{\nu}\\
&\ge  \kappa+\frac{\alpha -\ell}{2^k} - nh_2(1/2-t/n) - 2k+\log{\nu},\\
&\ge  \kappa+\frac{(n-k)2^k-\ell}{2^k} - nh_2(1/2-t/n) - 2k+c_{\kappa}\kappa,\\
&\ge  (1+c_{\kappa})\kappa+\frac{(n-k)2^k-2k2^{k}-\ell}{2^k} - nh_2(1/2-t/n)\\
&>  (1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) .
\end{align*}
\noindent
We now focus on showing a parameter setting when $\psi:=\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n)\ge 0$.
\begin{description}
\item[Condition 4]
Let $0< c_\ell<1$ be a parameter such that 
$
\ell\le 3c_{\ell} n2^k,$
\item[Condition 5] 
Suppose that 
\[
\gamma \le \frac{n(1-3c_{|k|} -c_{\ell}- h_2(1/2-t/n))}{3}.
\]
\end{description}
\noindent
Then it holds that 
\begin{align*}
\psi:&=\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) \\
&\ge   \frac{(n-3k)2^k-c_{\ell}n2^k}{2^k} - nh_2(1/2-t/n)\\
&\ge   \frac{n(1-3(\gamma/n+c_{|k|}) -c_{\ell}2^k)}{2^k} - nh_2(1/2-t/n)\\
&\ge n(1-3c_{|k|}-c_{\ell}) - 3\gamma - nh_2(1/2-t/n) \ge 0
\end{align*}
which suffices to ensure that 
\[
\log{\epsilon_1} \le  -\left((1+c_{\kappa})\kappa+\psi \right) \le  -(1+c_{\kappa})\kappa = -\omega(\log{n}).
\]
\subsubsection{Overall Parameters}
Combining Conditions 2 and 5 one obtains a negligible statistical distance as long as for constants $c_\kappa, c_{|k|}, c_{\ell}\in (0,1)$ one has:
\begin{align*}
\nu &= 2^{c_{\kappa}\kappa},\\
k &=\gamma + c_{|k|}n,\\
\ell&\le 3c_{\ell}n2^k,\\
0\le \frac{\gamma}{n} &\le \min\left\{(1-h_2(t/n)) +\frac{\log{n}+1-2\log{2e}}{2n}, \frac{1-3c_{|k|} - c_{\ell}-h_2(1/2-t/n)}{3}\right\}.\\
\end{align*}
%
%\paragraph{Setting of $\kappa = \Theta(n)$}
%
%In this setting,
%\begin{description}
%\item[Condition 4b] Set 
%\begin{align*}
%\ell&\le  3(c_{k,1}-c_{k,2})n2^{\gamma+c_{k,1}n}
%\end{align*}
%\item[Condition 5b] Let $c_{\kappa^*} n = \kappa+\log{\nu}$ and 
%\begin{align*}
%\gamma&\le \frac{n}{3}\left(1+c_{\kappa^*} -3c_{k,2}-h_2(1/2-t/n)\right) - \omega(\log{n})
%\end{align*}
%\todo{this needs to be fixed}
%\end{description}
%Together these conditions yield  \[ (1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) = \omega(\log n)\] 


\section{Secure Sketches}
\label{sec:ss}
This section creates an upper bound on the quality of efficient secure sketches.  This bound considers has stronger parameters than Theorem~\ref{thm:main theorem} due largely to the difference between Lemmas~\ref{lem:smallgeneralviable} and~\ref{lem:smallgeneralviable ss}.

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters.
\begin{enumerate}
\itemsep0em
\item The $\beta$-density is at least $1$ which is  satisfied as long as $t< n/2 $ (see Definition~\ref{def:b density}),
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =(n(1-h_2(t/n)) +h_2(2\delta))/(1-2\delta)$.
\end{enumerate}
For a $2^{-\tilde{m}}$ fraction of the distributions in the family $\pcodeu$ (indexed by $z\in\pcodeset$) there is no $(\{0,1\}^n, W_z, \tilde{m}, t, \epsilon,\delta, \ell)$-secure sketch for 
\[
\tilde{m}\ge  -\log{1-2\epsilon'} +1 + 2\max\left\{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
where $\epsilon' = \epsilon+\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.$
\label{thm:main theorem ss}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:main theorem ss}]

Fuller, Reyzin, and Smith~\cite[Lemma 7.3]{fuller2020fuzzy} showed that good secure sketches are bounded in size as they imply good Shannon error correcting codes.  This result holds true if one considers a secure sketch that retains smooth min-entropy with no loss in parameters because it only relies on the correctness of the secure sketch (not the security property).  

\begin{lemma}
\label{lem:smallgeneralviable ss}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric. Let $\wnk$ be a family indexed by set $\znkset$ and let $\znk$ denote the uniform distribution over $\znk$.  Suppose $(\sketch, \rec, \aux)$ is a $(\zo^n, \mathcal{W}, \tilde{m}, t, \epsilon_\sketch, \ell, \delta)$-secure sketch with distribution advice. 
   For every $v\in \zo^n$ and any value $a \in \zo^\ell$ there exists a set $\goodsketch_{v,a}$ where $\Pr[\sketch(v, a)\in \goodsketch_{v,a}] \ge 1/2$ and for any fixed $ss$,
    
    \[
   \mu:= \log{|\{v \in \zo^n | ss \in \goodsketch_{v,a}\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
\end{lemma}

\noindent
Define $\alpha :=\Hoo(\znk)$ and note that $\Hav(\znk|A)\ge \alpha-\ell$ and define $\alpha_a:=\Hav(\znk|A=a)$.
 Note that \[\expe_{a\leftarrow A} \left(2^{-\alpha_a} \right) =2^{-\Hav(\znk|A)}\le 2^{-(\alpha-\ell)}.\] 
For a triplet $(v, ss, a)$ define $\viable(v, ss, a, z)=1$ if 
\begin{enumerate}
\item $\Pr[\sketch(v, a) = ss]>0$,
\item $ss\in\goodsketch_{v,a}$, and 
\item $v\in\supp(W_z)$.
\end{enumerate} Otherwise set $\viable(v, ss, a, z)=0$.  Define $\viable(v, ss, a) = \Pr_{z\leftarrow \znk|A=a}[\viable(v,ss,a,z)=1]$. Define $\hviable(v, ss, a, \easy) = \viable(v,ss, a)$ if $v\not\in \easy$ and $0$ otherwise.
We present an analog of Claim~\ref{clm:entr of members} adapted to the secure sketch setting. 

\begin{claim}
Let $\mu$ and $\goodsketch$ be defined as in Lemma~\ref{lem:smallgeneralviable ss}.
Let $A$ be a distribution and let $a$ be a fixed value such that $\Hoo(\wnk|A=a) =\alpha_a$.  Fix some value $v$.  Each value $w^*\in\zo^n$ defines a set $\easy_{a, w^*}$ where $|\easy_{a,w^*}| \le \nu+1$ such that
\begin{align*}
\log{\sum_{ss}\hviable(v,ss, a, \easy_{a,w^*})}\le -\frac{\alpha_a}{2^k}+1+\mu+k-\log{\nu}.
\end{align*}

\label{clm:ent members ss}
\end{claim}
The proof of Claim~\ref{clm:ent members ss} follows the structure of the proof of Claim~\ref{clm:entr of members} and is omitted. 
Claim~\ref{clm:ent members ss} suffices to bound how many points are ``viable'' from the output of the secure sketch.

\begin{corollary}
\label{corollary:info loss ss}
Let $\mu$ be defined as in Lemma~\ref{lem:smallgeneralviable ss}.  Fix an arbitrary point $w^*\in \zo^n$, some $ss$ and some $a$. Define $\easy_{a,w^*}$ as in Claim~\ref{clm:ent members ss}.  By Claim~\ref{clm:ent members ss} on average across $z\leftarrow \znk| A=a$ (by union bound across the points in $W_z$) one has that:
\begin{align*}
\log{\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle| \begin{aligned} w\not\in \easy_{a, w^*} \\\viable(w,ss, a, z)\end{aligned} \right\} \right|} \le -\frac{\alpha_a}{2^k}+1+\mu+2k-\log{\nu}.
\end{align*}
Then on average across $a\in A$ (using the fact that $\expe_{a\leftarrow A} \left(2^{-\alpha_a} \right) \le 2^{-(\alpha-\ell)}.$) one has 
\begin{align*}
\log{\expe_{a\leftarrow A}\left(\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle| \begin{aligned} w\not\in \easy_{a, w^*} \\\viable(w,ss, a, z)\end{aligned} \right\} \right| \right)}\le -\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}.
\end{align*}
And finally, 
\begin{align*}
\expe_{a\leftarrow A}\left(\expe_{z\leftarrow \znk| A=a} \left|\left\{ 
w\middle|\viable(w, ss, a, z)\right\} \right| \right)\le 2^{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
\end{corollary}

\begin{lemma}
\label{lem:convert distinguisher ss}
Let all parameters be as in Corollary~\ref{corollary:info loss ss} with $\nu \in\mathbb{Z}^+$.  For the family $\wnkz$ there is no $(\{0,1\}^n, \wnkz, \tilde{m}, t, \epsilon, \ell, \delta)$-secure sketch with distributional advice if
\[
\tilde{m}> -\log{1-2\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
\noindent
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $w, ss, z$ that are correctly generated by the secure sketch. 
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher ss}]
First recall for every $v\in \zo^n$ and advice string $a$  there exists a set $\goodsketch_{v, a}$ where $\Pr[\sketch(v, a) \in\goodsketch_{v,a}]\ge 1/2$. 
We first need an elementary claim which says that just predicting $w$ when the sketch is in good sketch implies a predictor for the full setting with only a single bit of loss. 
\begin{claim}
Let $(X, Y)$ be a pair of random variables and, $S(X, Y)$ be a set, let $f$ be a randomized function taking inputs on the domain of $(X, Y)$. Then \label{clm:smooth ent cond}
\[
\Haveps(X|Y, f(X, Y) \in S(X, Y))\ge \Haveps(X|Y) + \log{ \Pr[f(X, Y) \in S(X, Y)]}.
\]
\end{claim}
\begin{proof}[Proof of Claim~\ref{clm:smooth ent cond}]
Let $X', Y'$ be a distribution such that $\Delta((X, Y), (X', Y')) \le \epsilon$.  By \cite[Lemma 7.8]{fuller2020fuzzy} for any event $\eta$
\begin{align*}
\Hav(X'| Y', \eta) \ge \Hav(X' | Y') +\log{\Pr[\eta]}.
\end{align*}
Let $\eta$ denote the event that $f(X, Y) \in S(X, Y)$.
The proof completes by noting that $\Delta((X, Y), (X', Y'))\le \epsilon$ implies that 
\[
\Delta((X, Y, f(X, Y) \overset{?}\in S(X, Y)), (X', Y,' f(X', Y')\overset{?}\in S(X', Y')))\le \epsilon\] by the information processing lemma. This is turn implies that 
\[
\Haveps(X|Y, f(X, Y) \in S(X, Y))\ge \Haveps(X|Y) +\Pr[f(X, Y) \in S(X, Y)].
\]
This completes the proof of Claim~\ref{clm:smooth ent cond}.
\end{proof}

We now proceed with the proof of Lemma~\ref{lem:convert distinguisher ss}. Define $\mathcal{D}(v, ss, z) =1$ if $ss\not\in\goodsketch_v$ or $\viable(v, ss, \aux(z), z) =1$.  $\mathcal{D}$ outputs $0$ otherwise.  Note that for $w, ss, z$ correctly generated as the output of $\sketch$ $\mathcal{D}$ always outputs $1$. Denote 
\begin{align*}
A&:= \advice(\znk)\\
X&:=\wnkz,\\
Y&:=\sketch(\wnkz, A).
\end{align*}
 By Claim~\ref{clm:smooth ent cond}:
\[
\Haveps(X| Y, Z) \le \Haveps(X| Y,Z, X \in \goodsketch_{Y, A}) +1.
\]
In the above note that $A$ and thus $\goodsketch_{\wnk, A}$ are computable from the pair $\wnk, Z$ since $\aux$ is deterministic. 
Let $X', Y', Z'$ be a triple of random variables where 
\begin{align*}
\Delta((X, Y, Z), (X', Y', Z')) &\le \epsilon.
\end{align*} Our goal is to bound the min-entropy of $X', Y', Z' | X'\in\goodsketch_{Y', \aux(Z')}$. Noting that by Claim~\ref{clm:smooth ent cond} the smooth min-entropy without conditioning on this event increases by at most $1$.
\[
\Pr[\viable(W, \sketch(W), Z) = 1| \sketch_W \in \goodsketch_W] \ge 1/2.
\]
First note that \[\Pr_{(x, y, z) \leftarrow X', Y', Z' }[\mathcal{D}(x,y,z)=1| X'\in\goodsketch_{Y', \aux(Z')}]\ge 1-2\epsilon.\]
Let $A':= \aux(Z')$.  When $X'\in\goodsketch_{Y', \aux(Z')}$ in order for $\mathcal{D}$ to output $1$ it must be the case that $\viable(x', y', \aux(z'), z') =1$. That is, the support of $x'$ must be drawn from points in $W_z'$.  For any fixed value of $y\in Y$ and arbitrary random variable $A'$ of length at most $\ell$ by Corollary~\ref{corollary:info loss ss} the number of such $x'$ is at most $2^{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.$ For any fixed support the min-entropy is maximized by considering the uniform distribution over such points. \begin{align*}
\Hav&(X'|Y' ,Z', X'\in\goodsketch_{Y', \aux(Z')}, \mathcal{D}(X', Y', Z')=1)\\ &\le \log {2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1}\\
&\le 2\max\left\{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\end{align*}

One then has, 
\begin{align*}
&2^{-\Hav(X'|Y', Z',X'\in\goodsketch_{Y', \aux(Z')})} \\&\ge \Pr_{x, y, z\leftarrow (X', Y', Z')} [\mathcal{D}(x, y, z)= | X'\in\goodsketch_{Y', \aux(Z')}] 2^{-\Hav(X'|Y', Z', X'\in\goodsketch_{Y', \aux(Z')}, \mathcal{D}(X', Y', Z')=1) }\\&+ \Pr_{x, y, z\leftarrow (X', Y', Z') } [\mathcal{D}(x, y, z)=0| X'\in\goodsketch_{Y', \aux(Z')}] 2^{-\Hav(X'|Y', Z', X'\in\goodsketch_{Y', \aux(Z')}, \mathcal{D}(X', Y', Z')=0) }\\
 &\ge  (1-2\epsilon)2^{-\Hav(X'|Y',Z', X'\in\goodsketch_{Y', \aux(Z')},  \mathcal{D}(X', Y', Z')=1) }
\end{align*}
And thus, 
\[
\Hav(X'|Y', Z', X'\in\goodsketch_{Y', \aux(Z')}) \le  -\log{1-2\epsilon} +\Hav(X'|Y', X'\in W_Z).
\]
This implies that 
\[
\Haveps(\wnkz| \sketch(\wnkz, A), \znk) \le -\log{1-2\epsilon} +1 + 2\max\left\{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
This completes the Proof of Lemma~\ref{lem:convert distinguisher ss}.
\end{proof}

\noindent
We now proceed to the proof of Theorem~\ref{thm:main theorem ss}.   Let \[\tilde{m}:= -\log{1-2\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha+-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.\]  Restating Lemma~\ref{lem:convert distinguisher ss} one has that 
\[
\Hav^{\epsilon_\sketch}(\wnkz |  \sketch(\wnkz, A), \znk) \le \tilde{m}.
\]
That is, for all $E, F,G$ such that $\Delta((E, F, G), (\wnkz,  \sketch(\wnkz, A), \znk)\le \epsilon_{\sketch}$ it is true that $\Hav(E|F,G) \le \tilde{m}$.  Define \begin{align*}A'&:=\aux(\pcode),\\
SS'&:=\sketch(\pcodeuz, A'),\\
\epsilon_{\wnkz}&:= \left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.
\end{align*}
By Lemma~\ref{lem:close family} $\Delta(\wnkz, \pcodeuz) \le \epsilon_{\wnkz}$ and thus
$
\Hav^{\epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}}(\pcode | SS', \pcodeuz)\le \tilde{m}.
$
Suppose not, then there exists some $E, F, G$ where 
\[
\Delta((E, F, G), (\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}.
\]
and $\Hav(E|F, G)>\tilde{m}$.
Thus, 
\begin{align*}
\Delta&((E, F, G), (\wnkz ,  \sketch(\wnkz, A), \znk))\\
&\le \Delta((E, F, G), ((\pcode , SS', \pcodeuz))+\Delta(\znk, \pcode)\\
&\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}+ \epsilon_{\mathtt{PCode}} = \epsilon_{\sketch}.
\end{align*}
This contradicts the fact that $\Hav^{\epsilon_\sketch}(\wnkz | \sketch(\wnk), \znk)) \le \chi.$
Finally, Theorem~\ref{thm:main theorem ss} follows by application of Lemma~\ref{lem:distributional advice suffices ss} with setting $\zeta = \chi$ and noting that $\chi\ge 1$.

\end{proof}


\subsection{Proof of Lemma~\ref{lem:convert distinguisher ss}}
\label{ssec:ss proof}


\subsection{Analysis of parameters}
We assume that $\epsilon\le 1/8$ and $\delta<1/4$.  As before for  
$
\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n$ to be negligible it suffices that
\begin{description}
\item[Condition 1] That $\gamma \le \beta-\log{2e}.$\footnote{As in Section~\ref{ssec:analysis params} the additional constraint that 
$
\gamma \le \beta - \log{2e}$ 
imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 
}
\item[Condition 2] Let $0<c_k<1$ be some arbitrary constant and suppose that $k= \gamma+c_kn$ which implies that $k\ge \gamma + \log{n+\omega(\log{n})}$.
\end{description}
These two conditions imply that $\epsilon'\le 1/2$ and $-\log{1-\epsilon'}\le 1$.
\begin{description}
\item[Condition 3] That  $\nu = 1$.
\end{description}
Define \[\chi: =-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}=-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k\]
We now turn to our analysis of $\chi$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le \frac{(n(1-h_2(t/n)) +h_2(2\delta))}{(2\delta)},\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
This implies that 
\begin{align*}
\chi&= -\frac{\alpha-\ell-k}{2^k} + \mu +2k+1\\
&\le  -\frac{\alpha +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}  + 2k+1,\\
&\le - \frac{(n-k)2^k +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta} + 2k+1,\\
&\le  -\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1.
\end{align*}
We consider two settings for $\delta$ one when $\delta<1/4$ and another when $\delta=0$.

\paragraph{Constant error, $\delta<1/4$} 
As long as for constants $c_k, c_\ell$ one has
\begin{align*}
\ell&\le 3c_\ell n2^k,\\
\delta &< 1/4,\\
0\le\frac{\gamma}{n} &\le \min\left\{(1-h_2(t/n)) +\frac{\log{n}+1-2\log{2e}}{2n}, \frac{2}{3}h_2(t/n)-\frac{1}{3}-\frac{4c_k+c_\ell}{3}-\frac{2}{3n}\right\}.
\end{align*} then 
\begin{align*}
\chi\le-&\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1\\
&\le -\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1 \\
&\le -\frac{(n-3k)2^k-\ell-k}{2^k} + 2n(1-h_2(t/n)) +2\\
 &\le -(n-(4c_k+c_\ell)n - 3\gamma) + 2n(1-h_2(t/n)) +2\\
&\le -n+(4c_k+c_\ell)n+3\gamma + 2n(1-h_2(t/n))+2 \\\
&\le n+(4c_k+c_\ell)n+3\gamma -2nh_2(t/n))+2 \le 0\\
\end{align*}
then $\tilde{m} \le 2+2\max\{\chi, \log{2}\} \le 4$ which implies that $1/16$ of the distributions have no secure sketch.

\paragraph{No error, $\delta=0$}
When $\delta = 0$ one has
\begin{align*}
\ell&\le 3c_\ell n2^k,\\
\delta &< 1/4,\\
0\le\frac{\gamma}{n} &\le \min\left\{(1-h_2(t/n)) +\frac{\log{n}+1-2\log{2e}}{2n}, \frac{1}{3}h_2(t/n)-\frac{4c_k+c_\ell}{3}-\frac{2}{3n}\right\}.
\end{align*} 
yielding $\tilde{m}\le 4$ which implies that $1/16$ of the distributions have no secure sketch.


%\begin{align*}
%2^{(n-k)2^k}+2^{2k}-|\advice|-k - 2^knh_2(1/2-t/n) \ge 0\\
%\end{align*}
%Or equivalently that, 
%\begin{align*}
%|\advice| &\le 2^{(n-k)2^k+2k} -k - 2^knh_2(1/2-t/n)\\
%&\le 2^{\Theta(n)2^{\Theta(n)}}.
%\end{align*}


