%!TEX root = main.tex

% Proof for NegFE

\section{Main Result}
\begin{theorem}
Fix $(\gen, \rep, \advise)$, let $n, \kappa, t, \ell $ be parameters.  Let $\mathcal{W} = \{W_Z\}$ be a family of distributions where for each $W_Z, \Hfuzz(W) \ge \alpha$.  If $(\mathcal{M,W},\kappa, t, \epsilon, \ell)$-fuzzy extractor with auxiliary input where the following conditions are satisfied:
\begin{enumerate}
\itemsep0em
\item $\kappa = \omega(\log \lambda)$ \todo{how much really}
\item $\ell = \poly(n)$ \todo{how much really}
\item Some complicated relationship with key length and distance \todo{be precise}.
\item $\alpha$ is big enough.
\end{enumerate}
Then \[
\epsilon> ....\]
\end{theorem}

\subsection{Proof Sketch}
Our proof uses the following structure
\begin{enumerate}
\item We start by using Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string.
\item We then further restrict this setting by showing that in order for an adversary to succeed on average they have to be able to align these viable points with a distribution that they have only a single sample and a polynomial length advice sting. 
\item We then argue that for large high entropy distributions this advice string can only reduce the entropy of a large fraction of viable points by a small amount. 
\item Then, we show that this small reduction of entropy for each point in the distribution means that on average the adversary cannot align the viable points with the distribution and there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

\paragraph{The family $\mathcal{W}$}
We consider the following family $Z$ which chooses $k$ points without replacement from the space $\{0,1\}^n$.  That is, if one samples $Z$ uniformly then one obtains the distribution $U_{n,k}$. We note there are distributions $W_Z$ in this family that have little fuzzy min-entropy.  However, we will show that this family is statistically close to a family where every distribution has fuzzy min-entropy.  For the remainder of this proof we consider $U_{n,k}$ and then show that the fuzzy extractor cannot perform differently on the family $\mathtt{PCode}_{n, k, t, \alpha}^{*}$ where every distribution does have fuzzy min-entropy. For convenience we use $W_{z, 1},..., W_{z,k}$ to describe the distribution $k$ points with nonzero probability in $W_z$ for some value $z$.  We assume no ordering over these points.  For all values $x, y$, 
\[
\Pr[W_{z, i} =x | W_{z, j} = y] = \begin{cases} \frac{1}{2^n-1} &x\neq y\\0&x=y.\end{cases}
\]


\begin{lemma}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise \leftarrow \aux(Z)$ be of length $\ell$.  For a quad $v, p, r, \advise$ define 
\[
\viable(v, p, r, \advise) = \begin{cases} 1& \Pr[\gen(v, \advise) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
Fix some $p$ and let $\mathtt{GoodKey}$ be defined as in Lemma~\ref{lem:smallgeneralviable}. For each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\ell$, then it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\Pr\left[\viable(w_i, p, \mathsf{key}, \advise) = 1\middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(\mathsf{key}, p)\leftarrow \gen(w, \advise) \\\mathsf{key}\in\mathtt{GoodKey}\end{aligned} \right]< some value.
\]
\end{lemma}
\begin{proof}
Let $Z$ and $\advise$ be defined as above. We begin by noting that \[
\Hoo(W_{z, 1},..., W_{z, k}) = \log {n\choose k}\overset{def}=\alpha.\]
 It is thus, the case that 
\[
\Hav(W_{z, 1},..., W_{z, k} | \advise) \ge \alpha - |\advise|.
\]
\begin{claim}
One has that 
\begin{align*}
\expe_{i=1}^k \left[\Hav(W_{z, i} | \advise)\right] \ge \frac{\alpha - |\advise|}{k}.
\end{align*}
\label{clm:entropy distributes}
\end{claim}
\begin{proof}
We begin by noting that for any random variable $\advise$ it is true that 
\[
\Hav(W_{z,i} | \advise, W_{z, 1}=w_{z,1},..., W_{z,i-1}) \ge \Hav(W_{z,i} | \advise).
\]
This is because conditioned on $W_{z, j} =w_{z,j}$ only removes the outcome $w_{z,j}$ from the support of $W_{z,i}$ increasing all other outcomes proportionally.  

We now proceed to the proof of the claim
Suppose not, then there exists the following predictor $\mathcal{P}$ for the joint distribution $W_{z, 1},..., W_{z, k} | \advise$:
\begin{enumerate}
\item For $i=1$ to $k$ predict $w_{z,i}\leftarrow W_{z, i}| \advise, W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}$
\item Output the joint prediction $w_{z,1},..., w_{z, k}$.  
\end{enumerate}
Let $\alpha_i$ denote the values $\Hav(W_{z,i}\advise) = \alpha_i$ for $i=1$ to $k$. 
The probability that $\mathcal{P}$ issues a correct prediction is
\begin{align*}
\prod_{i=1}^k 2^{-\Hav(W_i | \advise,  W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1})} \ge \prod_{i=1}^k 2^{-\Hav(W_i | \advise)} = \prod_{i=1}^k 2^{-\alpha_i}  = 2^{-\sum_{i=1}^k \alpha_i}.
\end{align*}

Suppose that $\sum_{i=1}^k \alpha_i < \alpha-\advise$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advise)}$, a contradiction.  Thus, $\expe_{1\le i\le k} \alpha_i \ge (\alpha-\advise)/k$. 
This completes the proof of Claim~\ref{clm:entropy distributes}.
\end{proof}

Now by Lemma~\ref{lem:markovpred} it is true that there exists a set $\mathcal{I}\subseteq \{1,...,k\}$ where $| \mathcal{I} | \le k/\beta$ such that for all $i\not \in \{1,...,k\} \setminus\mathcal{I}$ is true that 
\[
\Hav(W_{z,i} | \advise) \ge \frac{\alpha -|\advise| - \log \beta}{k}.
\]

Let $i^*$ denote the index of the point that will be given to $\gen$ that is $p\leftarrow (w_{z,i^*}, \advise)$.  We define the set $\mathcal{I}_\advise = \mathcal{I} \cup \{w_{z,i^*}\}$ where $w$ is the point given to $\gen(w, \advise)$.  
Let \[\gamma:=\frac{\alpha -|\advise| - \log \beta}{k},\] 
then for $i\not\in \mathcal{I}_{\advise}$ it is true that 
\[
\Hav(W_{z,i} | \advise, W_{z,i^*}= w_{z,i^*}) \ge -\log{\frac{1}{2^{\gamma}-1}} \ge \gamma-1.
\]
We now proceed to bounding $\viable(w_i, p, \mathsf{key}, \advise)$.  By Lemma~\ref{lem:smallgeneralviable} we know that there are at 

\here
\end{proof}

\subsection{Proof Setting}
Consider $\mathcal{W}$ such that each $W \in \mathcal{W}$ is a set of $2^{\phi}$ uniformly chosen independent random points in $\zeroone{n}$. 
Let $|\mathcal{W}| = r$ and let $Z \in \br{r}$ be an indexing variable for the selection of $W_Z$ from $\mathcal{W}$. 

In this proof the goal is to show that a fuzzy extractor cannot hope to hide the input point from an outside party. 
We will call the party that is building the Fuzzy Extractor the constructor and the party attempting to distiguish two settings the distinguisher. 
The Fuzzy Extractor is set up with an enrolled sample from our two stage sampling proceedure before the distiguishing game begins.
The game that the distinguisher plays is given one of two triples, real or random, do distinguish the realm to which the triple belongs. 
The real triple is the key value corresponding to the enrolled value in the fuzzy extractor, the public value produced by Gen, and the description of the distribution $W_Z$.
The random triple is the same except the key value corresponding to the enrolled value is substituted with a uniform value in the domain of the key values.

In this setting the constructor when creating the Fuzzy Extractor is aided by another party, we call this party the advisor. 
The advisor gets a full description of the distribution $W_Z$ and is allowed unbounded computation and time to produce an advice string $\mathsf{info}$. 
The advisor and constructor are unbounded in their computation and time, but the length of this string is required to be polynomial in the security parameter $\lambda$. 
The advice string is then communicated to the constructor, who also gets a sample from the distribution $w \in W_Z$. 
The constructor then is tasked with creating a Fuzzy Extractor, specifically choosing a public value $\mathsf{pub}$ which induces a partition over the space $\zeroone{n}$ and a key labeling of the partition which determines a key value for the initial sample.

\subsection{Maximum size of a Fuzzy Extractor}
From Lemma \ref{lem:smallgeneralviable}, we have the largest size of a set of viable points is $2^{\psi}$ where $\psi = n \cdot \prns{1 - \frac{2}{\ln{2}} \cdot \tau^{2}}$.

Now, it is clear from our setting that when building the Fuzzy Extractor the constructor has a single point $w$, a full description of the family of distributions $\mathcal{W}$, and the prepared advice string $\mathsf{info}$ about the specific selected distribution $W_Z$. 
We are primarily concerned with the ability of the constructor to align the points in the selected $W_Z$, with the partition induced by $\mathsf{pub}$. 
Clearly, if $\mathsf{info}$ is allowed to describe every point in $W_Z$ (or every point but $w$) then the constructor can include a point from $W_Z$ in each viable section of the partition induced by $\mathsf{pub}$. 
Fortuntely for us, the distribution has exponential entropy and the advice string has polynomial length so the advice string cannot describe the entire remainder of the distribution. 
The question remains, how much entropy remains in the distribution after seeing the advice string? 

Since each point in the distribution is independent and uniform in $\zeroone{n}$ the beginning  entropy (and min-entropy) of the distribution is $|W_Z| \cdot n$. 
Now we consider the advice string; since this string is allowed to arbitrarily depend on the distribution we can upperbound the min-entropy of the distribution conditioned on the advice string using a standard min-entropy argument found in Lemma \ref{lem:conditionalminentloss}. 
\[
    \acminent{W_Z}{\mathsf{info}} = |W_Z| \cdot 2^n - \log{|info|}
\]