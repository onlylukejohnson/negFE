%!TEX root = main.tex

% Proof for NegFE

\section{Efficient Information-Theoretic Fuzzy Extractors do not exist for most distributions with fuzzy min-entropy}
\label{sec:fe}
\input{family}

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters. 
\begin{enumerate}
\itemsep0em
\item The $\beta$-density is at least $1$ which is  satisfied as long as $t< n/2 $ (see Definition~\ref{def:b density}),
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =  n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}$.
\end{enumerate}
For a $(1/4-(\epsilon_1+\epsilon_2+\epsilon_3)/2)$-fraction of the distributions $W\in \mathtt{PCode}_{n, k, t, \gamma}$ there is no $(\{0,1\}^n, W, \kappa, t, \epsilon, \ell)$-fuzzy extractor for 
\[
\epsilon< \frac{1}{3} - \frac{2(\epsilon_1+\epsilon_2+\epsilon_3)}{3}.
\]
For 
\begin{align*}\log{\epsilon_1}&:= -\left(\kappa+\frac{\alpha -\ell}{2^k} - \mu -2k+\log{\nu}\right),\\
\epsilon_2&:=\frac{\nu+1}{2^{\kappa+1}}\\
\epsilon_3&:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.
\end{align*}
\label{thm:main theorem}
\end{theorem}

Recall that Lemma~\ref{lem:distributional advice suffices} states that to show the hardness of building an efficient fuzzy extractor (Definition~\ref{def:fe}) for the family $\mathtt{PCode}_{n, k, t, \gamma}$ it suffices to show the hardness of building a fuzzy extractor with distributional advice (Definition~\ref{def:fe distributional}) for the family $\mathtt{PCode}_{n, k, t, \gamma}$. 

Our proof uses the following structure:
\begin{enumerate}

\item The family $\mathtt{PCode}_{n, k, t, \gamma}$ is complicated to work with.  Thus, as a first step we show it is statistically close to the family that picks $k$ points without replacement, which we denote as $U_{n,k}$.\footnote{For $U_{n,k}$, there are ${2^{n}\choose 2^k}$ outcomes each occurring with probability $1/{2^{n}\choose 2^k}$.}  Lemma~\ref{lem:close family} shows that the distribution $\mathtt{PCode}_{n, k, t, \gamma}$ is statistically close to the distribution $U_{n,k}$.  For the remainder of the proof we consider $U_{n,k}$.
\item Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string. 

\item Lemma~\ref{lem:entr of members} for the distribution, $U_{n,k}$ the advice string can only reduce the entropy of almost all viable points by a small amount. We call such points \textbf{Average Points}.  There are some points that the adversary has a large amount of information on that we call \textbf{Free Points}.

\item Corollary~\ref{corollary:info loss} puts together the above two Lemmas to show that the adversary includes a small number of points from the distribution in the viable set.

\item Lemma~\ref{lem:convert distinguisher} shows that since the construction cannot align the viable points with the distribution there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

\noindent
The rest of this section is organized as follows:
\begin{description}
\item[Section~\ref{ssec:proof overview}] We present the formal versions of the above statements,
\item[Section~\ref{ssec:proof main theorem}] We present the proof of Theorem~\ref{thm:main theorem}, 
\item[Section~\ref{ssec:analysis params}] We present our preferred setting of parameters, and
\item[Section~\ref{sec:tech lemmas}] We prove the above statements.
\end{description}

\subsection{Proof Overview}
\label{ssec:proof overview}
Lemma~\ref{lem:close family} shows that $\mathtt{PCode}_{n, k, t, \gamma}$ is statistically close to $U_{n,k}$.

\begin{lemma}
 \label{lem:close family}
Fix some $n\in \mathbb{Z}^+$ and let $t<n/2$.  Let $k, \gamma  \in \mathbb{Z}^+$.  Then one has that 
\[\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \gamma}) \le \left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.\]
\end{lemma}

Fuller, Reyzin, and Smith~\cite{fuller2016fuzzy,fuller2020fuzzy} bound the number of points that could have produced the output of a fuzzy extractor.  Such points are called viable.  We present a stronger version of their lemma that is contained in their proof.  The difference is we bound the union of viable points across different values of $\mathsf{key}$ while they only bound the size of a single $\mathsf{key}$ corresponding to the true point.  Their argument is purely geometric, so it also applies to fuzzy extractors with distributional advice. 
We restate the stronger version of \cite[Lemma 5.2]{fuller2020fuzzy}.

\begin{lemma}
    \label{lem:smallgeneralviable}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, $\epsilon > 0, \ell\in\mathbb{Z}^+$. 
    Suppose $(\mathsf{Gen, Rep})$ is a $(\mathcal{M,W},\kappa, t, \ell, \epsilon)$-fuzzy extractor with distributional advice for some distribution family $\mathcal{W}$ over $\mathcal{M}$. 
    For any fixed $p$, for any value $\advice \in \zo^\ell$, there is a set $\mathsf{GoodKey}_p \subseteq \zeroone{\kappa}$ of size at least $2^{\kappa - 1}$ such that for every $\mathsf{key} \in \goodkey_p$,
    \[
      \mu:= \sum_{\mathsf{key} \in \goodkey_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v, \advice)}}|}\right) \leq n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}.
    \]   
\end{lemma}

We consider the following family $Z$ which chooses $2^k$ points without replacement from the space $\{0,1\}^n$.  That is, if one samples $Z$ uniformly then one obtains the distribution $U_{n,k}$. 
For convenience we use $W_{z, 1},..., W_{z,2^k}$ to describe the $2^k$ points with nonzero probability in $W_z$ for some value $z$.  We assume no ordering over these points.  

Lemma~\ref{lem:entr of members} bounds how much information $\advice$ contains about the points in the distribution.  Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be a fuzzy extractor with distributional advice. Suppose, without of generali Let $\advice = \aux(Z)$ be of length $\ell$.  For a tuple $(v, p, r, \advice)$ define 
\[
\viable(v, p, r, \advice) = \begin{cases} 1& \Pr[\gen(v, \advice) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
Fix some $p$ and let $\mathtt{GoodKey}_p$ be defined as in Lemma~\ref{lem:smallgeneralviable}. 

\begin{lemma}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $\advice = \aux(Z)$ be of length $\ell$.
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  Let $\mu$ be defined as in Lemma~\ref{lem:smallgeneralviable}.  
For each value $\advice$ there is some set $\mathcal{I}_{\advice}$ of size at most $\nu+1$, it is true for each $i\not \in \mathcal{I}_\advice$ that
\[
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advice) = 1 \right]}\le -\frac{\alpha -|\advice| }{2^k}+1+\mu+k-\log{\nu}.
\]
\label{lem:entr of members}
\end{lemma}

\begin{corollary}
\label{corollary:info loss}
Suppose that 
\[
   \sum_{\mathsf{key}\in \mathtt{GoodKey}_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v)}}|}\right) \le 2^\mu 
 \]
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advice$ there is some set $\mathcal{I}_{\advice}$ of size at most $\nu+1$, then it is true for each $i\not \in \mathcal{I}_\advice$ that
\[
\Pr\left[\left(\sum_{\mathsf{key} \in \mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advice, z)\right) = 1\middle| \begin{aligned} z\leftarrow Z\\ \advice= \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advice) \end{aligned} \right]< 2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+k-\log{\nu} }.
\]
And thus, for on average across $Z$, the expected number of points outside of $\mathcal{I}_\advice$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i | w_{z,i}\not\in\mathcal{I}_\advice} \viable(w_{z,i}, p, \mathsf{key}, \advice) \middle| \begin{aligned} z\leftarrow Z\\ \advice\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advice)\end{aligned} \right]< 2^{-\frac{\alpha - |\advice|}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advice) \middle| \begin{aligned} z\leftarrow Z\\ \advice\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advice)\end{aligned} \right]< 2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}

\end{corollary}

\begin{lemma}
\label{lem:convert distinguisher}
Let all parameters be as in Corollary~\ref{corollary:info loss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-fuzzy extractor with distributional advice (see Definition~\ref{def:fe distributional}) if
\[
\epsilon<1/2-\left(\epsilon_1+\epsilon_2\right)\\
\]
where 
\begin{align*}
\epsilon_1&= 2^{-\kappa-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $r, p, z$ that are correctly generated by the fuzzy extractor.
\end{lemma}


\subsection{Proof of Theorem~\ref{thm:main theorem}}
\label{ssec:proof main theorem}
\begin{proof}[Proof of Theorem~\ref{thm:main theorem}]
Restating Lemma~\ref{lem:convert distinguisher} one has that for $(R_{U_{n,k}},P_{U_{n,k}}) \leftarrow \gen(U_{n,k}, \advice(Z_{U_{n,k}}))$
\[
\Delta((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}), (U_n, P_{U_{n,k}}, Z_{U_{n,k}}))\ge 1/2-(\epsilon_1+\epsilon_2).
\]
Let $\mathcal{D}$ be one distinguisher that always outputs $1$ on any value $\mathsf{key}, p, z$ for any distribution $z$, then
\begin{align*}
\Pr[\mathcal{D}((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}))=1] &=1\\
\Pr[\mathcal{D}(U_n, P_{U_{n,k}}, Z_{U_{n,k}})=1]&\le 1/2+(\epsilon_1+\epsilon_2).
\end{align*}
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_3$ by Lemma~\ref{lem:close family} .  Define \[(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\leftarrow \gen(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advice(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
it is thus true that 
\begin{align*}
\Pr[\mathcal{D}(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&=1,\\
\Pr[\mathcal{D}(U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&\le 1/2+(\epsilon_1+\epsilon_2+\epsilon_3).
\end{align*}
Where the second 
and thus that 
\[
\Delta((R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}), (U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\ge 1/2-(\epsilon_1+\epsilon_2+\epsilon_3).
\]

\noindent
Finally, the theorem follows by application of Lemma~\ref{lem:distributional advice suffices} with the setting of $\zeta = 1/4$.
\end{proof} 



\subsection{Analysis of parameters}
\label{ssec:analysis params}
We separately consider $\epsilon_1, \epsilon_2$ and $\epsilon_3$. We refer to these three terms as average points, free points, and distributional closeness respectively. This is because $\epsilon_1$ describes how much information the $\advice$ has about average points in $W_Z$, $\epsilon_2$ considers a small number of points that are more thoroughly described by $\advice$, and $\epsilon_3$ controls the statistical distance between $U_{n, k}$ and $\mathtt{PCode}_{n, k, t, \alpha}$.  We consider parameters in order of simplicity (rather than index ordering).



\subsubsection{Free Points - $\epsilon_2$}
For $\epsilon_2$ to be negligible it suffices that $\nu/2^\kappa$ is negligible.  Note that for the fuzzy extractor to have meaningful security requires $\kappa = \omega(\log{n})$.  We set
\begin{description}
\item[Condition 1] $\nu = 2^{c_{\kappa}\kappa}$ for some constant $0 < c_{\kappa}<1$.
\end{description} 
This yields 
\[
\epsilon_2= \frac{\nu+1}{2^\kappa} = 2^{(c_{\kappa}-1)\kappa}+ 2^{-\kappa} = \ngl(n).
\]
\subsubsection{Distributional Closeness - $\epsilon_3$}
Recall that $\epsilon_3:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n$.  Consider the following settings:
\begin{description}
\item[Condition 2] That $\gamma\le \beta-\log{2e}$, which implies $2^{\gamma - \beta} \le \frac{1}{2e}$, and 
\item[Condition 3] For some constant $0<c_{|k|} < 1$, we set $k = \gamma + c_{|k|}n$ which implies $2^{k-\gamma}\ge n+\omega(\log{n})$. 
\end{description}
Together, these settings imply that 
\begin{align*}
\epsilon_3&=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n\\
&\le\left(\frac{1}{2}\right)^{n+\omega(\log{n})}2^n= 2^{-\omega(\log{n})} = \ngl(n).
\end{align*}

\paragraph{Discussion}
By Proposition~\ref{prop:max fuzz ent} for any $W$ in $\zo^n$ it is true that \[
\Hfuzz(W)\le n\left(1-h_2\left(\frac{t}{n}\right)\right)+ \frac{\log{n}}{2} +1/2.\]  Thus, the additional constraint that 
\[
\Hfuzz(W):=\gamma \le \beta - \log{2e}
\le n\left(1-h_2\left(\frac{t}{n}\right)\right)+\frac{\log{n}}{2}+\frac{1}{2}-\log{2e}.\] imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 


\subsubsection{Average Points - $\epsilon_1$} 
We now turn to our analysis of $\epsilon_1$.  Recall that $\log{\epsilon_1}:= -\left(\kappa+\frac{\alpha -\ell}{2^k} - \mu -2k+\log{\nu}\right).$
Recall that 
\begin{align*}
(n/k)^k &\le {n\choose k} < ((ne)/k)^k\\
\mu&\le nh_2(1/2-t/n),\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
Recall that $\nu = 2^{c_\kappa \kappa}$.
This implies that 
\begin{align*}
-\log{\epsilon_1}&= \kappa+\frac{\alpha-\ell}{2^k} - \mu -2k+\log{\nu}\\
&\ge  \kappa+\frac{\alpha -\ell}{2^k} - nh_2(1/2-t/n) - 2k+\log{\nu},\\
&\ge  \kappa+\frac{(n-k)2^k-\ell}{2^k} - nh_2(1/2-t/n) - 2k+c_{\kappa}\kappa,\\
&\ge  (1+c_{\kappa})\kappa+\frac{(n-k)2^k-2k2^{k}-\ell}{2^k} - nh_2(1/2-t/n)\\
&>  (1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) .
\end{align*}
\noindent
We now focus on showing a parameter setting when $\psi:=\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n)\ge 0$.
\begin{description}
\item[Condition 4]
Let $0< c_\ell<1$ be a parameter such that 
$
\ell\le 3c_{\ell} n2^k,$
\item[Condition 5] 
Suppose that 
\[
\gamma \le \frac{n(1-3c_{|k|} -c_{\ell}- h_2(1/2-t/n))}{3}.
\]
\end{description}
\noindent
Then it holds that 
\begin{align*}
\psi:&=\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) \\
&\ge   \frac{(n-3k)2^k-c_{\ell}n2^k}{2^k} - nh_2(1/2-t/n)\\
&\ge   \frac{n(1-3(\gamma/n+c_{|k|}) -c_{\ell}2^k)}{2^k} - nh_2(1/2-t/n)\\
&\ge n(1-3c_{|k|}-c_{\ell}) - 3\gamma - nh_2(1/2-t/n) \ge 0
\end{align*}
which suffices to ensure that 
\[
\log{\epsilon_1} \le  -\left((1+c_{\kappa})\kappa+\psi \right) \le  -(1+c_{\kappa})\kappa = -\omega(\log{n}).
\]
\subsubsection{Overall Parameters}
Combining Conditions 2 and 5 one obtains a negligible statistical distance as long as for constants $c_\kappa, c_{|k|}, c_{\ell}\in (0,1)$ one has:
\begin{align*}
\nu &= 2^{c_{\kappa}\kappa},\\
k &=\gamma + c_{|k|}n,\\
\ell&\le 3c_{\ell}n2^k,\\
0\le \frac{\gamma}{n} &\le \min\left\{(1-h_2(t/n)) +\frac{\log{n}+1-2\log{2e}}{2n}, \frac{1-3c_{|k|} - c_{\ell}-h_2(1/2-t/n)}{3}\right\}.\\
\end{align*}
%
%\paragraph{Setting of $\kappa = \Theta(n)$}
%
%In this setting,
%\begin{description}
%\item[Condition 4b] Set 
%\begin{align*}
%\ell&\le  3(c_{k,1}-c_{k,2})n2^{\gamma+c_{k,1}n}
%\end{align*}
%\item[Condition 5b] Let $c_{\kappa^*} n = \kappa+\log{\nu}$ and 
%\begin{align*}
%\gamma&\le \frac{n}{3}\left(1+c_{\kappa^*} -3c_{k,2}-h_2(1/2-t/n)\right) - \omega(\log{n})
%\end{align*}
%\todo{this needs to be fixed}
%\end{description}
%Together these conditions yield  \[ (1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) = \omega(\log n)\] 

\subsection{Proof of Technical Lemmas}
\label{sec:tech lemmas}
\subsubsection{Proof of Lemma~\ref{lem:close family}}
\input{sampling}


\subsubsection{Proof of Lemma~\ref{lem:entr of members}}

\begin{proof}[Proof of Lemma~\ref{lem:entr of members}]
Let $Z$ and $\advice$ be defined as above. We begin by noting that 
\[
\Hoo(W_{z, 1},..., W_{z, 2^k})=\alpha.
\]
 It is thus, the case that 
\[
\Hav(W_{z, 1},..., W_{z, 2^ k} | \advice) \ge \alpha - |\advice|.
\]
For all values $x, y$, 
\[
\Pr[W_{z, i} =x | W_{z, j} = y] = \begin{cases} \frac{1}{2^n-1} &x\neq y\\0&x=y.\end{cases}
\]
\begin{claim}
One has that 
\begin{align*}
\sum_{i=1}^{2^k} \frac{\Hav(W_{z, i} | \advice)}{2^k} \ge \frac{\alpha - |\advice|}{2^k}.
\end{align*}
\label{clm:entropy distributes}
\end{claim}
\begin{proof}
It is true that $\forall w_{z,1},..., w_{z,i-1}$ that 
\[
\Hav(W_{z,i} | \advice, W_{z, 1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}) \le \Hav(W_{z,i} | \advice).
\]
This is because conditioned on $W_{z, j} =w_{z,j}$ only removes the outcome $w_{z,j}$ from the support of $W_{z,i}$ increasing all other outcomes proportionally.  
We now proceed to the proof of the claim. 

Suppose not, then there exists the following predictor $\mathcal{P}$ for the joint distribution $W_{z, 1},..., W_{z, k} | \advice$:
\begin{enumerate}
\item For $i=1$ to $k$ predict $w_{z,i}\leftarrow W_{z, i}| \advice, W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}$
\item Output the joint prediction $w_{z,1},..., w_{z, k}$.  
\end{enumerate}
Let $\alpha_i$ denote the values $\Hav(W_{z,i}| \advice) = \alpha_i$ for $i=1$ to $k$. 
The probability that $\mathcal{P}$ issues a correct prediction is
\begin{align*}
\prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advice,  W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1})} \ge \prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advice)} = \prod_{i=1}^{2^k} 2^{-\alpha_i}  = 2^{-\sum_{i=1}^{2^k} \alpha_i}.
\end{align*}

\noindent
Suppose that $\sum_{i=1}^{2^k} \alpha_i < \alpha-\advice$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advice)}$, a contradiction.  Thus, \[\sum_{1\le i\le k} \frac{\alpha_i}{2^k} \ge (\alpha-\advice)/2^k.\] 
This completes the proof of Claim~\ref{clm:entropy distributes}.
\end{proof}

\noindent
Before proceeding to the proof of Lemma~\ref{lem:entr of members} we need an elementary lemma:
\begin{lemma}
    \label{lem:markovpred}
    Let $\vec{X} = (X_1, X_2, \ldots, X_{2^k})$ be random variables and let $Y$ be a random variable arbitrarily correlated with $\vec{X}$. Suppose that $\expe_i[\Hoo(X_i | Y)] \ge \Delta$. Then 
    \[
    \left|\left\{X_i | \Hav(X_i |Y) < \Delta - (k-\log{\nu})\right\}\right|\le \nu.
    \]
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:markovpred}]
Let $\expe_i [\Hav(X_i|Y)] = \Delta$. Then by Markov's inequality
\[
    \Problim{i}{\expe_{y}\left( \max\limits_{x} \Prob{X_i = x \,|\, Y = y}\right) \geq \alpha \cdot 2^{-\Delta}} \leq \frac{1}{\alpha} 
  \]
  Which implies that 
      \[  
    \Problim{i}{-\log{\expe_y\left(\max\limits_{x} \Prob{X_i = x \,|\, Y = y}\right)} \leq \Delta -\log{\alpha}} \leq \frac{1}{\alpha} 
    \]
      and finally
        \[
      \Problim{i}{\Hav(X_i\, |\, Y)< \Delta -\log{\alpha}} \leq \frac{1}{\alpha}
      \]

The statement of the lemma follows by setting $\alpha = 2^k/\nu$.
\end{proof}
By Lemma~\ref{lem:markovpred} it is true that there exists a set $\mathcal{I}\subseteq \{1,...,2^k\}$ of size $\nu$ where  such that for all $i\not \in \{1,...,2^k\} \setminus\mathcal{I}$ is true that 
\[
\eta:=\Hav(W_{z,i} | \advice) \ge \frac{\alpha -|\advice|}{2^k} -\left(k-\log{\nu}\right).
\]
Let $i^*$ denote the index of the point that will be given to $\gen$ that is $p\leftarrow (w_{z,i^*}, \advice)$.  We define the set $\mathcal{I}_\advice = \mathcal{I} \cup \{w_{z,i^*}\}$ where $w$ is the point given to $\gen(w, \advice)$.  
Then for $i\not\in \mathcal{I}_{\advice}$ it is true that 
\begin{align*}
\Hav(W_{z,i} | \mathsf{key} \in \mathtt{GoodKey}, p)&\ge \\
\Hav(W_{z,i} | \advice, W_{z,i^*}= w_{z,i^*}) &\ge -\log{\frac{1}{2^{\eta}-1}} \ge \eta-1.
\end{align*}
We now proceed to bounding $\viable(w_i, p, \mathsf{key})$.  By assumption, By Lemma~\ref{lem:smallgeneralviable} we know that there are at most $2^\mu$ points in $\viable(w_i, p, \mathsf{key})$.  Thus, for all $i\not \in\mathcal{I}_{\advice}$ 

\begin{align*}
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advice) = 1 \right]}&\le - (\eta-1 -\mu) \\
&=-\left(\frac{\alpha -|\advice| }{2^k}-1-\mu-\left(k-\log{\nu}\right)\right).
\end{align*}
This completes the proof of Lemma~\ref{lem:entr of members}.
\end{proof}


\subsubsection{Proof of Lemma~\ref{lem:convert distinguisher}}

\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher}].
We prove the result for an average member of $Z$.
Consider the following distinguisher $\mathcal{D}$ for triples of the form $r, p, z$:
\begin{enumerate}
\item If $r \not\in \mathsf{GoodKey}_p$ output $1$,
\item If $\sum_{i} \viable(w_{z, i}, r, p, \advice(z)) =0 $ output $0$,
\item Else output $1$.
\end{enumerate}
First note that by perfect correctness it is always the case that when given $\mathsf{key}, p$ that $\mathcal{D}$ outputs $1$.  We proceed to bound the probability that $\mathcal{D}$ outputs $1$ when given $U_\kappa, p$.  Note that the probability that $\Pr[U_\kappa \in \mathtt{GoodKey}_p] \ge 1/2$ by the definition of $\mathtt{GoodKey}_p$. 

We bound the number of parts with at least one point in viable.  We begin by assuming that all points in viable are in different  values $r$ so the bound on the size of 
\[
\left\{w_{z, i} \middle|\sum_{\mathsf{key}\in\mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advice)>0 \wedge 1\le i \le k\right\}
\] 
gives a bound on the number of nonempty parts. By Corollary~\ref{corollary:info loss} 
\[
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advice) \middle| \begin{aligned} z\leftarrow Z\\ \advice\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advice)\end{aligned} \right]< 2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\]
Thus the fraction of non-empty parts in $\mathsf{Goodkey}_p$ on average is at most 
\[
2^{-\kappa-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\frac{\nu+1}{2^\kappa}.
\]
Thus, the probability that $\mathcal{D}$ outputs $0$ when given $U_\kappa, p, z$ is at least 
$1/2-(\epsilon_1+\epsilon_2)$
where 
\begin{align*}
\epsilon_1:&=2^{-\kappa-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2:&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
\noindent
This completes the Proof of Lemma~\ref{lem:convert distinguisher}.
\end{proof}

\section{Secure Sketches}
\label{sec:ss}
This section creates an upper bound on the quality of efficient secure sketches.  This bound considers has stronger parameters than Theorem~\ref{thm:main theorem} due largely to the difference between Lemmas~\ref{lem:smallgeneralviable} and~\ref{lem:smallgeneralviable ss}.

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters.
\begin{enumerate}
\itemsep0em
\item The $\beta$-density is at least $1$ which is  satisfied as long as $t< n/2 $ (see Definition~\ref{def:b density}),
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =(n(1-h_2(t/n)) +h_2(2\delta))/(1-2\delta)$.
\end{enumerate}
For $2^{-\tilde{m}}$ of the distributions $W\in \mathtt{PCode}_{n, k, t, \gamma}^{*}$ there is no $(\{0,1\}^n, W, \tilde{m}, t, \epsilon,\delta, \ell)$-secure sketch for 
\[
\tilde{m}\ge  -\log{1-\epsilon'} +1 + 2\max\left\{-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
where $\epsilon' = \epsilon+\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.$
\label{thm:main theorem ss}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:main theorem ss}]

Fuller, Reyzin, and Smith~\cite[Lemma 7.3]{fuller2020fuzzy} showed that good secure sketches are bounded in size as they imply good Shannon error correcting codes.  This result holds true if one considers a secure sketch that retains smooth min-entropy with no loss in parameters because it only relies on the correctness of the secure sketch (not the security property).  

\begin{lemma}
\label{lem:smallgeneralviable ss}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric. Suppose $(\sketch, \rec)$ is a $(\zo^n, \mathcal{W}, \tilde{m}, t, \epsilon_\sketch, \delta)$-secure sketch, for some family $\mathcal{W}$. 
    
%    Let $W\in \mathcal{W}$, and let $W', SS'$ be a distribution such that $\Delta((W, \sketch(W)), (W', SS'))\le \epsilon$ and $\Hav(W' |SS')\ge \tilde{m}$.
   For every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
   \mu:= \log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
\end{lemma}

We present an analog of Lemma~\ref{lem:entr of members} adapted to the secure sketch setting. Let $\goodsketch_v$ be defined as in Lemma~\ref{lem:smallgeneralviable ss}. For a triple $(v, ss, z)$ define $\viable(v, ss, z)=1$ if
\begin{enumerate}
\itemsep0em
\item $\Pr[\gen(v, \advice(z)) = ss]>0$,
\item $ss\in\goodsketch_v$, and
\item $\Pr[W_z = v]>0$.
\end{enumerate}
and set $\viable(v, ss, z)=0$ otherwise. 

\begin{lemma}
\label{lem:ent members ss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $\advice = \aux(Z)$ be of length $\ell$.
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  Let $\mu$ be defined as in Lemma~\ref{lem:smallgeneralviable ss}. 
For each value $\advice$ there is some set $\mathcal{I}_{\advice}$ of size at most $\nu+1$, it is true for each $i\not \in \mathcal{I}_\advice$ that
\[
\log{\Pr\left[\viable(w_{z,i}, ss, \advice) = 1 \right]}\le -\frac{\alpha -|\advice| }{2^k}+1+\mu+k-\log{\nu}.
\]
\end{lemma}

The proof of Lemma~\ref{lem:ent members ss} is identical to the proof of Lemma~\ref{lem:entr of members} and is omitted. 
Lemma~\ref{lem:ent members ss} suffices to bound how many points are ``viable'' from the output of the secure sketch.


\begin{corollary}
\label{corollary:info loss ss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an fuzzy extractor with distributional advice of length $\ell$.       For every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
    \mu:=\log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
For a triple $(v, ss, z)$ define $\viable(v, ss, z)=1$ if
\begin{enumerate}
\itemsep0em
\item $\Pr[\gen(v, \advice(z)) = ss]>0$,
\item $ss\in\goodsketch_v$, and
\item $\Pr[W_z = v]>0$.
\end{enumerate}
and set $\viable(v, ss, z)=0$ otherwise. 
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advice$ there is some set $\mathcal{I}_{\advice}$ of size at most $\nu+1$, then it is true for each $\forall i\not \in \mathcal{I}_\advice, \forall ss$
\[
\Pr_{z\leftarrow Z}\left[\viable(w_{z,i},ss,z) =1\right]< 2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu + k-\log{\nu}}.
\]
And thus, for on average across $Z$, for all $ss$ the expected number of points outside of $\mathcal{I}_\advice$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i | w_{z,i}\not\in\mathcal{I}_\advice} \viable(w_{z,i}, ss, z) \right]< 2^{-\frac{\alpha- |\advice|}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, for all $ss$
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i }  \viable(w_{z,i}, ss, \advice)  \right]< 2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
\end{corollary}
\noindent
The above corollary follows by combination of Lemma~\ref{lem:smallgeneralviable ss} and \ref{lem:ent members ss}.
\end{proof}
\begin{lemma}
\label{lem:convert distinguisher ss}
Let all parameters be as in Corollary~\ref{corollary:info loss ss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon_\sketch, \delta, \ell)$-secure sketch with distributional advice (see Definition~\ref{def:ss distributional}) if
\[
\tilde{m}> -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]

Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $w, ss, z$ that are correctly generated by the secure sketch.
\end{lemma}

\noindent
We defer the proof of Lemma~\ref{lem:convert distinguisher ss} to Section~\ref{ssec:ss proof} and proceed with the proof of Theorem~\ref{thm:main theorem ss}.

\paragraph{The main proof}
We now proceed to the proof of Theorem~\ref{thm:main theorem ss}.   Let \[\chi:= -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha+-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.\]  Restating Lemma~\ref{lem:convert distinguisher ss} one has that 
\[
\Hav^{\epsilon_\sketch}(U_{n,k} | \sketch(U_{n,k}), Z_{U_{n,k}})) \le \chi.
\]
That is, for all $A, B, C$ such that $\Delta((A, B, C), (U_{n,k},\sketch(U_{n,k}), Z_{U_{n,k}}))\le \epsilon_{\sketch}$ it is true that $\Hav(A|B, C) \le \tilde{m}$.  Define \[P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}\leftarrow \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advice(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_{\mathtt{PCode}}$ by Lemma~\ref{lem:close family} .  
It is thus true that 
\[
\Hav^{\epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}}(\mathtt{PCode}_{n, k, t, \alpha}^{*} | \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\le \tilde{m}.
\]
Suppose not, then there exists some $A, B, C$ where 
\[
\Delta((A, B, C), (\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}.
\]
and $\Hav(A|B, C)>\chi$.
Thus, 
\begin{align*}
\Delta&((A, B, C), (U_{n,k},\sketch(U_{n,k}), Z_{U_{n,k}}))\\&\le \Delta((A, B, C), ((\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))+\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*})\\
&\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}+ \epsilon_{\mathtt{PCode}} = \epsilon_{\sketch}.
\end{align*}
This contradicts the fact that $\Hav^{\epsilon_\sketch}(U_{n,k} | \sketch(U_{n,k}), Z_{U_{n,k}})) \le \chi.$
Finally, Theorem~\ref{thm:main theorem ss} follows by application of Lemma~\ref{lem:distributional advice suffices ss} with setting $\zeta = \chi$ and noting that $\chi\ge 1$.


\subsection{Proof of Lemma~\ref{lem:convert distinguisher ss}}
\label{ssec:ss proof}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher ss}]
We prove the result for an average member of $Z$.  First note that for every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$.  We first need an elementary lemma: 
\begin{lemma}
Let $(X, Y)$ be a pair of random variables and, $S(X, Y)$ be a set, let $f$ be a randomized function taking inputs on the domain of $(X, Y)$. Then 
\[
\Haveps(X|Y, f(X, Y) \in S(X, Y))\ge \Haveps(X|Y) + \log{ \Pr[f(X, Y) \in S(X, Y)]}.
\]
\label{lem:smooth ent cond}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:smooth ent cond}]
Let $X', Y'$ be a distribution such that $\Delta((X, Y), (X', Y')) \le \epsilon$.  By \cite[Lemma 7.8]{fuller2020fuzzy} for any event $\eta$
\begin{align*}
\Hav(X'| Y', \eta) \ge \Hav(X' | Y') +\log{\Pr[\eta]}.
\end{align*}
Let $\eta$ denote the event that $f(X, Y) \in S(X, Y)$.
The proof completes by noting that $\Delta((X, Y), (X', Y'))\le \epsilon$ implies that 
\[
\Delta((X, Y, f(X, Y) \in S(X, Y)), (X', Y,' f(X', Y')\in S(X', Y')))\le \epsilon\] by the information processing lemma. This is turn implies that 
\[
\Haveps(X|Y, f(X, Y) \in S(X, Y))\ge \Haveps(X|Y) +\Pr[f(X, Y) \in S(X, Y)].
\]
This completes the proof of Lemma~\ref{lem:smooth ent cond}.
\end{proof}

\noindent
By Lemma~\ref{lem:smooth ent cond}
\[
\Haveps(W| \sketch(W)) \le \Haveps(W| \sketch(W), \sketch(W)\in \goodsketch_W) +1.
\]
We now restrict our attention to the case when $\sketch(w) \in \goodsketch_w$.   
Let $X', Y'$ be distributions  be an event where $\Delta((W, \sketch(W)), (X', Y'))\le \epsilon$. Note that 
\[
\Pr[\viable(W, \sketch(W), Z) = 1| \sketch_W \in \goodsketch_W] = 1.
\]
Thus, it must be the case that 
\[
\Pr[\viable(X', Y', Z) = 1] \ge 1-\epsilon.
\]
By the definition of $\viable$, the support of $X'$ must be drawn from points in $W_Z$.  For any fixed value of $y\in Y$ by Corollary~\ref{corollary:info loss ss}, it is true that 
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i }  \viable(w_{z,i}, y', \advice)  \right]< 2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
One has, 
\begin{align*}
2^{-\Hav(X'|Y')} &\ge  \Pr[X' \in W_Z] 2^{-\Hav(X'|Y', X'\in W_Z) }+ \Pr[X'\not\in W_Z] 2^{-\Hav(X'|Y', X'\not\in W_Z)}\\
 &\ge  (1-\epsilon)2^{-\Hav(X'|Y', X'\in W_Z) }
\end{align*}
And thus, 
\[
\Hav(X'|Y') \le  -\log{1-\epsilon_\sketch} +\Hav(X'|Y', X'\in W_Z).
\]
The min-entropy of $X' |Y'$ is maximized by considering the uniform distribution over such points. \begin{align*}
\Hav(X'|Y' , X'\in W_Z) &\le \log {2^{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1}\\
&\le 2\max\left\{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\end{align*}
This implies that 
\[
\Haveps(W| \sketch(W)) \le -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
This completes the Proof of Lemma~\ref{lem:convert distinguisher ss}.
\end{proof}


\subsection{Analysis of parameters}
We assume that $\epsilon\le 1/4$ and $\delta<1/4$.  As before for  
$
\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n$ to be negligible it suffices that
\begin{description}
\item[Condition 1] That $\gamma \le \beta-\log{2e}.$\footnote{As in Section~\ref{ssec:analysis params} the additional constraint that 
$
\gamma \le \beta - \log{2e}$ 
imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 
}
\item[Condition 2] Let $0<c_k<1$ be some arbitrary constant and suppose that $k= \gamma+c_kn$ which implies that $k\ge \gamma + \log{n+\omega(\log{n})}$.
\end{description}
These two conditions imply that $\epsilon'\le 1/2$ and $-\log{1-\epsilon'}\le 1$.
\begin{description}
\item[Condition 3] That  $\nu = 1$.
\end{description}
Define \[\chi: =-\frac{\alpha-\ell}{2^k}+1+\mu+2k-\log{\nu}=-\frac{\alpha-|\advice|}{2^k}+1+\mu+2k\]
We now turn to our analysis of $\chi$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le \frac{(n(1-h_2(t/n)) +h_2(2\delta))}{(2\delta)},\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
This implies that 
\begin{align*}
\chi&= -\frac{\alpha-\ell-k}{2^k} + \mu +2k+1\\
&\le  -\frac{\alpha +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}  + 2k+1,\\
&\le - \frac{(n-k)2^k +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta} + 2k+1,\\
&\le  -\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1.
\end{align*}
We consider two settings for $\delta$ one when $\delta<1/4$ and another when $\delta=0$.

\paragraph{Constant error, $\delta<1/4$} 
As long as for constants $c_k, c_\ell$ one has
\begin{align*}
\ell&\le 3c_\ell n2^k,\\
\delta &< 1/4,\\
0\le\frac{\gamma}{n} &\le \min\left\{(1-h_2(t/n)) +\frac{\log{n}+1-2\log{2e}}{2n}, \frac{2}{3}h_2(t/n)-\frac{1}{3}-\frac{4c_k+c_\ell}{3}-\frac{2}{3n}\right\}.
\end{align*} then 
\begin{align*}
\chi\le-&\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1\\
&\le -\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1 \\
&\le -\frac{(n-3k)2^k-\ell-k}{2^k} + 2n(1-h_2(t/n)) +2\\
 &\le -(n-(4c_k+c_\ell)n - 3\gamma) + 2n(1-h_2(t/n)) +2\\
&\le -n+(4c_k+c_\ell)n+3\gamma + 2n(1-h_2(t/n))+2 \\\
&\le n+(4c_k+c_\ell)n+3\gamma -2nh_2(t/n))+2 \le 0\\
\end{align*}
then $\tilde{m} \le 2+2\max\{\chi, \log{2}\} \le 4$ which implies that $1/16$ of the distributions have no secure sketch.

\paragraph{No error, $\delta=0$}
When $\delta = 0$ one has
\begin{align*}
\ell&\le 3c_\ell n2^k,\\
\delta &< 1/4,\\
0\le\frac{\gamma}{n} &\le \min\left\{(1-h_2(t/n)) +\frac{\log{n}+1-2\log{2e}}{2n}, \frac{1}{3}h_2(t/n)-\frac{4c_k+c_\ell}{3}-\frac{2}{3n}\right\}.
\end{align*} 
yielding $\tilde{m}\le 4$ which implies that $1/16$ of the distributions have no secure sketch.


%\begin{align*}
%2^{(n-k)2^k}+2^{2k}-|\advice|-k - 2^knh_2(1/2-t/n) \ge 0\\
%\end{align*}
%Or equivalently that, 
%\begin{align*}
%|\advice| &\le 2^{(n-k)2^k+2k} -k - 2^knh_2(1/2-t/n)\\
%&\le 2^{\Theta(n)2^{\Theta(n)}}.
%\end{align*}


