%!TEX root = main.tex

% Proof for NegFE

\section{Efficient Information-Theoretic Fuzzy Extractors do not exist for most distributions with fuzzy min-entropy}
\label{sec:fe}
\input{family}

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters. 
\begin{enumerate}
\itemsep0em
\item The $\beta$-density is at least $1$ which is  satisfied as long as $t< n/2 $ (see Definition~\ref{def:b density}),
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =  n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}$.
\end{enumerate}
For a quarter of the distributions $W\in \mathtt{PCode}_{n, k, t, \gamma}$ there is no $(\{0,1\}^n, W, \kappa, t, \epsilon, \ell)$-fuzzy extractors for 
\[
\epsilon< \frac{1}{3} - \frac{4(\epsilon_1+\epsilon_2+\epsilon_3)}{3}.
\]
For 
\begin{align*}\log{\epsilon_1}&:= -\left(\kappa+\frac{\alpha -\ell}{2^k} - \mu -2k+\log{\nu}\right),\\
\epsilon_2&:=\frac{\nu+1}{2^{\kappa+1}}\\
\epsilon_3&:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.
\end{align*}
\label{thm:main theorem}
\end{theorem}

Recall that Lemma~\ref{lem:distributional advise suffices} states that to show the hardness of building a fuzzy extractor with distributional advise for $\mathtt{PCode}_{n, k, t, \gamma}$ it suffices to show the hardness of building an auxiliary input fuzzy extractor for the family $\mathtt{PCode}_{n, k, t, \gamma}$. 

Our proof uses the following structure:
\begin{enumerate}

\item Lemma~\ref{lem:close family} shows that the distribution $\mathtt{PCode}_{n, k, t, \gamma}$ is statistically close to the distribution $U_{n,k}$.  For the remainder of the proof we consider $U_{n,k}$.
\item Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string. 

\item Lemma~\ref{lem:entr of members} for the distribution, $U_{n,k}$ the advice string can only reduce the entropy of almost all viable points by a small amount. We call such points \textbf{Average Points}.  There are some points that the adversary has a large amount of information on that we call \textbf{Free Points}.

\item Corollary~\ref{corollary:info loss} puts together the above two Lemmas to show that the adversary includes a small number of points from the distribution in the viable set.

\item Lemma~\ref{lem:convert distinguisher} shows that since the construction cannot align the viable points with the distribution there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

We present the formal versions of the above statements first and then present the proof of Theorem~\ref{thm:main theorem}.

\subsection{Proof Overview}
We show that $\mathtt{PCode}_{n, k, t, \alpha}$ is statistically close to $U_{n,k}$.

\begin{lemma}
 \label{lem:close family}
Let $n, k, t, \gamma$ be parameters such that the $\beta$ density is at least $0$ (which is satisfied as long as $t< n/2 $). Then one has that 
\[\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \gamma}) \le \left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.\]
\end{lemma}

Fuller, Reyzin, and Smith~\cite{fuller2016fuzzy,fuller2020fuzzy} bound the number of points that could have produced the output of a fuzzy extractor.  Such points are called viable.  We present a stronger version of their lemma that is contained in their proof.  The difference is we bound the union of viable points across different values of $\mathsf{key}$ while they only bound the size of a single $\mathsf{key}$ corresponding to the true point.  Their argument is purely geometric, so it also applies to fuzzy extractors with distributional advice. 
We restate the stronger version of \cite[Lemma 5.2]{fuller2020fuzzy}.

\begin{lemma}
    \label{lem:smallgeneralviable}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, and $\epsilon > 0$. 
    Suppose $(\mathsf{Gen, Rep})$ is a $(\mathcal{M,W},\kappa, t, \ell, \epsilon)$-fuzzy extractor with distributional advice for some distribution family $\mathcal{W}$ over $\mathcal{M}$. 
    For any fixed $p$, for any value $\advise \in \zo^\ell$, there is a set $\mathsf{GoodKey}_p \subseteq \zeroone{\kappa}$ of size at least $2^{\kappa - 1}$ such that for every $\mathsf{key} \in \goodkey_p$,
    \[
      \mu:= \sum_{\mathsf{key} \in \goodkey_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v, \advise)}}|}\right) \leq n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}.
    \]   
\end{lemma}

We consider the following family $Z$ which chooses $2^k$ points without replacement from the space $\{0,1\}^n$.  That is, if one samples $Z$ uniformly then one obtains the distribution $U_{n,k}$. 
For convenience we use $W_{z, 1},..., W_{z,2^k}$ to describe the $2^k$ points with nonzero probability in $W_z$ for some value $z$.  We assume no ordering over these points.  

Lemma~\ref{lem:entr of members} bounds how much information $\advise$ contains about the points in the distribution.  Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise = \aux(Z)$ be of length $\ell$.  For a tuple $(v, p, r, \advise)$ define 
\[
\viable(v, p, r, \advise) = \begin{cases} 1& \Pr[\gen(v, \advise) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
Fix some $p$ and let $\mathtt{GoodKey}_p$ be defined as in Lemma~\ref{lem:smallgeneralviable}. 

\begin{lemma}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $\advise = \aux(Z)$ be of length $\ell$.
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  Let $\mu$ be defined as in Lemma~\ref{lem:smallgeneralviable}.  
For each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advise) = 1 \right]}\le -\frac{\alpha -|\advise| }{2^k}+1+\mu+k-\log{\nu}.
\]
\label{lem:entr of members}
\end{lemma}

\begin{corollary}
\label{corollary:info loss}
Suppose that 
\[
   \sum_{\mathsf{key}\in \mathtt{GoodKey}_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v)}}|}\right) \le 2^\mu 
 \]
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, then it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\Pr\left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise, z) \ge 1\middle| \begin{aligned} z\leftarrow Z\\ \advise= \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise) \end{aligned} \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+k-\log{\nu} }.
\]
And thus, for on average across $Z$, the expected number of points outside of $\mathcal{I}_\advise$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i | w_{z,i}\not\in\mathcal{I}_\advise} \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha - |\advise|}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}

\end{corollary}

\begin{lemma}
\label{lem:convert distinguisher}
Let all parameters be as in Corollary~\ref{corollary:info loss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-fuzzy extractor with distributional advise (see Definition~\ref{def:fe distributional}) if
\[
\epsilon<1/2-\left(\epsilon_1+\epsilon_2\right)\\
\]
where 
\begin{align*}
\epsilon_1&= 2^{-\kappa-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $r, p, z$ that are correctly generated by the fuzzy extractor.
\end{lemma}


\subsection{Proof of Theorem~\ref{thm:main theorem}}
\begin{proof}[Proof of Theorem~\ref{thm:main theorem}]
Restating Lemma~\ref{lem:convert distinguisher} one has that for $(R_{U_{n,k}},P_{U_{n,k}}) \leftarrow \gen(U_{n,k}, \advise(Z_{U_{n,k}}))$
\[
\Delta((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}), (U_n, P_{U_{n,k}}, Z_{U_{n,k}}))\ge \epsilon_1+\epsilon_2.
\]
Let $\mathcal{D}$ be one distinguisher that always outputs $1$ on any value $\mathsf{key}, p, z$ for any distribution $z$, then
\begin{align*}
\Pr[\mathcal{D}((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}))=1] &=1\\
\Pr[\mathcal{D}(U_n, P_{U_{n,k}}, Z_{U_{n,k}})=1]&\le 1-(\epsilon_1+\epsilon_2).
\end{align*}
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_3$ by Lemma~\ref{lem:close family} .  Define \[(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\leftarrow \gen(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advise(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
it is thus true that 
\begin{align*}
\Pr[\mathcal{D}(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&=1\\
\Pr[\mathcal{D}(U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&\le 1-(\epsilon_1-\epsilon_2).
\end{align*}
and thus that 
\[
\Delta((R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}), (U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\ge \epsilon_1-\epsilon_2.
\]

\noindent
Finally, the theorem follows by application of Lemma~\ref{lem:distributional advise suffices} with the setting of $\zeta = 1/4$.
\end{proof} 



\subsection{Analysis of parameters}
\label{ssec:analysis params}
We separately consider $\epsilon_1, \epsilon_2$ and $\epsilon_3$. We refer to these three terms as average points, free points, and distributional closeness respectively. This is because $\epsilon_1$ describes how much information the $\advise$ has about average points in $W_Z$, $\epsilon_2$ considers a small number of points that are more thoroughly described by $\advise$, and $\epsilon_3$ controls the statistical distance between $U_{n, k}$ and $\mathtt{PCode}_{n, k, t, \alpha}$.



\subsubsection{Free Points}
For $\epsilon_2$ to be negligible it suffices that $\nu/2^\kappa$ is negligible.  Note that for the fuzzy extractor to have meaningful security requires $\kappa = \omega(\log{n})$.  We set $\nu = 2^{c_{\kappa}\kappa}$ for some constant $0 < c_{\kappa}<1$ which yields 
\[
\frac{\nu+1}{2^\kappa} = 2^{(c_{\kappa}-1)\kappa}+ 2^{-\kappa} = \ngl(n).
\]
\subsubsection{Distributional Closeness}
 Lemma~\ref{lem:close family} yields a negligible statistical distance as long as 
\begin{align*}
2^{\gamma - \beta} &\le \frac{1}{2e},\\
 \gamma &\le \beta -\log{2e}.\\
2^{k-\gamma}&\ge n+\omega(\log n)\\
k &\ge \gamma + \log{n+ \omega(\log{n})}.
\end{align*}


By Lemma~\ref{lem:max fuzz ent} for any $W$ in $\zo^n$ it is true that $\Hfuzz(W) \le n -\log{|B_t|} \le n(1-h_2(t/n))$.  Thus, the additional constraint that 
\[
\gamma \le \beta - \log{2e}
\le n(1-h_2(t/n)) - \log{2e}\] imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 
For some constant $0<c_{k,1} < 1$, we set $k = \gamma + c_{k,1}n$. 

\subsubsection{Average Points} 
We now turn to our analysis of $\epsilon_1$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le nh_2(1/2-t/n),\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
This implies that 
\begin{align*}
-\log{\epsilon_1}&= \kappa+\frac{\alpha-\ell}{2^k} - \mu -2k+\log{\nu}\\
&\ge  \kappa+\frac{\alpha -\ell}{2^k} - nh_2(1/2-t/n) - 2k+\log{\nu},\\
&\ge  \kappa+\frac{(n-k)2^k-\ell}{2^k} - nh_2(1/2-t/n) - 2k+c_{\kappa}\kappa,\\
&\ge  (1+c_{\kappa})\kappa+\frac{(n-k)2^k-2k2^{k}-\ell}{2^k} - nh_2(1/2-t/n)\\
&>  (1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) .
\end{align*}

Below we consider two different parameter settings when one is trying to derive a nontrivial key $\kappa = \omega(\log n)$ and when one is deriving a maximal length key where $\kappa = \Theta(n)$.
\paragraph{Setting of $\kappa =\omega(\log n)$} 
Recall that $k = \gamma+c_{k,1}n$. let $c_{k,2}$ be some constant such that $c_{k,2}<c_{k,1}$. Then as long as
\[
\ell\le 3(c_{k,1}-c_{k,2}) n2^k= 3(c_{k,1}-c_{k,2})n2^{\gamma+c_{k,1}n}
%n\ge \frac{\ell+k}{2^k(c_{k,1}-c_{k,2})}.
\]
and 
\[
\gamma \le \frac{n(1-3c_{k,2} - h_2(1/2-t/n))}{3}.
\]
 then it holds that 
\begin{align*}
\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) 
\ge n(1-3c_{k,2}) - 3\gamma - nh_2(1/2-t/n) \ge 0
\end{align*}
which suffices to ensure that 
\[
\epsilon_1 \le   2^{-\left((1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) \right)} \le  2^{-(1+c_{\kappa})\kappa} = \ngl(n).
\]
\noindent
Adding the constraint from \textbf{Distributional Closeness}
this means that 
\[
\frac{\gamma}{n} \le \min\left\{(1-h_2(t/n)) - \frac{\log{2e}}{n}, \frac{(1-3c_{k,2} - h_2(1/2-t/n))}{3}\right\}.
\]
as long as $\ell\le 3(c_{k,1}-c_{k,2})n2^{\gamma+c_{k,1}n}.$

\paragraph{Setting of $\kappa = \Theta(n)$}
In this setting let $c_{\kappa^*} n = \kappa+\log{\nu}$.  Setting 
\begin{align*}
\ell&\le  3(c_{k,1}-c_{k,2})n2^{\gamma+c_{k,1}n}\\
\gamma&\le \frac{n}{3}\left(1+c_{\kappa^*} -3c_{k,2}-h_2(1/2-t/n)\right) - \omega(\log{n})
\end{align*}
yields that \[ (1+c_{\kappa})\kappa+\frac{(n-3k)2^k-\ell}{2^k} - nh_2(1/2-t/n) = \omega(\log n)\] 

\section{Proof of Technical Lemmas}
\label{sec:tech lemmas}
\subsection{Proof of Lemma~\ref{lem:close family}}
\input{sampling}


\subsection{Proof of Lemma~\ref{lem:entr of members}}

\begin{proof}[Proof of Lemma~\ref{lem:entr of members}]
Let $Z$ and $\advise$ be defined as above. We begin by noting that 
\[
\Hoo(W_{z, 1},..., W_{z, 2^k})=\alpha.
\]
 It is thus, the case that 
\[
\Hav(W_{z, 1},..., W_{z, 2^ k} | \advise) \ge \alpha - |\advise|.
\]
For all values $x, y$, 
\[
\Pr[W_{z, i} =x | W_{z, j} = y] = \begin{cases} \frac{1}{2^n-1} &x\neq y\\0&x=y.\end{cases}
\]
\begin{claim}
One has that 
\begin{align*}
\expe_{i=1}^{2^k} \left[\Hav(W_{z, i} | \advise)\right] \ge \frac{\alpha - |\advise|}{2^k}.
\end{align*}
\label{clm:entropy distributes}
\end{claim}
\begin{proof}
It is true that $\forall w_{z,1},..., w_{z,i-1}$ that 
\[
\Hav(W_{z,i} | \advise, W_{z, 1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}) \le \Hav(W_{z,i} | \advise).
\]
This is because conditioned on $W_{z, j} =w_{z,j}$ only removes the outcome $w_{z,j}$ from the support of $W_{z,i}$ increasing all other outcomes proportionally.  
We now proceed to the proof of the claim. 

Suppose not, then there exists the following predictor $\mathcal{P}$ for the joint distribution $W_{z, 1},..., W_{z, k} | \advise$:
\begin{enumerate}
\item For $i=1$ to $k$ predict $w_{z,i}\leftarrow W_{z, i}| \advise, W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}$
\item Output the joint prediction $w_{z,1},..., w_{z, k}$.  
\end{enumerate}
Let $\alpha_i$ denote the values $\Hav(W_{z,i}| \advise) = \alpha_i$ for $i=1$ to $k$. 
The probability that $\mathcal{P}$ issues a correct prediction is
\begin{align*}
\prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advise,  W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1})} \ge \prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advise)} = \prod_{i=1}^{2^k} 2^{-\alpha_i}  = 2^{-\sum_{i=1}^{2^k} \alpha_i}.
\end{align*}

\noindent
Suppose that $\sum_{i=1}^{2^k} \alpha_i < \alpha-\advise$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advise)}$, a contradiction.  Thus, $\expe_{1\le i\le k} \alpha_i \ge (\alpha-\advise)/2^k$. 
This completes the proof of Claim~\ref{clm:entropy distributes}.
\end{proof}

\noindent
We now proceed to the proof of Lemma~\ref{lem:entr of members}
By Lemma~\ref{lem:markovpred} it is true that there exists a set $\mathcal{I}\subseteq \{1,...,2^k\}$ of size $\nu$ where  such that for all $i\not \in \{1,...,2^k\} \setminus\mathcal{I}$ is true that 
\[
\eta:=\Hav(W_{z,i} | \advise) \ge \frac{\alpha -|\advise|}{2^k} -\left(k-\log{\nu}\right).
\]
Let $i^*$ denote the index of the point that will be given to $\gen$ that is $p\leftarrow (w_{z,i^*}, \advise)$.  We define the set $\mathcal{I}_\advise = \mathcal{I} \cup \{w_{z,i^*}\}$ where $w$ is the point given to $\gen(w, \advise)$.  
Then for $i\not\in \mathcal{I}_{\advise}$ it is true that 
\begin{align*}
\Hav(W_{z,i} | \mathsf{key} \in \mathtt{GoodKey}, p)&\ge \\
\Hav(W_{z,i} | \advise, W_{z,i^*}= w_{z,i^*}) &\ge -\log{\frac{1}{2^{\eta}-1}} \ge \eta-1.
\end{align*}
We now proceed to bounding $\viable(w_i, p, \mathsf{key})$.  By assumption, By Lemma~\ref{lem:smallgeneralviable} we know that there are at most $2^\mu$ points in $\viable(w_i, p, \mathsf{key})$.  Thus, for all $i\not \in\mathcal{I}_{\advise}$ 

\begin{align*}
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advise) = 1 \right]}&\le - (\eta-1 -\mu) - \\
&=-\left(\frac{\alpha -|\advise| }{2^k}-1-\mu-\left(k-\log{\nu}\right)\right).
\end{align*}
This completes the proof of Lemma~\ref{lem:entr of members}.
\end{proof}


\subsection{Proof of Lemma~\ref{lem:convert distinguisher}}

\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher}].
We prove the result for an average member of $Z$.
Consider the following distinguisher $\mathcal{D}$ for triples of the form $r, p, z$:
\begin{enumerate}
\item If $r \not\in \mathsf{GoodKey}_p$ output $1$,
\item If $\sum_{i} \viable(w_{z, i}, r, p, \advise(z)) =0 $ output $0$,
\item Else output $1$.
\end{enumerate}
First note that by perfect correctness it is always the case that when given $\mathsf{key}, p$ that $\mathcal{D}$ outputs $1$.  We proceed to bound the probability that $\mathcal{D}$ outputs $1$ when given $U_\kappa, p$.  Note that the probability that $\Pr[U_\kappa \in \mathtt{GoodKey}_p] \ge 1/2$ by the definition of $\mathtt{GoodKey}_p$. 

We bound the number of parts with at least one point in viable.  We begin by assuming that all points in viable are in different  values $r$ so the bound on the size of 
\[
\left\{w_{z, i} \middle|\sum_{\mathsf{key}\in\mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise)>0\right\}_{i=1}^k 
\] 
gives a bound on the number of nonempty parts. By Corollary~\ref{corollary:info loss} 
\[
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\]
Thus the fraction of non-empty parts in $\mathsf{Goodkey}_p$ on average is at most 
\[
2^{-\kappa-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\frac{\nu+1}{2^\kappa}.
\]
Thus, the probability that $\mathcal{D}$ outputs $0$ when given $U_\kappa, p, z$ is at least 
$1/2-(\epsilon_1+\epsilon_2)$
where 
\begin{align*}
\epsilon_1:&=2^{-\kappa-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2:&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
\noindent
This completes the Proof of Lemma~\ref{lem:convert distinguisher}.
\end{proof}

\section{Secure Sketches}
\label{sec:ss}
This section creates an upper bound on the quality of efficient secure sketches.  This bound considers has stronger parameters than Theorem~\ref{thm:main theorem} due largely to the difference between Lemmas~\ref{lem:smallgeneralviable} and~\ref{lem:smallgeneralviable ss}.

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters.
\begin{enumerate}
\itemsep0em
\item The $\beta$-density is at least $1$ which is  satisfied as long as $t< n/2 $ (see Definition~\ref{def:b density}),
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =(n(1-h_2(t/n)) +h_2(2\delta))/(1-2\delta)$.
\end{enumerate}
For half of the distributions $W\in \mathtt{PCode}_{n, k, t, \gamma}^{*}$ there is no $(\{0,1\}^n, W, \tilde{m}, t, \epsilon,\delta, \ell)$-secure sketch for 
\[
\tilde{m}\ge  -\log{1-\epsilon'} +1 + 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
where $\epsilon' = \epsilon+\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.$
\label{thm:main theorem ss}
\end{theorem}

\subsection{Analysis of parameters}
We assume that $\epsilon\le 1/4$ and $\delta<1/4$.  As before for  
\[
\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n = \ngl(n)\]  it suffices 
\begin{align*}
 \gamma &\le \beta -\log{2e}.\\
k &\ge \gamma + \log{n+ \omega(\log{n})}.
\end{align*}
These two conditions imply that $\epsilon'\le 1/2$ and $-\log{1-\epsilon'}\le 1$.
We assume that $\nu = 1$.
As in Section~\ref{ssec:analysis params} the additional constraint that 
\[
\gamma \le \beta - \log{2e}
\le n(1-h_2(t/n)) - \log{2e}\] imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 
For some arbitrary constant $0<c_1 < 1$ We set $k = \gamma + c_1n$. 
Define \[\chi: =-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}=-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k\]
We now turn to our analysis of $\chi$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le \frac{(n(1-h_2(t/n)) +h_2(2\delta))}{(2\delta)},\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
This implies that 
\begin{align*}
\chi&= -\frac{\alpha-\ell-k}{2^k} + \mu +2k+1\\
&\le  -\frac{\alpha +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}  + 2k+1,\\
&\le - \frac{(n-k)2^k +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta} + 2k+1,\\
&\le  -\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1.
\end{align*}
Let $0<c_2<c_1$ be an arbitrary constant. We consider two settings for $\delta$ one when $\delta<1/4$ and another when $\delta=0$.

\paragraph{Constant error, $\delta<1/4$} 
As long as 
\begin{align*}
\ell&\le 3(c_1-c_2) n2^k = 3(c_1-c_2)n2^{\gamma+c_1n},\\
\delta &< 1/4,\\
\gamma &\le \frac{n(2h_2(t/n)-1-3c_2 )-2}{3}.
\end{align*} then 
\begin{align*}
-&\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1\\
&\le -\frac{(n-3k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1 \\
&\le -\frac{(n-3k)2^k-\ell-k}{2^k} + 2n(1-h_2(t/n)) +2\\
 &\le -(n-3c_2n - 3\gamma) + 2n(1-h_2(t/n)) +2\\
&\le -n+3c_2n+3\gamma + 2n(1-h_2(t/n))+2 \\\
&\le n+3c_2n+3\gamma -2nh_2(t/n))+2 \le 0\\
\end{align*}
Adding the constraint from \textbf{Distributional Closeness}
this means that 
\[
\frac{\gamma}{n} \le \min\left\{(1-h_2(t/n)) - \frac{\log{2e}}{n}, \frac{2}{3}h_2(t/n)-\frac{1}{3}-c_2-\frac{2}{3}n\right\}.
\]
suffices to ensure $\tilde{m} \le 4$ if $\ell\le 3(c_1-c_2)n2^{\gamma+c_1n}$ .

\paragraph{No error, $\delta=0$}
When $\delta = 0$ one has
\[
\gamma \le \frac{n( h_2(t/n)-3c_2)-1}{3}.\]
yielding
\[
\frac{\gamma}{n} \le \min\left\{(1-h_2(t/n)) - \frac{\log{2e}}{n}, \frac{1}{3}h_2(t/n)-c_2-\frac{1}{3n}\right\}.
\]
if $\ell\le 3(c_1-c_2)n2^{\gamma+c_1n}$.

\subsection{Proof}
\begin{proof}[Proof of Theorem~\ref{thm:main theorem ss}]

Fuller, Reyzin, and Smith~\cite[Lemma 7.3]{fuller2020fuzzy} showed that good secure sketches are bounded in size as they imply good Shannon error correcting codes.  This result holds true if one considers a secure sketch that retains smooth min-entropy with no loss in parameters because it only relies on the correctness of the secure sketch (not the security property).  

\begin{lemma}
\label{lem:smallgeneralviable ss}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric. Suppose $(\sketch, \rec)$ is a $(\zo^n, \mathcal{W}, \tilde{m}, t, \epsilon_\sketch, \delta)$-secure sketch, for some family $\mathcal{W}$. 
    
%    Let $W\in \mathcal{W}$, and let $W', SS'$ be a distribution such that $\Delta((W, \sketch(W)), (W', SS'))\le \epsilon$ and $\Hav(W' |SS')\ge \tilde{m}$.
   For every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
   \mu:= \log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
\end{lemma}

We present an analog of Lemma~\ref{lem:entr of members} adapted to the secure sketch setting. Let $\goodsketch_v$ be defined as in Lemma~\ref{lem:smallgeneralviable ss}. For a triple $(v, ss, z)$ define $\viable(v, ss, z)=1$ if
\begin{enumerate}
\itemsep0em
\item $\Pr[\gen(v, \advise(z)) = ss]>0$,
\item $ss\in\goodsketch_v$, and
\item $\Pr[W_z = v]>0$.
\end{enumerate}
and set $\viable(v, ss, z)=0$ otherwise. 

\begin{lemma}
\label{lem:ent members ss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $\advise = \aux(Z)$ be of length $\ell$.
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  Let $\mu$ be defined as in Lemma~\ref{lem:smallgeneralviable ss}. 
For each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\log{\Pr\left[\viable(w_{z,i}, ss, \advise) = 1 \right]}\le -\frac{\alpha -|\advise| }{2^k}+1+\mu+k-\log{\nu}.
\]
\end{lemma}

The proof of Lemma~\ref{lem:ent members ss} is identical to the proof of Lemma~\ref{lem:entr of members} and is omitted. 
Lemma~\ref{lem:ent members ss} suffices to bound how many points are ``viable'' from the output of the secure sketch.


\begin{corollary}
\label{corollary:info loss ss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise = \aux(Z)$ be of length $\ell$.       For every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
    \mu:=\log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
For a triple $(v, ss, z)$ define $\viable(v, ss, z)=1$ if
\begin{enumerate}
\itemsep0em
\item $\Pr[\gen(v, \advise(z)) = ss]>0$,
\item $ss\in\goodsketch_v$, and
\item $\Pr[W_z = v]>0$.
\end{enumerate}
and set $\viable(v, ss, z)=0$ otherwise. 
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, then it is true for each $\forall i\not \in \mathcal{I}_\advise, \forall ss$
\[
\Pr_{z\leftarrow Z}\left[\viable(w_{z,i},ss,z) =1\right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu + k-\log{\nu}}.
\]
And thus, for on average across $Z$, for all $ss$ the expected number of points outside of $\mathcal{I}_\advise$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i | w_{z,i}\not\in\mathcal{I}_\advise} \viable(w_{z,i}, ss, z) \right]< 2^{-\frac{\alpha- |\advise|}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, for all $ss$
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i }  \viable(w_{z,i}, ss, \advise)  \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
\end{corollary}
\noindent
The above corollary follows by combination of Lemma~\ref{lem:smallgeneralviable ss} and \ref{lem:ent members ss}.
\end{proof}
\begin{lemma}
\label{lem:convert distinguisher ss}
Let all parameters be as in Corollary~\ref{corollary:info loss ss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon_\sketch, \delta, \ell)$-secure sketch with distributional advise (see Definition~\ref{def:ss distributional}) if
\[
\tilde{m}> -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]

Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $w, ss, z$ that are correctly generated by the secure sketch.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher ss}]
We prove the result for an average member of $Z$.  First note that for every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$.  By Lemma~\ref{lem:smooth ent cond}
\[
\Haveps(W| \sketch(W)) \le \Haveps(W| \sketch(W), \sketch(W)\in \goodsketch_W) +1.
\]
We now restrict our attention to the case when $\sketch(w) \in \goodsketch_w$.   
Let $X', Y'$ be distributions  be an event where $\Delta((W, \sketch(W)), (X', Y'))\le \epsilon$. Note that 
\[
\Pr[\viable(W, \sketch(W), Z) = 1| \sketch_W \in \goodsketch_W] = 1.
\]
Thus, it must be the case that 
\[
\Pr[\viable(X', Y', Z) = 1] \ge 1-\epsilon.
\]
By the definition of $\viable$, the support of $X'$ must be drawn from points in $W_Z$.  For any fixed value of $y\in Y$ by Corollary~\ref{corollary:info loss ss}, it is true that 
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i }  \viable(w_{z,i}, y', \advise)  \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
One has, 
\begin{align*}
2^{-\Hav(X'|Y')} &\ge  \Pr[X' \in W_Z] 2^{-\Hav(X'|Y', X'\in W_Z) }+ \Pr[X'\not\in W_Z] 2^{-\Hav(X'|Y', X'\not\in W_Z)}\\
 &\ge  (1-\epsilon)2^{-\Hav(X'|Y', X'\in W_Z) }
\end{align*}
And thus, 
\[
\Hav(X'|Y') \le  -\log{1-\epsilon_\sketch} +\Hav(X'|Y', X'\in W_Z).
\]
The min-entropy of $X' |Y'$ is maximized by considering the uniform distribution over such points. \begin{align*}
\Hav(X'|Y' , X'\in W_Z) &\le \log {2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1}\\
&\le 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\end{align*}
This implies that 
\[
\Haveps(W| \sketch(W)) \le -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
This completes the Proof of Lemma~\ref{lem:convert distinguisher ss}.
\end{proof}

\noindent
We now proceed to the proof of Theorem~\ref{thm:main theorem ss}.   Let \[\chi:= -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha+-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.\]  Restating Lemma~\ref{lem:convert distinguisher} one has that 
\[
\Hav^{\epsilon_\sketch}(U_{n,k} | \sketch(U_{n,k}), Z_{U_{n,k}})) \le \chi.
\]
That is, for all $A, B, C$ such that $\Delta((A, B, C), (U_{n,k},\sketch(U_{n,k}), Z_{U_{n,k}}))\le \epsilon_{\sketch}$ it is true that $\Hav(A|B, C) \le \tilde{m}$.  Define \[P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}\leftarrow \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advise(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_{\mathtt{PCode}}$ by Lemma~\ref{lem:close family} .  
It is thus true that 
\[
\Hav^{\epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}}(\mathtt{PCode}_{n, k, t, \alpha}^{*} | \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\le \tilde{m}.
\]
Suppose not, then there exists some $A, B, C$ where 
\[
\Delta((A, B, C), (\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}.
\]
and $\Hav(A|B, C)>\chi$.
Thus, 
\begin{align*}
\Delta&((A, B, C), (U_{n,k},\sketch(U_{n,k}), Z_{U_{n,k}}))\\&\le \Delta((A, B, C), ((\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))+\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*})\\
&\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}+ \epsilon_{\mathtt{PCode}} = \epsilon_{\sketch}.
\end{align*}
This contradicts the fact that $\Hav^{\epsilon_\sketch}(U_{n,k} | \sketch(U_{n,k}), Z_{U_{n,k}})) \le \chi.$
Finally, Theorem~\ref{thm:main theorem ss} follows by application of Lemma~\ref{lem:distributional advise suffices ss} with setting $\zeta = \chi$ and noting that $\chi\ge 1$.



%\begin{align*}
%2^{(n-k)2^k}+2^{2k}-|\advise|-k - 2^knh_2(1/2-t/n) \ge 0\\
%\end{align*}
%Or equivalently that, 
%\begin{align*}
%|\advise| &\le 2^{(n-k)2^k+2k} -k - 2^knh_2(1/2-t/n)\\
%&\le 2^{\Theta(n)2^{\Theta(n)}}.
%\end{align*}


