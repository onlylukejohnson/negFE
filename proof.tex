%!TEX root = main.tex

% Proof for NegFE

\section{Efficient Information-Theoretic Fuzzy Extractors do not exist for all distributions with fuzzy min-entropy}
\label{sec:fe}

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters. 
\begin{enumerate}
\itemsep0em
\item Define $2^\beta:=\frac{2^n-|B_t|}{|B_t|}$ and assume that $2^\beta\ge 1$ which is  satisfied as long as $t< n/2 $,
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =  n \cdot h_2\prns{\frac{1}{2} - \frac{t}{n}}$.
\end{enumerate}
For a quarter of the distributions $W\in \mathtt{PCode}_{n, k, t, \gamma}^{*}$ there is no $(\{0,1\}^n, W, \kappa, t, \epsilon, \ell)$-fuzzy extractors for 
\[
\epsilon< \frac{1}{3} - \frac{4(\epsilon_1+\epsilon_2+\epsilon_3)}{3}.
\]
For 
\begin{align*}\log{\epsilon_1}&:= -\left(\kappa+\frac{\alpha -\ell}{2^k} - \mu -2k+\log{\nu}\right),\\
\epsilon_2&:=\frac{\nu+1}{2^{\kappa+1}}\\
\epsilon_3&:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.
\end{align*}
\label{thm:main theorem}
\end{theorem}

\subsection{Proof Outline}
Our proof uses the following structure
\begin{enumerate}

\item Lemma~\ref{lem:distributional advise suffices} shows that to show hardness of building a fuzzy extractor with distributional advise for $\mathtt{PCode}_{n, k, t, \gamma}^{*}$ it suffices to show hardness of building an auxiliary input fuzzy extractor for the family $U_{n,k}$ where each element is $2^k$ points chosen without replacement. For the remainder of the proof we consider $U_{n,k}$.

\item Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string. 

\item Lemma~\ref{lem:entr of members} for the distribution, $U_{n,k}$ the advice string can only reduce the entropy of a large fraction of viable points by a small amount. We call such points \textbf{Average Points}.  There are some points that the adversary has a large amount of information on that we call \textbf{Free Points}

\item Corollary~\ref{corollary:info loss} puts together the above two Lemmas to show that the adversary includes a small number of points from the distribution in the viable set.

\item Lemma~\ref{lem:convert distinguisher} Shows that since the construction cannot align the viable points with the distribution there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

\subsection{Analysis of parameters}
\label{ssec:analysis params}
We separately consider $\epsilon_1, \epsilon_2$ and $\epsilon_3$. We refer to these three terms as average points, free points, and distributional closeness respectively. This is because $\epsilon_1$ describes how much information the $\advise$ has about average points in $W_Z$, $\epsilon_2$ considers a small number of points that are more thoroughly described by $\advise$, and $\epsilon_3$ controls the statistical distance between $U_{n, k}$ and $\mathtt{PCode}_{n, k, t, \alpha}$.



\paragraph{Free Points}
For $\epsilon_2$ to be negligible it suffices that $\nu/2^\kappa$ is negligible.  In the case when $\kappa = \omega(\log{n})$, setting $\nu = \poly(n)$ suffices.
\paragraph{Distributional Closeness}
 Lemma~\ref{lem:close family} yields a negligible statistical distance as long as 
\begin{align*}
2^{\gamma - \beta} &\le \frac{1}{2e},\\
 \gamma &\le \beta -\log{2e}.\\
2^{k-\gamma}&\ge n+\omega(\log n)\\
k &\ge \gamma + \log{n+ \omega(\log{n})}.
\end{align*}


By Lemma~\ref{lem:max fuzz ent} for any $W$ in $\zo^n$ it is true that $\Hfuzz(W) \le n -\log{|B_t|} \le n(1-h_2(t/n))$.  Thus, the additional constraint that 
\[
\gamma \le \beta - \log{2e}
\le n(1-h_2(t/n)) - \log{2e}\] imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 
For some arbitrary constant $0<c_1 < 1$ We set $k = \gamma + c_1n$. 

\paragraph{Average Points} 
We now turn to our analysis of $\epsilon_1$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le nh_2(1/2-t/n),\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
This implies that 
\begin{align*}
-\log{\epsilon_1}&= \kappa+\frac{\alpha +\log{\nu}-\ell-k}{2^k} - \mu -k\\
&\ge  \kappa+\frac{\alpha +\log{\nu}-\ell-k}{2^k} - nh_2(1/2-t/n) - k,\\
&\ge  \kappa+\frac{(n-k)2^k +\log{\nu}-\ell-k}{2^k} - nh_2(1/2-t/n) - k,\\
&\ge  \kappa+\frac{(n-k)2^k-k2^{k} +\log{\nu}-\ell-k}{2^k} - nh_2(1/2-t/n)\\
&>  \kappa+\frac{(n-2k)2^k-\ell-k}{2^k} - nh_2(1/2-t/n) .
\end{align*}

Since we know that $\kappa =\omega(\log n)$. Recall that $k = \gamma+c_1n$, let $c_2$ be some constant such that $c_2<c_1$ Then as long as
\[
\ell\le 2(c_1-c_2) n2^k -k= 2(c_1-c_2)n2^{\gamma+c_1n}-(\gamma+c_1n)
%n\ge \frac{\ell+k}{2^k(c_1-c_2)}.
\]
 then it holds that 
\begin{align*}
0 &\le \frac{(n-2k)2^k-\ell-k}{2^k} - nh_2(1/2-t/n) \\
&\le \frac{(n-2(\gamma+c_1n))2^k- 2(c_1-c_2)n2^k }{2^k} - nh_2(1/2-t/n) \\
 &\le n(1-2c_2) - 2\gamma - nh_2(1/2-t/n) \\
\gamma &\le \frac{n(1-2c_2 - h_2(1/2-t/n))}{2}.\\
\end{align*}
which suffices to ensure that 
\[
\epsilon_1 \le   2^{-\left(\kappa+\frac{(n-2k)2^k-\ell-k}{2^k} - nh_2(1/2-t/n) \right)} \le  2^{-\kappa} = \ngl(n).
\]
\noindent
Adding the constraint from \textbf{Distributional Closeness}
this means that 
\[
\frac{\gamma}{n} \le \min\left\{(1-h_2(t/n)) - \frac{\log{2e}}{n}, \frac{(1-2c_2 - h_2(1/2-t/n))}{2}\right\}.
\]
as long as $\ell\le 2(c_1-c_2)n2^{\gamma+c_1n}-(\gamma+c_1n).$

\subsection{Proof}
\noindent
We present Lemmas~\ref{lem:entr of members} and \ref{lem:convert distinguisher} first and then return to the proof of Theorem~\ref{thm:main theorem}.


\paragraph{The family $\mathcal{W}$}
We consider the following family $Z$ which chooses $2^k$ points without replacement from the space $\{0,1\}^n$.  That is, if one samples $Z$ uniformly then one obtains the distribution $U_{n,k}$. There are distributions in this family with little fuzzy min-entropy.  As shown in Lemma~\ref{lem:close family} $U_{n,k}$  is statistically close to $\mathtt{PCode}_{n, k, t, \gamma}^{*}$, a family with fuzzy min-entropy.  For the remainder of this proof we consider $U_{n,k}$ and then show that the fuzzy extractor cannot perform differently on the family $\mathtt{PCode}_{n, k, t, \gamma}^{*}$ where every distribution does have fuzzy min-entropy. 

For convenience we use $W_{z, 1},..., W_{z,2^k}$ to describe the $2^k$ points with nonzero probability in $W_z$ for some value $z$.  We assume no ordering over these points.  For all values $x, y$, 
\[
\Pr[W_{z, i} =x | W_{z, j} = y] = \begin{cases} \frac{1}{2^n-1} &x\neq y\\0&x=y.\end{cases}
\]

We begin with Lemma bounding how much information $\advise$ contains about the points in the distribution.
\begin{lemma}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $\advise = \aux(Z)$ be of length $\ell$.
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
For each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advise) = 1 \right]}\le -\frac{\alpha -|\advise| }{2^k}+1+\mu+k-\log{\nu}.
\]
\label{lem:entr of members}
\end{lemma}

\begin{proof}
Let $Z$ and $\advise$ be defined as above. We begin by noting that 
\[
\Hoo(W_{z, 1},..., W_{z, 2^k})=\alpha.
\]
 It is thus, the case that 
\[
\Hav(W_{z, 1},..., W_{z, 2^ k} | \advise) \ge \alpha - |\advise|.
\]
\begin{claim}
One has that 
\begin{align*}
\expe_{i=1}^{2^k} \left[\Hav(W_{z, i} | \advise)\right] \ge \frac{\alpha - |\advise|}{2^k}.
\end{align*}
\label{clm:entropy distributes}
\end{claim}
\begin{proof}
It is true that $\forall w_{z,1},..., w_{z,i-1}$ that 
\[
\Hav(W_{z,i} | \advise, W_{z, 1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}) \le \Hav(W_{z,i} | \advise).
\]
This is because conditioned on $W_{z, j} =w_{z,j}$ only removes the outcome $w_{z,j}$ from the support of $W_{z,i}$ increasing all other outcomes proportionally.  
We now proceed to the proof of the claim. 

Suppose not, then there exists the following predictor $\mathcal{P}$ for the joint distribution $W_{z, 1},..., W_{z, k} | \advise$:
\begin{enumerate}
\item For $i=1$ to $k$ predict $w_{z,i}\leftarrow W_{z, i}| \advise, W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}$
\item Output the joint prediction $w_{z,1},..., w_{z, k}$.  
\end{enumerate}
Let $\alpha_i$ denote the values $\Hav(W_{z,i}| \advise) = \alpha_i$ for $i=1$ to $k$. 
The probability that $\mathcal{P}$ issues a correct prediction is
\begin{align*}
\prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advise,  W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1})} \ge \prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advise)} = \prod_{i=1}^{2^k} 2^{-\alpha_i}  = 2^{-\sum_{i=1}^{2^k} \alpha_i}.
\end{align*}

\noindent
Suppose that $\sum_{i=1}^{2^k} \alpha_i < \alpha-\advise$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advise)}$, a contradiction.  Thus, $\expe_{1\le i\le k} \alpha_i \ge (\alpha-\advise)/2^k$. 
This completes the proof of Claim~\ref{clm:entropy distributes}.
\end{proof}

\noindent
We now proceed to the proof of Lemma~\ref{lem:entr of members}
By Lemma~\ref{lem:markovpred} it is true that there exists a set $\mathcal{I}\subseteq \{1,...,2^k\}$ of size $\nu$ where  such that for all $i\not \in \{1,...,2^k\} \setminus\mathcal{I}$ is true that 
\[
\eta:=\Hav(W_{z,i} | \advise) \ge \frac{\alpha -|\advise|}{2^k} -\left(k-\log{\nu}\right).
\]
Let $i^*$ denote the index of the point that will be given to $\gen$ that is $p\leftarrow (w_{z,i^*}, \advise)$.  We define the set $\mathcal{I}_\advise = \mathcal{I} \cup \{w_{z,i^*}\}$ where $w$ is the point given to $\gen(w, \advise)$.  
Then for $i\not\in \mathcal{I}_{\advise}$ it is true that 
\begin{align*}
\Hav(W_{z,i} | \mathsf{key} \in \mathtt{GoodKey}, p)&\ge \\
\Hav(W_{z,i} | \advise, W_{z,i^*}= w_{z,i^*}) &\ge -\log{\frac{1}{2^{\eta}-1}} \ge \eta-1.
\end{align*}
We now proceed to bounding $\viable(w_i, p, \mathsf{key})$.  By assumption, By Lemma~\ref{lem:smallgeneralviable} we know that there are at most $2^\mu$ points in $\viable(w_i, p, \mathsf{key})$.  Thus, for all $i\not \in\mathcal{I}_{\advise}$ 

\begin{align*}
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advise) = 1 \right]}&\le - (\eta-1 -\mu) - \\
&=-\left(\frac{\alpha -|\advise| }{2^k}-1-\mu-\left(k-\log{\nu}\right)\right).
\end{align*}
This completes the proof of Lemma~\ref{lem:entr of members}.
\end{proof}

\begin{corollary}
\label{corollary:info loss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise = \aux(Z)$ be of length $\ell$.  For a tuple $(v, p, r, \advise)$ define 
\[
\viable(v, p, r, \advise) = \begin{cases} 1& \Pr[\gen(v, \advise) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
Fix some $p$ and let $\mathtt{GoodKey}_p$ be defined as in Lemma~\ref{lem:smallgeneralviable}. Suppose that 
\[
   \sum_{\mathsf{key}\in \mathtt{GoodKey}_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v)}}|}\right) \le 2^\mu 
 \]
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, then it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\Pr\left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise, z) \ge 1\middle| \begin{aligned} z\leftarrow Z\\ \advise= \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise) \end{aligned} \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+k-\log{\nu} }.
\]
And thus, for on average across $Z$, the expected number of points outside of $\mathcal{I}_\advise$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i | w_{z,i}\not\in\mathcal{I}_\advise} \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha - |\advise|}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}

\end{corollary}


\begin{lemma}
\label{lem:convert distinguisher}
Let all parameters be as in Corollary~\ref{corollary:info loss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-fuzzy extractor with distributional advise (see Definition~\ref{def:fe distributional}) if
\[
\epsilon<1/2-\left(\epsilon_1+\epsilon_2\right)\\
\]
where 
\begin{align*}
\epsilon_1&= 2^{-\kappa-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $r, p, z$ that are correctly generated by the fuzzy extractor.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher}].
We prove the result for an average member of $Z$.
Consider the following distinguisher $\mathcal{D}$ for triples of the form $r, p, z$:
\begin{enumerate}
\item If $r \not\in \mathsf{GoodKey}_p$ output $1$,
\item If $\sum_{i} \viable(w_{z, i}, r, p, \advise(z)) =0 $ output $0$,
\item Else output $1$.
\end{enumerate}
First note that by perfect correctness it is always the case that when given $\mathsf{key}, p$ that $\mathcal{D}$ outputs $1$.  We proceed to bound the probability that $\mathcal{D}$ outputs $1$ when given $U_\kappa, p$.  Note that the probability that $\Pr[U_\kappa \in \mathtt{GoodKey}_p] \ge 1/2$ by the definition of $\mathtt{GoodKey}_p$. 

We bound the number of parts with at least one point in viable.  We begin by assuming that all points in viable are in different  values $r$ so the bound on the size of 
\[
\left\{w_{z, i} \middle|\sum_{\mathsf{key}\in\mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise)>0\right\}_{i=1}^k 
\] 
gives a bound on the number of nonempty parts. By Corollary~\ref{corollary:info loss} 
\[
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\]
Thus the fraction of non-empty parts in $\mathsf{Goodkey}_p$ on average is at most 
\[
2^{-\kappa-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\frac{\nu+1}{2^\kappa}.
\]
Thus, the probability that $\mathcal{D}$ outputs $0$ when given $U_\kappa, p, z$ is at least 
$1/2-(\epsilon_1+\epsilon_2)$
where 
\begin{align*}
\epsilon_1:&=2^{-\kappa-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}},\\
\epsilon_2:&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
\noindent
This completes the Proof of Lemma~\ref{lem:convert distinguisher}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:main theorem}]
Restating Lemma~\ref{lem:convert distinguisher} one has that for $(R_{U_{n,k}},P_{U_{n,k}}) \leftarrow \gen(U_{n,k}, \advise(Z_{U_{n,k}}))$
\[
\Delta((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}), (U_n, P_{U_{n,k}}, Z_{U_{n,k}}))\ge \epsilon_1+\epsilon_2.
\]
Let $\mathcal{D}$ be one distinguisher that always outputs $1$ on any value $\mathsf{key}, p, z$ for any distribution $z$, then
\begin{align*}
\Pr[\mathcal{D}((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}))=1] &=1\\
\Pr[\mathcal{D}(U_n, P_{U_{n,k}}, Z_{U_{n,k}})=1]&\le 1-(\epsilon_1+\epsilon_2).
\end{align*}
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_3$ by Lemma~\ref{lem:close family} .  Define \[(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\leftarrow \gen(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advise(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
it is thus true that 
\begin{align*}
\Pr[\mathcal{D}(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&=1\\
\Pr[\mathcal{D}(U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&\le 1-(\epsilon_1-\epsilon_2).
\end{align*}
and thus that 
\[
\Delta((R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}), (U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\ge \epsilon_1-\epsilon_2.
\]

\noindent
Finally, the theorem follows by application of Lemma~\ref{lem:distributional advise suffices} with the setting of $\zeta = 1/4$.
\end{proof} 

\section{Secure Sketches}
\label{sec:ss}
This section creates an upper bound on the quality of efficient secure sketches.  This bound considers has stronger parameters than Theorem~\ref{thm:main theorem} due largely to the difference between Lemmas~\ref{lem:smallgeneralviable} and~\ref{lem:smallgeneralviable ss}.

\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters.
\begin{enumerate}
\itemsep0em
\item Define $2^\beta:=\frac{2^n-|B_t|}{|B_t|}$ and suppose that $2^\beta\ge 1$ which is satisfied as long as $t< n/2 $. 
\item Let $\nu \in \mathbb{Z}^+$ be a free parameter, and
\item Let $\mu =(n(1-h_2(t/n)) +h_2(2\delta))/(1-2\delta)$.
\end{enumerate}
For half of the distributions $W\in \mathtt{PCode}_{n, k, t, \gamma}^{*}$ there is no $(\{0,1\}^n, W, \tilde{m}, t, \epsilon,\delta, \ell)$-secure sketch for 
\[
\tilde{m}\ge  -\log{1-\epsilon'} +1 + 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
where $\epsilon' = \epsilon+\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.$
\label{thm:main theorem ss}
\end{theorem}

\subsection{Analysis of parameters}
We assume that $\epsilon\le 1/4$ and $\delta<1/4$.  As before for  
\[
\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n = \ngl(n)\]  it suffices 
\begin{align*}
 \gamma &\le \beta -\log{2e}.\\
k &\ge \gamma + \log{n+ \omega(\log{n})}.
\end{align*}
These two conditions imply that $\epsilon'\le 1/2$ and $-\log{1-\epsilon'}\le 1$.
We assume that $\nu = \poly(n)$ and define a constant $0 < c_3< 1$ such that  $\log{\nu+1} \le c_3\log{n}$.
As in Section~\ref{ssec:analysis params} the additional constraint that 
\[
\gamma \le \beta - \log{2e}
\le n(1-h_2(t/n)) - \log{2e}\] imposes an additive $\log{2e}$ impact on the maximal fuzzy min-entropy that can be supported. 
For some arbitrary constant $0<c_1 < 1$ We set $k = \gamma + c_1n$. 
Define \[\chi: =-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu+k\]
We now turn to our analysis of $\chi$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le \frac{(n(1-h_2(t/n)) +h_2(2\delta))}{(2\delta)},\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
\end{align*}
This implies that 
\begin{align*}
\chi&= -\frac{\alpha +\log{\nu}-\ell-k}{2^k} + \mu +k+1\\
&\le  -\frac{\alpha +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}  + k+1,\\
&\le - \frac{(n-k)2^k +\log{\nu}-\ell-k}{2^k} +\frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta} + k+1,\\
&\le  -\frac{(n-k)2^k-k2^{k} +\log{\nu}-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}+1\\
&< - \frac{(n-2k)2^k-\ell-k}{2^k} + \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta} +1.
\end{align*}
Let $0<c_2<c_1$ be an arbitrary constant. 
As long as $\ell\le 2(c_1-c_2) n2^k -k= 2(c_1-c_2)n2^{\gamma+c_1n}-(\gamma+c_1n)$ and $\delta < 1/4$ then 
\begin{align*}
0 &\ge -\frac{(n-2k)2^k-\ell-k}{2^k} + 2n(1-h_2(t/n)) +2\\
 &\ge -(n-2c_2n - 2\gamma) + 2n(1-h_2(t/n)) +2\\
&\ge -n+2c_2n+2\gamma + 2n(1-h_2(t/n))+2\\
\gamma &\le \frac{n(-1-2c_2 + 2h_2(t/n))}{2}-1.\\
\end{align*}
Adding the constraint from \textbf{Distributional Closeness}
this means that 
\[
\frac{\gamma}{n} = \Theta\left(\min\left\{(1-h_2(t/n)) - \frac{\log{2e}}{n}, \frac{(1-2c_2 - 2h_2(1-t/n))}{2}\right\}\right).
\]
suffices to ensure $\tilde{m} = \Theta(\log n)$ if $\ell\le 2(c_1-c_2)n2^{\gamma+c_1n}-(\gamma+c_1n)$ .
When $\delta = 0$ one has
\[
\gamma \le \frac{n(2c_2 + h_2(t/n))}{2}-1.\]
yielding
\[
\frac{\gamma}{n} \le \Theta\left(\left\{(1-h_2(t/n)) - \frac{\log{2e}}{n}, \frac{(2c_2 + h_2(1-t/n))}{2}\right\}\right).
\]
if $\ell\le 2(c_1-c_2)n2^{\gamma+c_1n}-(\gamma+c_1n)$.

\subsection{Proof}
\begin{proof}[Proof of Theorem~\ref{thm:main theorem ss}]
We begin with a corollary bounding how many points are ``viable'' from the output of the secure sketch.


\begin{corollary}
\label{corollary:info loss ss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise = \aux(Z)$ be of length $\ell$.       For every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$ and for any fixed $\sketch$,
    
    \[
    \mu:=\log{|\{v \in \mathcal{M} | \sketch \in \goodsketch_v\}|} \le \frac{n - \log{|B_t|} +h_2(2\delta)}{1-2\delta} \le \frac{n(1-h_2(t/n)) +h_2(2\delta)}{1-2\delta}.
    \]
For a triple $(v, ss, z)$ define $\viable(v, ss, z)=1$ if
\begin{enumerate}
\itemsep0em
\item $\Pr[\gen(v, \advise(z)) = ss]>0$,
\item $ss\in\goodsketch_v$, and
\item $\Pr[W_z = v]>0$.
\end{enumerate}
and set $\viable(v, ss, z)=0$ otherwise. 
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, then it is true for each $\forall i\not \in \mathcal{I}_\advise, \forall ss$
\[
\Pr_{z\leftarrow Z}\left[\viable(w_{z,i},ss,z) =1\right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu - (k-\log{\nu})}.
\]
And thus, for on average across $Z$, for all $ss$ the expected number of points outside of $\mathcal{I}_\advise$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i | w_{z,i}\not\in\mathcal{I}_\advise} \viable(w_{z,i}, ss, z) \right]< 2^{-\frac{\alpha- |\advise|}{2^k}+1+\mu+2k-\log{\nu}}.
\end{align*}
And finally, for all $ss$
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i }  \viable(w_{z,i}, ss, \advise)  \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
\end{corollary}
\noindent
The above corollary follows by combination of Lemma~\ref{lem:smallgeneralviable ss} and \ref{lem:entr of members}.
\end{proof}
\begin{lemma}
\label{lem:convert distinguisher ss}
Let all parameters be as in Corollary~\ref{corollary:info loss ss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon_\sketch, \delta, \ell)$-secure sketch with distributional advise (see Definition~\ref{def:ss distributional}) if
\[
\tilde{m}> -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]

Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $w, ss, z$ that are correctly generated by the secure sketch.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher ss}]
We prove the result for an average member of $Z$.  First note that for every $v\in \mathcal{M}$ there exists a set $\goodsketch_v$ where $\Pr[\sketch(v)\in \goodsketch_v] \ge 1/2$.  By Lemma~\ref{lem:smooth ent cond}
\[
\Haveps(W| \sketch(W)) \le \Haveps(W| \sketch(W), \sketch(W)\in \goodsketch_W) +1.
\]
We now restrict our attention to the case when $\sketch(w) \in \goodsketch_w$.   
Let $X', Y'$ be distributions  be an event where $\Delta((W, \sketch(W)), (X', Y'))\le \epsilon$. Note that 
\[
\Pr[\viable(W, \sketch(W), Z) = 1| \sketch_W \in \goodsketch_W] = 1.
\]
Thus, it must be the case that 
\[
\Pr[\viable(X', Y', Z) = 1] \ge 1-\epsilon.
\]
By the definition of $\viable$, the support of $X'$ must be drawn from points in $W_Z$.  For any fixed value of $y\in Y$ by Corollary~\ref{corollary:info loss ss}, it is true that 
\begin{align*}
\expe_{z\leftarrow Z} \left[\sum_{i }  \viable(w_{z,i}, y', \advise)  \right]< 2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1.
\end{align*}
One has, 
\begin{align*}
2^{-\Hav(X'|Y')} &\ge  \Pr[X' \in W_Z] 2^{-\Hav(X'|Y', X'\in W_Z) }+ \Pr[X'\not\in W_Z] 2^{-\Hav(X'|Y', X'\not\in W_Z)}\\
 &\ge  (1-\epsilon)2^{-\Hav(X'|Y', X'\in W_Z) }
\end{align*}
And thus, 
\[
\Hav(X'|Y') \le  -\log{1-\epsilon_\sketch} +\Hav(X'|Y', X'\in W_Z).
\]
The min-entropy of $X' |Y'$ is maximized by considering the uniform distribution over such points. \begin{align*}
\Hav(X'|Y' , X'\in W_Z) &\le \log {2^{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}}+\nu+1}\\
&\le 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\end{align*}
This implies that 
\[
\Haveps(W| \sketch(W)) \le -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.
\]
This completes the Proof of Lemma~\ref{lem:convert distinguisher ss}.
\end{proof}

\noindent
We now proceed to the proof of Theorem~\ref{thm:main theorem ss}.   Let \[\chi:= -\log{1-\epsilon_\sketch} +1 + 2\max\left\{-\frac{\alpha+-|\advise|}{2^k}+1+\mu+2k-\log{\nu}, \log{\nu+1}\right\}.\]  Restating Lemma~\ref{lem:convert distinguisher} one has that 
\[
\Hav^{\epsilon_\sketch}(U_{n,k} | \sketch(U_{n,k}), Z_{U_{n,k}})) \le \chi.
\]
That is, for all $A, B, C$ such that $\Delta((A, B, C), (U_{n,k},\sketch(U_{n,k}), Z_{U_{n,k}}))\le \epsilon_{\sketch}$ it is true that $\Hav(A|B, C) \le \tilde{m}$.  Define \[P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}\leftarrow \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advise(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_{\mathtt{PCode}}$ by Lemma~\ref{lem:close family} .  
It is thus true that 
\[
\Hav^{\epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}}(\mathtt{PCode}_{n, k, t, \alpha}^{*} | \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\le \tilde{m}.
\]
Suppose not, then there exists some $A, B, C$ where 
\[
\Delta((A, B, C), (\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}.
\]
and $\Hav(A|B, C)>\chi$.
Thus, 
\begin{align*}
\Delta&((A, B, C), (U_{n,k},\sketch(U_{n,k}), Z_{U_{n,k}}))\\&\le \Delta((A, B, C), ((\mathtt{PCode}_{n, k, t, \alpha}^{*} , \sketch(\mathtt{PCode}_{n, k, t, \alpha}^{*}), Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))+\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*})\\
&\le \epsilon_{\sketch}-\epsilon_{\mathtt{PCode}}+ \epsilon_{\mathtt{PCode}} = \epsilon_{\sketch}.
\end{align*}
This contradicts the fact that $\Hav^{\epsilon_\sketch}(U_{n,k} | \sketch(U_{n,k}), Z_{U_{n,k}})) \le \chi.$
Finally, Theorem~\ref{thm:main theorem ss} follows by application of Lemma~\ref{lem:distributional advise suffices ss} with setting $\zeta = \chi$ and noting that $\chi\ge 1$.



%\begin{align*}
%2^{(n-k)2^k}+2^{2k}-|\advise|-k - 2^knh_2(1/2-t/n) \ge 0\\
%\end{align*}
%Or equivalently that, 
%\begin{align*}
%|\advise| &\le 2^{(n-k)2^k+2k} -k - 2^knh_2(1/2-t/n)\\
%&\le 2^{\Theta(n)2^{\Theta(n)}}.
%\end{align*}


