%!TEX root = main.tex

% Proof for NegFE

\section{Main Result}
\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \nu \in\mathbb{Z}^+$ be paraemters.
For the  family $ \mathtt{PCode}_{n, k, t, \gamma}^{*}$ there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-efficient fuzzy extractor for 
\[
\epsilon< \epsilon_1-\epsilon_2
\]
For 
\begin{align*}\epsilon_1&:= 1/2-\left(\frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k+\kappa+1}}}+\frac{\nu+1}{2^{\kappa+1}}\right)\\\epsilon_2 &:=2^{n+k\log e- nk2^{-\gamma}(1-h_2(t/n))}.\end{align*}
\label{thm:main theorem}
\end{theorem}

Our proof uses the following structure
\begin{enumerate}
\item Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string.
\item Lemma~\ref{lem:info loss} We then further restrict this setting by showing that in order for an adversary to succeed on average they have to be able to align these viable points with a distribution that they have only a single sample and a polynomial length advice sting. 

 We then argue that for large high entropy distributions this advice string can only reduce the entropy of a large fraction of viable points by a small amount. 
\item Lemma~\ref{lem:convert distinguisher} Then, we show that this small reduction of entropy for each point in the distribution means that on average the adversary cannot align the viable points with the distribution and there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

\noindent
We present Lemmas~\ref{lem:info loss} and \ref{lem:convert distinguisher} first and then return to the proof of Theorem~\ref{thm:main theorem}.

\paragraph{The family $\mathcal{W}$}
We consider the following family $Z$ which chooses $k$ points without replacement from the space $\{0,1\}^n$.  That is, if one samples $Z$ uniformly then one obtains the distribution $U_{n,k}$. We note there are distributions $W_Z$ in this family that have little fuzzy min-entropy.  However, we will show that this family is statistically close to a family where every distribution has fuzzy min-entropy.  For the remainder of this proof we consider $U_{n,k}$ and then show that the fuzzy extractor cannot perform differently on the family $\mathtt{PCode}_{n, k, t, \alpha}^{*}$ where every distribution does have fuzzy min-entropy. For convenience we use $W_{z, 1},..., W_{z,k}$ to describe the distribution $k$ points with nonzero probability in $W_z$ for some value $z$.  We assume no ordering over these points.  For all values $x, y$, 
\[
\Pr[W_{z, i} =x | W_{z, j} = y] = \begin{cases} \frac{1}{2^n-1} &x\neq y\\0&x=y.\end{cases}
\]


\begin{lemma}
\label{lem:info loss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise = \aux(Z)$ be of length $\ell$.  For a quad $v, p, r, \advise$ define 
\[
\viable(v, p, r, \advise) = \begin{cases} 1& \Pr[\gen(v, \advise) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
Fix some $p$ and let $\mathtt{GoodKey}_p$ be defined as in Lemma~\ref{lem:smallgeneralviable}. Suppose that 
\[
   \sum_{\mathsf{key}\in \mathtt{GoodKey}_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v)}}|}\right) \le 2^\mu 
 \]
 For a particular $Z=z$ let $w_{z,1},..., w_{z,k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose k}$.  
 Then for each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, then it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\Pr\left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise, z) \ge 1\middle| \begin{aligned} z\leftarrow Z\\ \advise= \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise) \end{aligned} \right]< \frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu}}{{2^{\alpha/k}}}.
\]
And thus, for on average across $Z$, the expected number of points outside of $\mathcal{I}_\advise$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i | w_{z,i}\not\in\mathcal{I}_\advise} \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< \frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k}}}.
\end{align*}
And finally, 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< \frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k}}}+\nu+1.
\end{align*}

\end{lemma}
\begin{proof}
Let $Z$ and $\advise$ be defined as above. We begin by noting that \[
\Hoo(W_{z, 1},..., W_{z, k})=\alpha.\]
 It is thus, the case that 
\[
\Hav(W_{z, 1},..., W_{z, k} | \advise) \ge \alpha - |\advise|.
\]
\begin{claim}
One has that 
\begin{align*}
\expe_{i=1}^k \left[\Hav(W_{z, i} | \advise)\right] \ge \frac{\alpha - |\advise|}{k}.
\end{align*}
\label{clm:entropy distributes}
\end{claim}
\begin{proof}
We begin it is true that $\forall w_{z,1},..., w_{z,i-1}$ that 
\[
\Hav(W_{z,i} | \advise, W_{z, 1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}) \le \Hav(W_{z,i} | \advise).
\]
This is because conditioned on $W_{z, j} =w_{z,j}$ only removes the outcome $w_{z,j}$ from the support of $W_{z,i}$ increasing all other outcomes proportionally.  
We now proceed to the proof of the claim. 

Suppose not, then there exists the following predictor $\mathcal{P}$ for the joint distribution $W_{z, 1},..., W_{z, k} | \advise$:
\begin{enumerate}
\item For $i=1$ to $k$ predict $w_{z,i}\leftarrow W_{z, i}| \advise, W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}$
\item Output the joint prediction $w_{z,1},..., w_{z, k}$.  
\end{enumerate}
Let $\alpha_i$ denote the values $\Hav(W_{z,i}| \advise) = \alpha_i$ for $i=1$ to $k$. 
The probability that $\mathcal{P}$ issues a correct prediction is
\begin{align*}
\prod_{i=1}^k 2^{-\Hav(W_i | \advise,  W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1})} \ge \prod_{i=1}^k 2^{-\Hav(W_i | \advise)} = \prod_{i=1}^k 2^{-\alpha_i}  = 2^{-\sum_{i=1}^k \alpha_i}.
\end{align*}

\noindent
Suppose that $\sum_{i=1}^k \alpha_i < \alpha-\advise$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advise)}$, a contradiction.  Thus, $\expe_{1\le i\le k} \alpha_i \ge (\alpha-\advise)/k$. 
This completes the proof of Claim~\ref{clm:entropy distributes}.
\end{proof}

\noindent
Now by Lemma~\ref{lem:markovpred} it is true that there exists a set $\mathcal{I}\subseteq \{1,...,k\}$ where $\beta: =  k/| \mathcal{I} | = k/\nu$ such that for all $i\not \in \{1,...,k\} \setminus\mathcal{I}$ is true that 
\[
\Hav(W_{z,i} | \advise) \ge \frac{\alpha -|\advise| - \log \beta}{k} = \frac{\alpha -|\advise| - \log k + \log{\nu}}{k}.
\]

Let $i^*$ denote the index of the point that will be given to $\gen$ that is $p\leftarrow (w_{z,i^*}, \advise)$.  We define the set $\mathcal{I}_\advise = \mathcal{I} \cup \{w_{z,i^*}\}$ where $w$ is the point given to $\gen(w, \advise)$.  
Let \[\gamma:=\frac{\alpha -|\advise| - \log \beta}{k},\] 
then for $i\not\in \mathcal{I}_{\advise}$ it is true that 
\begin{align*}
\Hav(W_{z,i} | \mathsf{key} \in \mathtt{GoodKey}, p)&\ge \\
\Hav(W_{z,i} | \advise, W_{z,i^*}= w_{z,i^*}) &\ge -\log{\frac{1}{2^{\gamma}-1}} \ge \gamma-1.
\end{align*}
We now proceed to bounding $\viable(w_i, p, \mathsf{key})$.  By assumption, By Lemma~\ref{lem:smallgeneralviable} we know that there are at most $2^\mu$ points in $\viable(w_i, p, \mathsf{key})$.  Thus, for all $i\not \in\mathcal{I}_{\advise}$ 

\begin{align*}
\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advise) = 1 \right]&\le 2^{- (\gamma-1 -\mu)}\\
&=2^{-(\frac{\alpha-|\advise|-\log{\beta}}{k}-1-\mu)}.
\end{align*}
\end{proof}

\begin{lemma}
\label{lem:convert distinguisher}
Let all parameters be as in Lemma~\ref{lem:info loss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-fuzzy extractor with distributional advise (see Definition~\ref{def:fe distributional}) if
\[
\epsilon<1/2-\left(\frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k+\kappa}}}+\frac{\nu+1}{2^{\kappa}}\right).
\]
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $r, p, z$ that are correctly generated by the fuzzy extractor.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher}].
We prove the result for an average member of $Z$.  Consider the following distinguisher $\mathcal{D}$ for triples of the form $r, p, z$:
\begin{enumerate}
\item If $r \in \mathsf{GoodKey}_p$ output $1$,
\item Compute $\viable(z)$.
\item If $\sum_{i} \viable(w_{z, i}, r, p) =0 $ output $0$,
\item Else output $1$.
\end{enumerate}
First note that by perfect correctness it is always the case that when given $\mathsf{key}, p$ that $\mathcal{D}$ outputs $1$.  We proceed to bound the probability that $\mathcal{D}$ outputs $1$ when given $U_\kappa, p$.  Note that the probability that $\Pr[U_\kappa \in \mathtt{GoodKey}_p] \ge 1/2$ by the definition of $\mathtt{GoodKey}_p$. 

We bound the number of parts with at least one point in viable.  We begin by assuming that all points in viable are in different  values $r$ so the bound on the size of 
\[
\left\{w_{z, i} \middle|\sum_{\mathsf{key}\in\mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise)>0\right\}_{i=1}^k 
\] 
gives a bound on the number of nonempty parts. By Lemma~\ref{lem:info loss} 
\[
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< \frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k}}}+\nu+1.
\]
Thus the fraction of non-empty parts in $\mathsf{Goodkey}_p$ on average is at most 
\[
\frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k+\kappa}}}+\frac{\nu+1}{2^\kappa}.
\]
Thus, the probability that $\mathcal{D}$ outputs $0$ when given $U_\kappa, p, z$ is at least 
\[
1/2-\left(\frac{2^{\frac{|\advise|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k+\kappa}}}+\frac{\nu+1}{2^{\kappa}}\right).
\]
\noindent
This completes the Proof of Lemma~\ref{lem:convert distinguisher}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:main theorem}]
Restating Lemma~\ref{lem:convert distinguisher} one has that for $(R_{U_{n,k}},P_{U_{n,k}}) \leftarrow \gen(U_{n,k}, \advise(Z_{U_{n,k}}))$
\[
\Delta((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}), (U_n, P_{U_{n,k}}, Z_{U_{n,k}}))\ge \epsilon_1.
\]
\todo{make it clear in the lemma statement that such a thing exists.}Noting that the above distinguisher always outputs $1$ on any value $\mathsf{key}, p, z$ for any distribution $z$ 
Let $\mathcal{D}$ be one distinguisher such that 
\begin{align*}
\Pr[\mathcal{D}((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}))=1] &=1\\
\Pr[\mathcal{D}(U_n, P_{U_{n,k}}, Z_{U_{n,k}})=1]&\le 1-\epsilon_1.
\end{align*}
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_2$ by Lemma~\ref{lem:close family} .  Define \[(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\leftarrow \gen(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advise(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
it is thus true that 
\begin{align*}
\Pr[\mathcal{D}(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&=1\\
\Pr[\mathcal{D}(U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&\le 1-(\epsilon_1-\epsilon_2).
\end{align*}
and thus that 
\[
\Delta((R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}), (U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\ge \epsilon_1-\epsilon_2.
\]

\noindent
Finally, the theorem follows by application of Lemma~\ref{lem:distributional advise suffices}.
\end{proof} 

\paragraph{Analysis of parameters}
Substituting that $\mu\le n(1-2/\ln 2 (t/n)^2).$ and ${2^n\choose k} \ge 2^{nk} /k^k$.
\begin{align*}
\epsilon_1 &\le \frac{1-\left(\frac{2^{\frac{|\ell|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k+\kappa}}}+\frac{\nu+1}{2^\kappa}\right)}{2}\\
 &\le \frac{1}{2}-\left(\frac{2^{\frac{|\ell|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{n-\log k+\kappa}}}+\frac{\nu+1}{2^{\kappa+1}}\right)\\
  &\le \frac{1}{2}-\left(\frac{1}{{2^{n-\log k+\kappa-(\frac{|\ell|+\log{k}-\log{\nu}}{k}+1+\mu+k)}}}+\frac{\nu+1}{2^{\kappa+1}}\right)\\
    &\le \frac{1}{2}-\left(\frac{1}{{2^{nh_2(1-t/n)-\log k+\kappa-(\frac{|\ell|+\log{k}-\log{\nu}}{k}+1+k)}}}+\frac{\nu+1}{2^{\kappa+1}}\right)
 \end{align*}
   Add in \[\epsilon_2\le \Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \gamma}^{*}) \le 2^{n+k\log e- nk2^{-\gamma}(1-h_2(t/n))}.\]


\[
\epsilon> \frac{1}{2}-\left(\frac{2^{\frac{|\ell|+\log{k}-\log{\nu}}{k}+1+\mu+k}}{{2^{\alpha/k+\kappa}}}+\frac{\nu+1}{2^\kappa}+\frac{2^{n+k\log e}}{2^{nk2^{-\gamma}(1-h_2(t/n))}}\right)
\]

\begin{enumerate}
\item $\ell$ length of advice large polynomial.
\item $k$ Number of points in code, think exponential in $n$.
\item $\gamma$ required entropy think $\omega(\log n)$ but higher is better.
\item $\alpha$ just shorthand for $ \log {2^n\choose k}$.
\item $\kappa$ length of key, shorter is better for result.
\item $\nu$ number of points we give away for free, this is a free parameter.
\item $\mu$ is our bound for number of viable points.  So far without errors this is $\mu:= n \cdot \prns{1 - \frac{2}{\ln{2}} \cdot (t/n)^{2}}$.
\item $t$ is distance corrected by fuzzy extractor, lower is better for result.
\end{enumerate}

Want three things to be negligible at the same time:
\[\frac{\nu+1}{2^\kappa} 
\]
Requires $\nu$ to be a super polynomial amount smaller than $2^\kappa$.  Requires $\kappa = \omega(\log \lambda)$.
 
 \[
\frac{2^{n+k\log e}}{2^{nk2^{-\gamma}(1-h_2(t/n))}}
 \]
 Think of $k$ as being fixed.  Then the requirement is that $n 2^{-\gamma}(1-h_2(t/n))>1$.  Think of $1-h_2(t/n) = \Theta(1)$ then this requires that $n2^{-\gamma} >1$  Requires a very big $n$ and is problematic. 
\subsection{Proof Setting}
Consider $\mathcal{W}$ such that each $W \in \mathcal{W}$ is a set of $2^{\phi}$ uniformly chosen independent random points in $\zeroone{n}$. 
Let $|\mathcal{W}| = r$ and let $Z \in \br{r}$ be an indexing variable for the selection of $W_Z$ from $\mathcal{W}$. 

In this proof the goal is to show that a fuzzy extractor cannot hope to hide the input point from an outside party. 
We will call the party that is building the Fuzzy Extractor the constructor and the party attempting to distiguish two settings the distinguisher. 
The Fuzzy Extractor is set up with an enrolled sample from our two stage sampling proceedure before the distiguishing game begins.
The game that the distinguisher plays is given one of two triples, real or random, do distinguish the realm to which the triple belongs. 
The real triple is the key value corresponding to the enrolled value in the fuzzy extractor, the public value produced by Gen, and the description of the distribution $W_Z$.
The random triple is the same except the key value corresponding to the enrolled value is substituted with a uniform value in the domain of the key values.

In this setting the constructor when creating the Fuzzy Extractor is aided by another party, we call this party the advisor. 
The advisor gets a full description of the distribution $W_Z$ and is allowed unbounded computation and time to produce an advice string $\mathsf{info}$. 
The advisor and constructor are unbounded in their computation and time, but the length of this string is required to be polynomial in the security parameter $\lambda$. 
The advice string is then communicated to the constructor, who also gets a sample from the distribution $w \in W_Z$. 
The constructor then is tasked with creating a Fuzzy Extractor, specifically choosing a public value $\mathsf{pub}$ which induces a partition over the space $\zeroone{n}$ and a key labeling of the partition which determines a key value for the initial sample.

\subsection{Maximum size of a Fuzzy Extractor}
From Lemma \ref{lem:smallgeneralviable}, we have the largest size of a set of viable points is $2^{\psi}$ where $\psi = n \cdot \prns{1 - \frac{2}{\ln{2}} \cdot \tau^{2}}$.

Now, it is clear from our setting that when building the Fuzzy Extractor the constructor has a single point $w$, a full description of the family of distributions $\mathcal{W}$, and the prepared advice string $\mathsf{info}$ about the specific selected distribution $W_Z$. 
We are primarily concerned with the ability of the constructor to align the points in the selected $W_Z$, with the partition induced by $\mathsf{pub}$. 
Clearly, if $\mathsf{info}$ is allowed to describe every point in $W_Z$ (or every point but $w$) then the constructor can include a point from $W_Z$ in each viable section of the partition induced by $\mathsf{pub}$. 
Fortunately for us, the distribution has exponential entropy and the advice string has polynomial length so the advice string cannot describe the entire remainder of the distribution. 
The question remains, how much entropy remains in the distribution after seeing the advice string? 

Since each point in the distribution is independent and uniform in $\zeroone{n}$ the beginning  entropy (and min-entropy) of the distribution is $|W_Z| \cdot n$. 
Now we consider the advice string; since this string is allowed to arbitrarily depend on the distribution we can upperbound the min-entropy of the distribution conditioned on the advice string using a standard min-entropy argument found in Lemma \ref{lem:conditionalminentloss}. 
\[
    \acminent{W_Z}{\mathsf{info}} = |W_Z| \cdot 2^n - \log{|info|}
\]