%!TEX root = main.tex

% Proof for NegFE

\section{Main Result}
\begin{theorem}
Let $\gamma \in\mathbb{R}^+, n, \kappa, t, \ell, \gamma \in\mathbb{Z}^+$ be parameters such that $|B_t|/2^n\ge 2$ which is satisfied as long as $t< n/2 $. Define \[2^\beta:=\frac{2^n-|B_t|}{|B_t|}\ge 1,\] 

Let $\nu \in \mathbb{Z}^+$ be a free parameter.
For the  family $ \mathtt{PCode}_{n, k, t, \gamma}^{*}$ there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-efficient fuzzy extractor for 
\[
\epsilon< 1/2 - (\epsilon_1+\epsilon_2+\epsilon_3)
\]
For 
\begin{align*}\log{\epsilon_1}&:= -\left(\kappa+\frac{\alpha +\log{\nu}-|\advise|-k}{2^k} - \mu -2^k\right),\\
\epsilon_2&:=\frac{\nu+1}{2^{\kappa+1}}\\
\epsilon_3&:=\left(e2^{\gamma-\beta}\right)^{2^{k-\gamma}}2^n.
\end{align*}
\label{thm:main theorem}
\end{theorem}

Our proof uses the following structure
\begin{enumerate}
\item Lemma~\ref{lem:smallgeneralviable} which bounds the number of ``viable'' points for most public values $p$.  Note that this Lemma bounds the total number of points and holds even if $\gen, \rep$ have access to an arbitrary advice string.
\item Lemma~\ref{lem:info loss} We then further restrict this setting by showing that in order for an adversary to succeed on average they have to be able to align these viable points with a distribution that they have only a single sample and a polynomial length advice sting. 

 We then argue that for large high entropy distributions this advice string can only reduce the entropy of a large fraction of viable points by a small amount. 
\item Lemma~\ref{lem:convert distinguisher} Then, we show that this small reduction of entropy for each point in the distribution means that on average the adversary cannot align the viable points with the distribution and there exists a distinguisher that can distinguish a uniform triple from a key triple. 
\end{enumerate}

\noindent
We present Lemmas~\ref{lem:info loss} and \ref{lem:convert distinguisher} first and then return to the proof of Theorem~\ref{thm:main theorem}.

\paragraph{The family $\mathcal{W}$}
We consider the following family $Z$ which chooses $2^k$ points without replacement from the space $\{0,1\}^n$.  That is, if one samples $Z$ uniformly then one obtains the distribution $U_{n,k}$. There are distributions in this family with little fuzzy min-entropy.  As shown in Lemma~\ref{lem:close family} $U_{n,k}$  is statistically close to $\mathtt{PCode}_{n, k, t, \gamma}^{*}$, a family with fuzzy min-entropy.  For the remainder of this proof we consider $U_{n,k}$ and then show that the fuzzy extractor cannot perform differently on the family $\mathtt{PCode}_{n, k, t, \gamma}^{*}$ where every distribution does have fuzzy min-entropy. 

For convenience we use $W_{z, 1},..., W_{z,2^k}$ to describe the $2^k$ points with nonzero probability in $W_z$ for some value $z$.  We assume no ordering over these points.  For all values $x, y$, 
\[
\Pr[W_{z, i} =x | W_{z, j} = y] = \begin{cases} \frac{1}{2^n-1} &x\neq y\\0&x=y.\end{cases}
\]


\begin{lemma}
\label{lem:info loss}
Let $Z$ describe a uniform sample of $U_{n,k}$.  Let $(\gen, \rep, \aux)$ be an auxiliary input fuzzy extractor.  Let $\advise = \aux(Z)$ be of length $\ell$.  For a tuple $(v, p, r, \advise)$ define 
\[
\viable(v, p, r, \advise) = \begin{cases} 1& \Pr[\gen(v, \advise) = (r, p)]>0\\0&\text{otherwise}\end{cases}.\]
Fix some $p$ and let $\mathtt{GoodKey}_p$ be defined as in Lemma~\ref{lem:smallgeneralviable}. Suppose that 
\[
   \sum_{\mathsf{key}\in \mathtt{GoodKey}_p} \left( \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v)}}|}\right) \le 2^\mu 
 \]
 For a particular $Z=z$ let $w_{z,1},..., w_{z,2^k}$ denote the points in $W_Z$. Let $\alpha:= \log {2^n\choose 2^k}$.  
 Then for each value $\advise$ there is some set $\mathcal{I}_{\advise}$ of size at most $\nu+1$, then it is true for each $i\not \in \mathcal{I}_\advise$ that
\[
\Pr\left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise, z) \ge 1\middle| \begin{aligned} z\leftarrow Z\\ \advise= \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise) \end{aligned} \right]< 2^{-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu}.
\]
And thus, for on average across $Z$, the expected number of points outside of $\mathcal{I}_\advise$ that are included in $\viable$ is at most 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i | w_{z,i}\not\in\mathcal{I}_\advise} \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha+\log{\nu} - |\advise|-k}{2^k}+1+\mu+k}.
\end{align*}
And finally, 
\begin{align*}
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu+k}+\nu+1.
\end{align*}

\end{lemma}
\begin{proof}
Let $Z$ and $\advise$ be defined as above. We begin by noting that 
\[
\Hoo(W_{z, 1},..., W_{z, 2^k})=\alpha.
\]
 It is thus, the case that 
\[
\Hav(W_{z, 1},..., W_{z, 2^ k} | \advise) \ge \alpha - |\advise|.
\]
\begin{claim}
One has that 
\begin{align*}
\expe_{i=1}^{2^k} \left[\Hav(W_{z, i} | \advise)\right] \ge \frac{\alpha - |\advise|}{2^k}.
\end{align*}
\label{clm:entropy distributes}
\end{claim}
\begin{proof}
It is true that $\forall w_{z,1},..., w_{z,i-1}$ that 
\[
\Hav(W_{z,i} | \advise, W_{z, 1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}) \le \Hav(W_{z,i} | \advise).
\]
This is because conditioned on $W_{z, j} =w_{z,j}$ only removes the outcome $w_{z,j}$ from the support of $W_{z,i}$ increasing all other outcomes proportionally.  
We now proceed to the proof of the claim. 

Suppose not, then there exists the following predictor $\mathcal{P}$ for the joint distribution $W_{z, 1},..., W_{z, k} | \advise$:
\begin{enumerate}
\item For $i=1$ to $k$ predict $w_{z,i}\leftarrow W_{z, i}| \advise, W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1}$
\item Output the joint prediction $w_{z,1},..., w_{z, k}$.  
\end{enumerate}
Let $\alpha_i$ denote the values $\Hav(W_{z,i}| \advise) = \alpha_i$ for $i=1$ to $k$. 
The probability that $\mathcal{P}$ issues a correct prediction is
\begin{align*}
\prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advise,  W_{z,1}=w_{z,1},..., W_{z,i-1}=w_{z,i-1})} \ge \prod_{i=1}^{2^k} 2^{-\Hav(W_i | \advise)} = \prod_{i=1}^{2^k} 2^{-\alpha_i}  = 2^{-\sum_{i=1}^{2^k} \alpha_i}.
\end{align*}

\noindent
Suppose that $\sum_{i=1}^{2^k} \alpha_i < \alpha-\advise$ then $\mathcal{P}$ correctly predicts with probability greater than $2^{-(\alpha-\advise)}$, a contradiction.  Thus, $\expe_{1\le i\le k} \alpha_i \ge (\alpha-\advise)/2^k$. 
This completes the proof of Claim~\ref{clm:entropy distributes}.
\end{proof}

\noindent
Now by Lemma~\ref{lem:markovpred} it is true that there exists a set $\mathcal{I}\subseteq \{1,...,2^k\}$ of size $\nu$ where  such that for all $i\not \in \{1,...,2^k\} \setminus\mathcal{I}$ is true that 
\[
\eta:=\Hav(W_{z,i} | \advise) \ge \frac{\alpha -|\advise| - \log{2^k/\nu}}{2^k} = \frac{\alpha +\log{\nu}-|\advise| - k}{2^k}.
\]
Let $i^*$ denote the index of the point that will be given to $\gen$ that is $p\leftarrow (w_{z,i^*}, \advise)$.  We define the set $\mathcal{I}_\advise = \mathcal{I} \cup \{w_{z,i^*}\}$ where $w$ is the point given to $\gen(w, \advise)$.  
Then for $i\not\in \mathcal{I}_{\advise}$ it is true that 
\begin{align*}
\Hav(W_{z,i} | \mathsf{key} \in \mathtt{GoodKey}, p)&\ge \\
\Hav(W_{z,i} | \advise, W_{z,i^*}= w_{z,i^*}) &\ge -\log{\frac{1}{2^{\eta}-1}} \ge \eta-1.
\end{align*}
We now proceed to bounding $\viable(w_i, p, \mathsf{key})$.  By assumption, By Lemma~\ref{lem:smallgeneralviable} we know that there are at most $2^\mu$ points in $\viable(w_i, p, \mathsf{key})$.  Thus, for all $i\not \in\mathcal{I}_{\advise}$ 

\begin{align*}
\log{\Pr\left[\viable(w_{z,i}, p, \mathsf{key}, \advise) = 1 \right]}&\le - (\eta-1 -\mu)\\
&=-\left(\frac{\alpha +\log{\nu}-|\advise| - k}{2^k}-1-\mu\right).
\end{align*}
\end{proof}

\begin{lemma}
\label{lem:convert distinguisher}
Let all parameters be as in Lemma~\ref{lem:info loss} with $\nu \in\mathbb{Z}^+$.  Then there is no $(\{0,1\}^n, \mathcal{W}, \kappa, t, \epsilon, \ell)$-fuzzy extractor with distributional advise (see Definition~\ref{def:fe distributional}) if
\[
\epsilon<1/2-\left(\epsilon_1+\epsilon_2\right)\\
\]
where 
\begin{align*}
\epsilon_1&= 2^{-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu+k},\\
\epsilon_2&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
Furthermore, there exists an algorithm $\mathcal{D}$ that always outputs $1$ when given samples of the form $r, p, z$ that are correctly generated by the fuzzy extractor.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:convert distinguisher}].
We prove the result for an average member of $Z$.  Consider the following distinguisher $\mathcal{D}$ for triples of the form $r, p, z$:
\begin{enumerate}
\item If $r \in \mathsf{GoodKey}_p$ output $1$,
\item Compute $\viable(z)$.
\item If $\sum_{i} \viable(w_{z, i}, r, p) =0 $ output $0$,
\item Else output $1$.
\end{enumerate}
First note that by perfect correctness it is always the case that when given $\mathsf{key}, p$ that $\mathcal{D}$ outputs $1$.  We proceed to bound the probability that $\mathcal{D}$ outputs $1$ when given $U_\kappa, p$.  Note that the probability that $\Pr[U_\kappa \in \mathtt{GoodKey}_p] \ge 1/2$ by the definition of $\mathtt{GoodKey}_p$. 

We bound the number of parts with at least one point in viable.  We begin by assuming that all points in viable are in different  values $r$ so the bound on the size of 
\[
\left\{w_{z, i} \middle|\sum_{\mathsf{key}\in\mathtt{Goodkey}_p} \viable(w_{z,i}, p, \mathsf{key}, \advise)>0\right\}_{i=1}^k 
\] 
gives a bound on the number of nonempty parts. By Lemma~\ref{lem:info loss} 
\[
\expe_{Z} \left[\sum_{\mathsf{key} \in \mathtt{Goodkey}_p}\sum_{i }  \viable(w_{z,i}, p, \mathsf{key}, \advise) \middle| \begin{aligned} z\leftarrow Z\\ \advise\leftarrow \aux(z)\\w\leftarrow W_Z\\(r, p)\leftarrow \gen(w, \advise)\end{aligned} \right]< 2^{-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu+k}+\nu+1.
\]
Thus the fraction of non-empty parts in $\mathsf{Goodkey}_p$ on average is at most 
\[
2^{-\kappa-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu+k}+\frac{\nu+1}{2^\kappa}.
\]
Thus, the probability that $\mathcal{D}$ outputs $0$ when given $U_\kappa, p, z$ is at least 
$1/2-(\epsilon_1+\epsilon_2)$
where 
\begin{align*}
\epsilon_1:&=2^{-\kappa-\frac{\alpha+\log{\nu}-|\advise|-k}{2^k}+1+\mu+k},\\
\epsilon_2:&=\frac{\nu+1}{2^{\kappa}}.
\end{align*}
\noindent
This completes the Proof of Lemma~\ref{lem:convert distinguisher}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:main theorem}]
Restating Lemma~\ref{lem:convert distinguisher} one has that for $(R_{U_{n,k}},P_{U_{n,k}}) \leftarrow \gen(U_{n,k}, \advise(Z_{U_{n,k}}))$
\[
\Delta((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}), (U_n, P_{U_{n,k}}, Z_{U_{n,k}}))\ge \epsilon_1.
\]
Let $\mathcal{D}$ be one distinguisher that always outputs $1$ on any value $\mathsf{key}, p, z$ for any distribution $z$, then
\begin{align*}
\Pr[\mathcal{D}((R_{U_{n,k}}, P_{U_{n,k}}, Z_{U_{n,k}}))=1] &=1\\
\Pr[\mathcal{D}(U_n, P_{U_{n,k}}, Z_{U_{n,k}})=1]&\le 1-\epsilon_1.
\end{align*}
Recall that $\Delta(U_{n,k}, \mathtt{PCode}_{n, k, t, \alpha}^{*}) \le \epsilon_2$ by Lemma~\ref{lem:close family} .  Define \[(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})\leftarrow \gen(\mathtt{PCode}_{n, k, t, \alpha}^{*}, \advise(Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})).\]
it is thus true that 
\begin{align*}
\Pr[\mathcal{D}(R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&=1\\
\Pr[\mathcal{D}(U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}})=1]&\le 1-(\epsilon_1-\epsilon_2).
\end{align*}
and thus that 
\[
\Delta((R_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}), (U_n, P_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}, Z_{\mathtt{PCode}_{n, k, t, \alpha}^{*}}))\ge \epsilon_1-\epsilon_2.
\]

\noindent
Finally, the theorem follows by application of Lemma~\ref{lem:distributional advise suffices}.
\end{proof} 

\paragraph{Analysis of parameters}
We first consider $\epsilon_2$ and $\epsilon_3$.  
\begin{enumerate}
\item For $\epsilon_2$ to be negligible it suffices that $\nu/2^\kappa$ is negligible.  Setting $\nu = \Theta(\log{n})$ suffices.
\item As discussed in Section~\ref{ssec:how much fuzzy} for $\epsilon_3$ to be negligible it suffices that 
\begin{align*}
 \gamma &\le \beta -\log{2e},\\
k &\ge \gamma + \log{n+ \omega(\log{n})}.
\end{align*}
For some arbitrary constant $0<c_1 < 1$ We set $k = \gamma + cn$. 
\end{enumerate}
We now turn to our analysis of $\epsilon_1$.  Recall that $(n/k)^k \le {n\choose k} < ((ne)/k)^k$.  Recalling parameters: 
\begin{align*}
\mu&\le n(1-2/\ln 2 (t/n)^2),\\
\alpha &= \log{2^n\choose 2^k} \ge \log{2^{n2^k} /2^{k2^k}} = (n-k)2^k,\\
k &\le n(1-h_2(t/n)) + \log{2n}
\end{align*}
This implies that 
\begin{align*}
-\log{\epsilon_1}&= \kappa+\frac{\alpha +\log{\nu}-|\advise|-k}{2^k} - \mu -k\\
&\ge  \kappa+\frac{\alpha +\log{\nu}-|\advise|-k}{2^k} - nh_2(1/2-t/n) - k,\\
&\ge  \kappa+\frac{(n-k)2^k +\log{\nu}-|\advise|-k}{2^k} - nh_2(1/2-t/n) - k,\\
&\ge  \kappa+\frac{(n-k)2^k-k2^{k} +\log{\nu}-|\advise|-k}{2^k} - nh_2(1/2-t/n)\\
&>  \kappa+\frac{(n-2k)2^k-|\advise|-k}{2^k} - nh_2(1/2-t/n) .
\end{align*}

Since we know that $\kappa =\omega(\log n)$. Then as long as $|\advise| = o(k 2^k) = o (n2^n)$ there exists some constant $0<c_2<1$ \todo{work out the size of $c_2$ in terms of $c_1$} such that 
\begin{align*}
0 &\le \frac{(n-2k)2^k-|\advise|-k}{2^k} - nh_2(1/2-t/n) \\
 &\le n-c_2n - 2\gamma - nh_2(1/2-t/n) \\
&\le n(1-c_2)-2\gamma - nh_2(1/2-t/n)\\
\gamma &\le \frac{n(1-c_2 - h_2(1/2-t/n))}{2}.\\
\end{align*}

Adding the constraint from Subsection~\ref{ssec:how much fuzzy}
this means that 
\[
\gamma \le \min\left\{n(1-h_2(t/n)) - \log{2e}, \frac{n(1-c_2 - h_2(1/2-t/n))}{2}\right\}.
\]

%\begin{align*}
%2^{(n-k)2^k}+2^{2k}-|\advise|-k - 2^knh_2(1/2-t/n) \ge 0\\
%\end{align*}
%Or equivalently that, 
%\begin{align*}
%|\advise| &\le 2^{(n-k)2^k+2k} -k - 2^knh_2(1/2-t/n)\\
%&\le 2^{\Theta(n)2^{\Theta(n)}}.
%\end{align*}


